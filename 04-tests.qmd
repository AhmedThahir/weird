# Statistical tests {#sec-tests}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

The early history of anomaly detection method involved statistical tests and assumptions about the distribution of the data. There are much better methods available these days, but these old methods are still widely used. So they are included here for historical interest, and to point out their flaws to those still using them. There is nothing in later chapters that assumes knowledge of this chapter, so if you want to skip on to more useful methods, head to @sec-boxplots.

We will only consider univariate methods in this chapter. There have been multivariate parametric methods for anomaly detection, but they are much less widely used and are also best avoided.

## Examples {#sec-examples}

We will test the methods using six examples: two involving real data, and four involving simulated data.

1. The cricket batting averages from the `cricket_batting` data set. As discussed in @sec-scatterplots, these appear to contain one genuine anomaly, the batter Don Bradman.
2. Old Faithful eruption durations since 2015, also discussed in @sec-scatterplots. For these data, there were two extreme anomalies --- one at nearly 2 hours and one at 1 second --- and several "inliers" in a area of low density between 140 and 180 seconds.
3. The first 18 rows of the second variable in the `n01` data, along with the values 4.0 and 4.5. The latter two are anomalies as they are unlikely to arise from the N(0,1) distribution. We save  this data set as `n01b`. The plot below shows the data for this example.

```{r}
#| label: fig-n01b
#| fig.asp: 0.2
#| fig.cap: "Eighteen observations from a N(0,1) distribution along with two anomalies."
n01b <- tibble(y = c(n01$v2[1:18], 4, 4.5))
n01b |>
  ggplot(aes(x = y, y = 1)) +
  geom_jitter(width = 0, alpha = 0.5) +
  scale_y_discrete() +
  labs(y = "", x = "Synthetic data")
```

The other three simulated examples will use 1000 observations from each of the following distributions:

4. a N(0,1) distribution (we will use the first variable in the `n01` data set);
5. a $\text{t}_3$ distribution, which has heavy tails (but finite variance);
6. a $\chi^2_4$ distribution, which is positively skewed.

The density functions for these three distributions are shown in @fig-testdist.

```{r}
#| label: fig-testdist
#| fig.cap: "Three distributions we will use to test various anomaly detection methods."
tibble(
    x = seq(-6, 11, l = 999),
    t3 = dt(x, df = 3),
    X4 = dchisq(x, df = 4),
    N01 = dnorm(x)
  ) |>
  pivot_longer(-x, names_to = "Distribution", values_to = "pdf") |>
  ggplot(aes(x = x, y = pdf, col = Distribution)) +
  geom_line() +
  labs(y = "Probability density") +
  scale_color_discrete(
    breaks = c("t3", "X4", "N01"),
    labels = c(
      latex2exp::TeX("$t_{~3}$"),
      latex2exp::TeX("$\\chi^{~2}_{~~4}$"),
      "N(0,1)"
    )) +
  theme(legend.text.align = 0) +
  coord_cartesian(xlim = c(-5, 10))
```

Data for the last two examples are generated below.

```{r}
#| label: fig-testdist-data
#| code-fold: false
set.seed(1)
t3 <- tibble(y = rt(1000, df = 3))
chisq4 <- tibble(y = rchisq(1000, df = 4))
```

All of the methods considered in this chapter assume the underlying data follow a normal distribution. In our examples, only #4 is from a normal distribution, so the methods should work well in that case, but perhaps not in the other cases.

Good anomaly detection methods should pick up one anomaly in the cricket batting example, at least two anomalies in the Old Faithful example, and the two anomalies in the `n01b` example. They should identify no anomalies in the remaining examples.


```{r}
#| label: check_numbers
#| include: false
#| depends: ["fig-n01b", "fig-testdist-data"]
nanomalies <- tibble(
    Example = c(
      "Cricket batting",
      "Old Faithful duration",
      "N(0,1) + 2 outliers",
      "N(0,1)",
      "t(3)",
      "Chi-squared(4)"
    ),
    `N observations` = c(
      NROW(cricket_batting),
      NROW(oldfaithful),
      NROW(n01b),
      NROW(n01),
      NROW(t3),
      NROW(chisq4)
    ),
    `Expected anomalies` = c(
      1, 2, 2, 0, 0, 0
    )
  )
```

## Z scores

Many parametric methods are based on $z$-scores which assume the data come from a normal distribution. If our data are given by $y_1,\dots,y_n$, then their $z$ scores are given by
$$z_i = (y_i - \bar{y})/s_y$$
where $\bar{y}$ is the mean and $s_y$ is the standard deviation of the observations.

Some books recommend that observations where the absolute value of $z$ is above some threshold (usually 3) be identified as anomalies. This is a bad idea for several reasons.

1. If the data really do come from a normal distribution that has been contaminated with anomalies, then the estimated mean ($\bar{y}$) and standard deviation $s_y$ will also be affected by those anomalies. Any anomaly detection method should be relatively robust to the anomalies in the data.
2. Having a fixed threshold regardless of sample size means that the probability of a spurious anomaly increases with the sample size.
3. The assumption of normality is very unlikely to be satisfied with most real data.

### Examples

Let's see what happens when we apply this test to the six examples.

```{r}
#| label: zscore1
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  mutate(z = (Average - mean(Average)) / sd(Average)) |>
  filter(abs(z) > 3) |>
  select(Player, Average, z)
oldfaithful |>
  mutate(z = (duration - mean(duration)) / sd(duration)) |>
  filter(abs(z) > 3)
n01b |>
  mutate(z = (y - mean(y)) / sd(y)) |>
  filter(abs(z) > 3)
n01 |>
  select(v1) |>
  mutate(z = (v1 - mean(v1)) / sd(v1)) |>
  filter(abs(z) > 3)
t3 |>
  mutate(z = (y - mean(y)) / sd(y)) |>
  filter(abs(z) > 3)
chisq4 |>
  mutate(z = (y - mean(y)) / sd(y)) |>
  filter(abs(z) > 3)
```

 1. Don Bradman is correctly identified as the only outlier in the cricket batting averages.
 2. Only the extreme 2-hour duration is identified. We already know there was an additional strange 1-second eruption, plus several unusual eruptions between 140 and 180 seconds in length.
 3. Neither of the two anomalies added to the 18 N(0,1) observations has been correctly identified.
 4. One spurious anomaly has been identified in the 1000 N(0,1) observations.
 5. Eighteen spurious anomalies have been identified in the 1000 $\text{t}_3$ observations.
 6. Fifteen spurious anomalies have been identified in the 1000 $\chi^2_4$ observations.


```{r}
#| label: check2
#| include: false
nanomalies <- nanomalies  |>
  mutate(Zscore = c(
    cricket_batting |>
      filter(Innings > 20) |>
      mutate(z = (Average - mean(Average)) / sd(Average)) |>
      filter(abs(z) > 3) |>
      NROW(),
    oldfaithful |>
      mutate(z = (duration - mean(duration)) / sd(duration)) |>
      filter(abs(z) > 3) |>
      NROW(),
    n01b |>
      mutate(z = (y - mean(y)) / sd(y)) |>
      filter(abs(z) > 3) |>
      NROW(),
    n01 |>
      mutate(z = (v1 - mean(v1)) / sd(v1)) |>
      filter(abs(z) > 3) |>
      NROW(),
    t3 |>
      mutate(z = (y - mean(y)) / sd(y)) |>
      filter(abs(z) > 3) |>
      NROW(),
    chisq4 |>
      mutate(z = (y - mean(y)) / sd(y)) |>
      filter(abs(z) > 3) |>
      NROW()
  ))
stopifnot(nanomalies$Zscore == c(1, 1, 0, 1, 18, 15))
```

### Probability of spurious anomalies

Even if the data did come from a normal distribution, the probability of an observation being more than 3 standard deviations from the mean is $`r round(2*pnorm(-3),4)`$ (assuming a large sample), so we would expect to see 1 in every $1/ `r round(2*pnorm(-3),4)` = `r round(.5/pnorm(-3))`$ regular observations being identified as an anomaly using this approach.

In fact, we can be more accurate than this. If our $n$ observations come from a normal distribution, then the z-scores follow (approximately) a $\text{t}_{n-1}$ distribution. (The approximation arises because we replace the mean and standard deviations by their sample estimate.) When we identify anomalies as those points with $|z|>c$, then the probability of finding at least one spurious anomaly is the probability of the maximum being above $c$ or the minimum being below $-c$. Since the distribution is symmetric, these probabilities are the same. Let $M_n$ be the maximum of $n$ z-scores computed from the data; then $P(M_n \le c) = (F_{t}(c; n-1))^n$ where $F_{t}(y; n-1)$ is the cumulative distribution function of a $t$ distribution with $n-1$ degrees of freedom. So the probability of at least one spurious anomaly is
$$
  1 - (F_{t}(c; n-1))^{2n}.
$$ {#eq-spuriousz}
This probability is accurate for large $n$, but not for small sample sizes. So, instead, we will use simulation to compute the probabilities; these are plotted in @fig-zspurious for several values of $c$ and $n$.

```{r}
#| label: pspurious
#| echo: false
# Function to compute probability of spurious anomaly for sample size n,
# distribution dist with parameters in ...
# c is zscore threshold, rep is number of replicates
pspurious <- function(n, dist, c = 3, rep = 1e5, ...) {
  if (length(n) == 1) {
    anomaly <- numeric(rep)
    for (j in seq(rep)) {
      anomaly[j] <- max(abs(scale(dist(n, ...)))) > c
    }
    return(mean(anomaly))
  } else {
    pv <- numeric(length(n))
    for (i in seq_along(n)) {
      pv[i] <- pspurious(n = n[i], dist = dist, c = c[i], rep = rep, ...)
    }
    return(pv)
  }
}
n <- c(2:10, round(exp(seq(log(15), log(3e4), l = 99))))
breaks <- sort(c(2:6, 8, 10^(1:4), 2.5 * 10^(1:3), 5 * 10^(1:3)))
```

```{r}
#| label: fig-zspurious
#| echo: false
#| dependson: pspurious
#| fig-cap: The probability of at least one spurious anomaly using z-scores and with a threshold $c$ for normally distributed data.
if (file.exists("zspuriousdf.rds")) {
  zspuriousdf <- readRDS("zspuriousdf.rds")
} else {
  zspuriousdf <- expand_grid(n = n, c = seq(2, 3.5, by = 0.5)) |>
    mutate(pmax = pspurious(n, rnorm, c = c))
  saveRDS(zspuriousdf, "zspuriousdf.rds")
}
labels <- zspuriousdf |>
  group_by(c) |>
  filter(abs(pmax - 0.75) == min(abs(pmax - 0.75))) |>
  slice(1) |>
  mutate(label = paste0("c=", sprintf("%.1f", c)))
zspuriousdf |>
  ggplot(aes(x = n, y = pmax, group = c, col = factor(c))) +
  geom_line() +
  scale_x_log10(breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(y = "Probability of at least one spurious anomaly", x = "Sample size (n)") +
  geom_label(aes(x = n * 0.9, y = 0.78, label = label, hjust = "right"), data = labels) +
  guides(col = "none") +
  theme(axis.text.x = element_text(size = 8)) +
  coord_cartesian(xlim = c(6, 7500))
```

Let's also compute the probability of spurious anomalies for different data distributions --- the $\text{t}_3$ and $\chi^2_4$ distributions shown in @fig-testdist.

```{r}
#| label: fig-zspurious2
#| echo: false
#| dependson: ["pspurious", "zspurious"]
#| fig-cap: The probability of at least one spurious anomaly using z-scores for different data distributions and with a threshold of $c=3$.
if (file.exists("zspurious2df.rds")) {
  zspurious2df <- readRDS("zspurious2df.rds")
} else {
  zspurious2df <- tibble(n = n, c = 3) |>
    mutate(
      ApproxNormal = 1 - pt(c, n - 1)^(2 * n),
      Normal = pspurious(n, rnorm, c = c),
      `t(3)` = pspurious(n, rt, df = 3, c = c),
      `Chi-squared(4)` = pspurious(n, rchisq, df = 4, c = c)
    ) |>
    select(-c) |>
    pivot_longer(-n, values_to = "Probability", names_to = "Distribution")
  saveRDS(zspurious2df, "zspurious2df.rds")
}
zspurious2df |>
  filter(Distribution != "ApproxNormal") |>
  ggplot(aes(x = n, y = Probability, group = Distribution, col = Distribution)) +
  geom_line() +
  labs(x = "n (sample size)", y = "Probability of at least one spurious anomaly") +
  scale_x_log10(breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(y = "Probability of at least one spurious anomaly", x = "Sample size (n)") +
  scale_color_discrete(
    breaks = c("t(3)", "Chi-squared(4)", "Normal"),
    labels = c(
      latex2exp::TeX("$t_{~3}$"),
      latex2exp::TeX("$\\chi^{~2}_{~~4}$"),
      "N(0,1)"
  )) +
  theme(legend.text.align = 0) +
  theme(axis.text.x = element_text(size = 8)) +
  coord_cartesian(xlim = c(6, 7500))
```

Notice how the departures from normality, either via skewness (for the $\chi^2$ distribution) or with heavier tails (for the $t$ distribution), lead to much greater probabilities for spurious anomalies. Any tests for anomaly detection that assume an underlying data distribution will be sensitive to the shape of that distribution.

Even if we were prepared to believe that the data come from a normal distribution, we need to adjust the threshold to allow for the sample size. This is the idea behind the next methods we will consider.

## Peirce's and Chauvenet's criteria

### Peirce's criterion {-}

Benjamin Peirce was a Harvard mathematician in the mid 1800s who worked with astronomical data which were prone to anomalous observations. He proposed the first known test based on what are now called $z$-scores [@Peirce1852]. His criterion was that any observations with $|z| > c$ should be "rejected", where $c$ is a complicated function depending on the sample size $n$ and the number of suspected anomalies.

@fig-zscorecriteria shows the value of $c$ as a function of sample size, when there is only one suspected anomaly.

```{r}
#| label: fig-zscorecriteria
#| warning: false
#| echo: false
#| fig.cap: "Criteria for anomalies based on z-scores."
#| dependson: pspurious
threshold <- numeric(length(n))
for (i in seq_along(n)) {
  threshold[i] <- weird:::peirce_threshold(n[i])
}
tibble(n = n, c = threshold) |>
  ggplot(aes(x = n, y = c)) +
  geom_line() +
  geom_line(aes(y = qnorm(1 - 0.25 / n)), col = "red") +
  scale_x_log10(limits = c(3, 20000), breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(x = "n (sample size)", y = "c (threshold)") +
  geom_label(aes(x = 1500, y = 3.95, label = "Peirce criterion")) +
  geom_label(aes(x = 2600, y = 3.4, label = "Chauvenet criterion"), col = "red") +
  coord_cartesian(xlim = c(3, 1.2e4), expand = FALSE)
```

The threshold increases with sample size $n$ to allow for the increasing likelihood of observations falling in the extreme tails of the distribution.

The `peirce_anomalies()` function returns a logical vector indicating which observations are anomalous under this criterion. Let's apply it to the six examples.

```{r}
#| label: peirce_examples
#| code-fold: false
cricket_batting |> filter(peirce_anomalies(Average))
oldfaithful |> filter(peirce_anomalies(duration))
n01 |>
  select(v1) |>
  filter(peirce_anomalies(v1))
n01b |> filter(peirce_anomalies(y))
t3 |> filter(peirce_anomalies(y))
chisq4 |> filter(peirce_anomalies(y))
```

```{r}
#| label: checkpeirce
#| include: false
nanomalies <- nanomalies  |>
  mutate(Peirce = c(
    cricket_batting |> filter(peirce_anomalies(Average)) |> NROW(),
    oldfaithful |> filter(peirce_anomalies(duration)) |> NROW(),
    n01b |> filter(peirce_anomalies(y)) |> NROW(),
    n01 |> filter(peirce_anomalies(v1)) |> NROW(),
    t3 |> filter(peirce_anomalies(y)) |> NROW(),
    chisq4 |> filter(peirce_anomalies(y)) |> NROW()
  ))

stopifnot(nanomalies$Peirce == c(0, 1, 2, 1, 10, 6))
```

1. When applied to the test cricket batting averages, it doesn't even find the obvious anomaly of Don Bradman.
2. When applied to the Old Faithful eruption durations, it finds only the most extreme duration.
3. Both anomalies have been correctly spotted amongst the 18 N(0,1) observations.
4. One spurious anomaly is identified in the 1000 N(0,1) observations.
5. Ten spurious anomalies have been identified in the 1000 $\text{t}_3$ observations.
6. Six spurious anomalies have been identified in the 1000 $\chi^2_4$ observations.

### Chauvenet's criterion {-}

Peirce's proposal was largely superseded by an alternative proposed by the astrophysicist William Chauvenet, which was much simpler to describe. He suggested [@Chauvenet1863] replacing the threshold $c$ by the $1-0.25/n$ quantile from the standard normal distribution. This threshold is also shown in @fig-zscorecriteria. A consequence of this choice is that Chauvenet's criterion will reject, on average, half an observation of genuine data from a normal distribution regardless of the value of $n$. However, for non-normal data, there is no such guarantee that genuine observations will not be detected as anomalies. Despite its flaws, the method is still widely used in some disciplines, especially engineering.

The `chauvenet_anomalies()` function can be used to implement this test. For our six examples, it gives similar results to those above for `peirce_anomalies()` (with two additional spurious anomalies for the $t_2$ example, and one additional spurious anomaly for the $\chi^2_4$ example.)

```{r}
#| label: checkchauvenet
#| include: false
nanomalies <- nanomalies  |>
  mutate(Chauvenet = c(
    cricket_batting |> filter(chauvenet_anomalies(Average)) |> NROW(),
    oldfaithful |> filter(chauvenet_anomalies(duration)) |> NROW(),
    n01b |> filter(chauvenet_anomalies(y)) |> NROW(),
    n01 |> filter(chauvenet_anomalies(v1)) |> NROW(),
    t3 |> filter(chauvenet_anomalies(y)) |> NROW(),
    chisq4 |> filter(chauvenet_anomalies(y)) |> NROW()
  ))

stopifnot(nanomalies$Chauvenet == c(0, 1, 2, 1, 12, 7))
```

## Grubbs' test

By the 20th century, the concept of hypothesis testing had been developed, and the $t$-distribution had been discovered, and both were applied to the identification of anomalies using $z$-scores. Many tests were developed under different assumptions about the underlying distribution and what was assumed to be known [@Hawkins1980]. We will mention just two of them here, as they are the most widely used.

Egon Pearson and Chandra Sekar proposed [@Pearson1936] that an observation be considered an anomaly if $|z_i|> c_\alpha$, where the critical value is given by
$$
c_\alpha = t_{\alpha/n, n-2} \sqrt{\frac{n-1}{n-2+t^2_{\alpha/n, n-2}}}
$$
and $t_{p, k}$ is the $1-p$ quantile of the t distribution with $k$ degrees of freedom. Later, this was extended by Frank Grubbs [@Grubbs1950] who proposed  using
$$
c_\alpha = \frac{(n-1)t_{\alpha/2n, n-2}}{\sqrt{n(n-2 + t^2_{\alpha/2n, n-2})}}.
$$

@fig-zscoretests shows the critical values at $\alpha=0.05$ for these tests, along with the corresponding value from @Chauvenet1863 for comparison.


```{r}
#| label: fig-zscoretests
#| echo: false
#| dependson: pspurious
#| fig-cap: Critical values for maximum $z$-score tests with $\alpha=0.05$.
alpha <- 0.05
df <- tibble(n = n) |>
  filter(n >= 3, n <= 10130) |>
  mutate(
    t1 = qt(1 - alpha / n, n - 2),
    t3 = qt(1 - alpha / (2 * n), n - 2),
    pearson = t1 * sqrt((n - 1) / (n - 2 + t1^2)),
    chauvenet = qnorm(1 - 0.25 / n),
    grubbs = (n - 1) / sqrt(n) * sqrt(t3^2 / (n - 2 + t3^2)),
  ) |>
  select(-t1, -t3) |>
  pivot_longer(pearson:grubbs, values_to = "c", names_to = "test")
df |>
  ggplot(aes(x = n, y = c, col = test)) +
  geom_line() +
  scale_x_log10(breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(x = "n (sample size)", y = latex2exp::TeX("$c_{\\alpha}$ (threshold)")) +
  guides(col = "none") +
  geom_label(aes(x = x, y = y, col = col, label = label, hjust = "left"),
    data = data.frame(
      x = rep(1.1e4, 3), y = c(4.35, 4.6, 4.0), col = c("pearson", "grubbs", "chauvenet"),
      label = c("Pearson-Sekar test", "Grubbs test", "Chauvenet criterion")
    )
  ) +
  coord_cartesian(xlim = c(4, 60000))
```

From these, we can compute the probability of a spurious anomaly in a normal distribution, using @eq-spuriousz. This probability should be equal to $\alpha=0.05$.

```{r}
#| label: fig-pzscoretests
#| echo: false
#| dependson: ["pspurious", "zscoretests"]
#| fig-cap: True size of the tests (i.e., the probability of a spurious anomaly) for normally distributed data with $\alpha=0.05$.
if (file.exists("pzscoredf.rds")) {
  pzscoredf <- readRDS("pzscoredf.rds")
} else {
  pzscoredf <- df |>
    mutate(pmax = pspurious(n, rnorm, c = c, rep = 1e6))
  saveRDS(pzscoredf, "pzscoredf.rds")
}
pzscoredf |>
  ggplot(aes(x = n, y = pmax, group = test, col = test)) +
  geom_hline(aes(yintercept = 0.05), col = "gray") +
  geom_line() +
  labs(x = "n (sample size)", y = "Probability of at least one spurious anomaly") +
  scale_y_continuous(breaks = c(0.05, seq(0.1, 0.8, by = 0.1)), minor_breaks = NULL) +
  scale_x_log10(breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(y = "Probability of at least one spurious anomaly", x = "Sample size (n)") +
  guides(col = "none") +
  theme(axis.text.x = element_text(size = 8)) +
  geom_label(aes(x = x, y = y, col = test, label = label, hjust = "left"),
    data = data.frame(
      x = rep(1.1e4, 3), y = c(0.10, 0.05, 0.40), test = c("pearson", "grubbs", "chauvenet"),
      label = c("Pearson-Sekar test", "Grubbs test", "Chauvenet criterion")
    )
  ) +
  coord_cartesian(xlim = c(4, 60000))
```

@fig-pzscoretests shows that only Grubbs' test gives reasonable results, with the others finding too many anomalies except in very small samples.

However, when we allow for different data distributions, Grubbs' test also gives poor results, showing it is very sensitive to the assumed data distribution.


```{r}
#| label: fig-pgrubbs
#| echo: false
#| dependson: ["pspurious","zscoretests"]
#| fig-cap: True size of Grubbs' tests (i.e., the probability of a false positive) for different data distributions with $\alpha=0.05$.
if (file.exists("pgrubbsdf.rds")) {
  pgrubbsdf <- readRDS("pgrubbsdf.rds")
} else {
  pgrubbsdf <- pzscoredf |>
    filter(test == "grubbs") |>
    mutate(
      Normal = pmax,
      ApproxNormal = 1 - pt(c, n - 1)^(2 * n),
      `t(3)` = pspurious(n, rt, df = 3, c = c),
      `Chi-squared(4)` = pspurious(n, rchisq, df = 4, c = c)
    ) |>
    select(-c, -test, -pmax) |>
    pivot_longer(-n, values_to = "Probability", names_to = "Distribution")
  saveRDS(pgrubbsdf, "pgrubbsdf.rds")
}
pgrubbsdf |>
  filter(Distribution != "ApproxNormal") |>
  ggplot(aes(x = n, y = Probability, group = Distribution, col = Distribution)) +
  geom_hline(aes(yintercept = 0.05), col = "gray") +
  geom_line() +
  labs(x = "n (sample size)", y = "Probability of at least one spurious anomaly") +
  scale_x_log10(limits = c(3, 2e4), breaks = breaks, labels = sprintf("%d", breaks), minor_breaks = NULL) +
  labs(y = "Probability of at least one spurious anomaly", x = "Sample size (n)") +
  #  guides(col = "none") +
  theme(axis.text.x = element_text(size = 8)) +
  scale_color_discrete(
    breaks = c("t(3)", "Chi-squared(4)", "Normal"),
    labels = c(
      latex2exp::TeX("$t_{~3}$"),
      latex2exp::TeX("$\\chi^{~2}_{~~4}$"),
      "N(0,1)"
  )) +
  theme(legend.text.align = 0) +
  theme(axis.text.x = element_text(size = 8)) +
  coord_cartesian(xlim = c(4, 7500))
```

We can apply the test using the `grubbs_anomalies()` function to our six examples.

```{r}
#| label: grubbs-anomalies
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  filter(grubbs_anomalies(Average)) |>
  select(Player, Country, Average)
oldfaithful |>
  filter(grubbs_anomalies(duration))
n01b |>
  filter(grubbs_anomalies(y))
n01 |>
  filter(grubbs_anomalies(v1))
t3 |>
  filter(grubbs_anomalies(y))
chisq4 |>
  filter(grubbs_anomalies(y))
```


```{r}
#| label: checkgrubbs
#| include: false
nanomalies <- nanomalies  |>
  mutate(Grubbs = c(
    cricket_batting |> filter(grubbs_anomalies(Average)) |> NROW(),
    oldfaithful |> filter(grubbs_anomalies(duration)) |> NROW(),
    n01b |> filter(grubbs_anomalies(y)) |> NROW(),
    n01 |> filter(grubbs_anomalies(v1)) |> NROW(),
    t3 |> filter(grubbs_anomalies(y)) |> NROW(),
    chisq4 |> filter(grubbs_anomalies(y)) |> NROW()
  ))

stopifnot(nanomalies$Grubbs == c(0, 1, 0, 0, 6, 1))
```

The clear failure is with the $\text{t}_3$ distribution which has no real anomalies. A spurious anomaly is also detected in the $\chi^2_4$ example, and real anomalies are missed in the Old Faithful data and in `n01b`.

## Dixon's Q test

If $y_{(1)},\dots,y_{(n)}$ denote the ordered values of our sample, then Dixon's Q statistic [@Dixon1950] is given by
$$
  Q = \frac{y_{(n)} - y_{(n-1)}}{y_{(n)} - y_{(1)}},
$$
the ratio of the difference between the two largest values to the range of the data. If the largest value is the only anomaly, then $Q$ will take a larger value than expected.

The corresponding test for the minimum to be an anomaly uses $y_{(2)}-y_{(1)}$ in the numerator instead, the difference between the second smallest and minimum observations. Both minimum and maximum values can be tested simultaneously using the two-sided test, where the numerator is the maximum of $y_{(n)} - y_{(n-1)}$ and $y_{(2)}-y_{(1)}$.

The test is clearly flawed for several reasons. First, if the two largest values are both anomalies of similar size, then $Q$ will be small and these anomalies will be missed. Also, if both maximum and minimum values are anomalies, the denominator will be larger than expected, thereby reducing the size of $Q$.

As with the other tests considered here, the test assumes that the underlying data distribution is normal, and anomalies are identified which appear inconsistent with that assumption.

Simulation can be used to compute the critical values for this test, assuming that the data come from a normal distribution. We can also use simulation to compute the probability of false positives, giving the results shown in @fig-spuriousdixon. As with the other tests we have considered, Dixon's test is very sensitive to the assumed data distribution making it largely useless for real data analysis.

```{r}
#| label: fig-spuriousdixon
#| fig.cap: "Probability of a spurious anomaly using Dixon's test."
#| dependson: "pspurious"
#| echo: false
# Function to compute probability of spurious anomaly for sample size n,
# distribution dist with parameters in ...
# rep is number of replicates
pspuriousdixon <- function(n, dist, rep = 1e6, ...) {
  if (length(n) == 1) {
    pv <- numeric(rep)
    for (j in seq(rep)) {
      pv[j] <- any(dixon_anomalies(dist(n, ...)))
    }
    return(mean(pv))
  } else {
    pv <- numeric(length(n))
    for (i in seq_along(n)) {
      # print(n[i])
      pv[i] <- pspuriousdixon(n = n[i], dist = dist, rep = rep, ...)
    }
    return(pv)
  }
}
if (file.exists("dixondf.rds")) {
  dixondf <- readRDS("dixondf.rds")
} else {
  dixondf <- tibble(n = c(3:10, seq(12, 26, by = 2), seq(30, 100, by = 5))) |>
    mutate(
      Normal = pspuriousdixon(n, rnorm),
      `t(3)` = pspuriousdixon(n, rt, df = 3),
      `Chi-squared(4)` = pspuriousdixon(n, rchisq, df = 4)
    )
  saveRDS(dixondf, "dixondf.rds")
}
dixondf |>
  pivot_longer(-n, values_to = "Probability", names_to = "Distribution") |>
  ggplot(aes(x = n, y = Probability, group = Distribution, col = Distribution)) +
  geom_hline(aes(yintercept = 0.05), col = "gray") +
  geom_line() +
  scale_y_continuous(breaks = c(0.05, seq(0.1, 0.8, by = 0.1)), minor_breaks = NULL) +
  labs(y = "Probability of at least one spurious anomaly", x = "Sample size (n)") +
  scale_color_discrete(
    breaks = c("t(3)", "Chi-squared(4)", "Normal"),
    labels = c(
      latex2exp::TeX("$t_{~3}$"),
      latex2exp::TeX("$\\chi^{~2}_{~~4}$"),
      "N(0,1)"
  )) +
  theme(legend.text.align = 0) +
  theme(axis.text.x = element_text(size = 8)) +
  coord_cartesian(xlim = c(4, 96))
```

We can apply the test using the `dixon_anomalies()` function, which does a two-sided test of both minimum and maximum observations.

```{r}
#| label: dixon-anomalies
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  filter(dixon_anomalies(Average)) |>
  select(Player, Country, Average)
oldfaithful |>
  filter(dixon_anomalies(duration))
n01b |>
  filter(dixon_anomalies(y))
n01 |>
  filter(dixon_anomalies(v1))
t3 |>
  filter(dixon_anomalies(y))
chisq4 |>
  filter(dixon_anomalies(y))
```


```{r}
#| label: checkdixon
#| include: false
nanomalies <- nanomalies  |>
  mutate(Dixon = c(
    cricket_batting |>
      filter(Innings > 20) |>
      filter(dixon_anomalies(Average)) |> NROW(),
    oldfaithful |> filter(dixon_anomalies(duration)) |> NROW(),
    n01b |> filter(dixon_anomalies(y)) |> NROW(),
    n01 |> filter(dixon_anomalies(v1)) |> NROW(),
    t3 |> filter(dixon_anomalies(y)) |> NROW(),
    chisq4 |> filter(dixon_anomalies(y)) |> NROW()
  ))

stopifnot(nanomalies$Dixon == c(1, 1, 0, 0, 1, 0))
```

For these examples, the test has worked relatively well apart from `n01b`, where it is failed to identify either anomaly, and `t3` where it has identified a spurious anomaly.

## Summary

We can summarise the results of the various tests used here in the following table.

```{r}
#| label: tbl-nanomalies
#| echo: false
#| tbl-cap: "Number of anomalies detected by each test. Cells in red indicate that the test has failed to identify all known anomalies and only known anomalies."
library(kableExtra)
nanomalies$Example[5] <- "<span class='math inline'>\\text{t}_3</span>"
nanomalies$Example[6] <- "<span class='math inline'>\\chi^2_4</span>"
nanomalies |>
  mutate(
    Example = paste0(row_number(), ". ", Example)
  ) |>
  knitr::kable(escape = FALSE, format = "html") %>%
  add_header_above(c(" " = 3, "Anomaly detection method" = 5)) %>%
  column_spec(4, color = if_else(nanomalies$Zscore != nanomalies$`Expected anomalies`, "red", "black")) %>%
  column_spec(5, color = if_else(nanomalies$Peirce != nanomalies$`Expected anomalies`, "red", "black")) %>%
  column_spec(6, color = if_else(nanomalies$Chauvenet != nanomalies$`Expected anomalies`, "red", "black")) %>%
  column_spec(7, color = if_else(nanomalies$Grubbs != nanomalies$`Expected anomalies`, "red", "black")) %>%
  column_spec(8, color = if_else(nanomalies$Dixon != nanomalies$`Expected anomalies`, "red", "black"))
```

None of the tests found the two clear anomalies in the Old Faithful data, and most failed to find either of the artificial anomalies in `n01b`. All tests found at least one spurious anomaly in the large simulated data sets with no real anomalies.

As noted at the start of this chapter, anomaly detection methods based on assumed data distributions are very unreliable. Until about 1975, they were the only viable methods given the lack of computing facilities available, because they could be implemented using tables and hand calculations. However, there is really no justifiable reason for continuing to use such methods.

They are particularly sensitive to the assumed data distribution, and the probability of detecting spurious anomalies is usually much higher in reality than under the ideal conditions in which the tests were conceived.

These tests are sometimes applied iteratively, where observations are removed from the data if determined to be anomalies, and the test re-applied to the remaining data. This process continues until no more anomalies are found. However, this procedure will clearly change the size of the test due to the problem of multiple comparisons.

Next, we will turn out attention to methods that arose in the latter part of the 20th century which were based on quantiles and data depth rather than on any underlying parametric data distribution.
