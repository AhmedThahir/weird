# Density-based methods {#sec-density-methods}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

Anomalies are observations that are unlikely to have come from the same distribution as the rest of the data. So one approach to identifying anomalies is to estimate the probability distribution of the data (perhaps leaving some observations out), and then identify observations that are unlikely to have come from that distribution. This is the approach taken by density-based methods, where anomalies are defined as observations of low probability.

## KDE scores {#kdescores}

The kernel density estimate at each observation is @eq-mkde
$$
  f_i = \hat{f}(\bm{y}_i) = \frac{1}{n} \sum_{j=1}^n K_H(\bm{y}_i-\bm{y}_j),
$$ {#eq-kdescores}
and we define the "**kde score**" at each observation as
$$
  s_i = -\log(f_i).
$$
These provide a measure of how anomalous each point is --- anomalies are points where the kde scores are relatively large. The largest possible score occurs when an observation has no other observations nearby. Then $f_i \approx K_H(\bm{0})/n$ because $K_H(\bm{y}_i-\bm{y}_j)\approx 0$ when $\|\bm{y}_i-\bm{y}_j\|$ is large. So the largest possible kde score, when using a Gaussian kernel, is
$$
  -\log(K_H(\bm{0})/n) \approx \log(n) + \frac{d}{2}\log(2\pi) + \frac{1}{2}\text{log det}(\bm{H}).
$$
For univariate data, when $d=1$, this simplifies to
$$
  -\log(K_h(0)/n) \approx \log(nh\sqrt{2\pi}).
$$

For the Old Faithful eruption duration data, we obtain the following kde scores.

```{r}
#| label: ofscores
#| code-fold: false
of_scores <- oldfaithful |>
  mutate(score = kde_scores(duration))
of_scores |> arrange(desc(score))
```

The two largest scores correspond to the extreme 2 hour duration, and the tiny 1 second duration. In both cases, the kde score is at its maximum. @fig-ofscores shows the kde scores for each observation. This has the shape of $-\log(f)$, and so we can see that the lowest score corresponds to the mode of the estimated density.


```{r}
#| label: fig-ofscores
#| fig.cap: "KDE scores for the Old Faithful eruption durations."
#| dependson: ofscores
of_scores |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration, y = score)) +
  geom_point()
```

For data sets of low dimension (say $d \le 3$), this approach works quite well in identifying anomalies. However, as $d$ increases, it becomes increasingly difficult to estimate the density, and so it does not work so effectively for large values of $d$.

## Lookout algorithm

A variation on the idea of kde scores, is to consider estimates of the density at each observation, after removing that observation from the calculation. This is known as "leave-one-out density estimation". Then the leave-one-out kde (or lookout) scores are simply
$$
  f_{-i} = \left[nf_i - K_H(\bm{0})\right]/(n-1),
$$ {#eq-lookout}
where $f_i$ is the kde estimate at $\bm{y}_i$ using all data. This is because the contribution of the $i$th point to the kernel density estimate at that point is $K_H(\bm{0})/n$, so the leave-one-out kernel density estimate is simple to compute (without needing to re-estimate the density).

The "lookout" algorithm (standing for Leave-One-Out Kernel density estimates for OUTlier detection) was proposed by @lookout2021 and uses these lookout scores to find the probability of each observation being an anomaly.

In this procedure, we fit a Generalized Pareto Distribution to $s_i$ using the POT approach discussed in @sec-evt, with the $90^{\text{th}}$ percentile as the threshold.

```{r}
#| label: ofpot
#| code-fold: false
of_scores <- oldfaithful |>
  mutate(
    score = kde_scores(duration),
    looscore = kde_scores(duration, loo = TRUE),
  )
threshold <- quantile(of_scores$score, prob = 0.90, type = 8)
gpd <- evd::fpot(of_scores$score, threshold = threshold)$estimate
```

Now we apply the fitted distribution to the leave-one-out scores to obtain the probability of each observation based on the distribution of the remaining observations.

```{r}
#| label: ofpot2
#| code-fold: false
#| dependson: ofpot
of_lookout <- of_scores |>
  mutate(
    pval = evd::pgpd(looscore,
      loc = threshold,
      scale = gpd["scale"], shape = gpd["shape"], lower.tail = FALSE
    )
  ) |>
  arrange(pval)
of_lookout
```

Low probabilities indicate likely outliers and high probabilities indicate normal points. This procedure has identified the minimum and maximum eruptions as clear outliers, with several other values around 90--93 seconds, 155 seconds and 304--305 seconds as likely outliers as well. These latter observations are in the regions of low density that we have previous noted. @fig-ofpot shows an HDR boxplot of the data, with those points identified as lookout anomalies highlighted in red.

```{r}
#| label: fig-ofpot
#| fig.cap: "HDR boxplot of the Old Faithful eruption durations, with the lookout anomalies highlighted in red."
#| dependson: ofpot2
#| fig.asp: 0.2
oldfaithful  |>
  filter(duration < 7200)  |>
  gg_hdrboxplot(duration) +
  geom_point(data = of_lookout |> filter(duration < 7200, pval < 0.05), aes(x = duration, y = 0), color = "red")


```

The above code was introduced to illustrate each step of the procedure, but it is simpler to use the `lookout_prob` function. Let's apply it to the six examples introduced in @sec-examples.

```{r}
#| label: lookout
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  mutate(lookout = lookout_prob(Average)) |>
  filter(lookout < 0.05) |>
  select(Player, Average, lookout)  |>
  arrange(lookout)
```

Here Bradman is a clear anomaly, with a handful of other batters (all males) also showing up as possible anomalies (with probabilities a little less than 0.05).

The same algorithm is easily applied in two dimensions. Here we use the `oldfaithful` data set, and consider the distribution of Duration and Waiting time, ignoring those observations that are greater than 2 hours in either dimension.

```{r}
#| label: lookout2
#| code-fold: false
of <- oldfaithful  |>
  select(duration, waiting) |>
  filter(duration < 7200, waiting < 7200)
of |>
  mutate(lookout = lookout_prob(of)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, duration)
```

The lookout algorithm has identified the 1 second eruption, as well as a few others in regions of low probability.

Next we consider the artificial examples, starting with the first 18 rows of the second variable in the `n01` data, along with the values 4.0 and 4.5.

```{r}
#| label: lookout3
#| code-fold: false
n01b <- tibble(y = c(n01$v2[1:18], 4, 4.5))
n01b |>
  mutate(lookout = lookout_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout)
```

As expected, only the two genuine anomalies have been identified.

Finally, we consider simulated data from each of the distributions, N(0,1), $\text{t}_3$ and $\chi^2_4$.

```{r}
#| label: lookout4
#| code-fold: false
set.seed(1)
t3 <- tibble(y = rt(1000, df = 3))
chisq4 <- tibble(y = rchisq(1000, df = 4))
n01 |>
  select(v1) |>
  mutate(lookout = lookout_prob(v1)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, v1)
t3 |>
  mutate(lookout = lookout_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, y)
chisq4 |>
  mutate(lookout = lookout_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, y)
```

The algorithm has found 6 of the 1000 N(0,1) observations to be possible anomalies, as well as the one true anomaly in the fourth example.

The `lookout` package also implements this method, but uses the Epanechnikov kernel rather than the Gaussian kernel, and selects the bandwidth using a different approach. This idea will be discussed in Chapter ??
