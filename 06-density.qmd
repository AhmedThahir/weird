# Density-based methods {#sec-density-methods}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
wine_reviews <- fetch_wine_reviews() |> cache("wine_reviews")
```

Anomalies are observations that are unlikely to have come from the same distribution as the rest of the data. So one approach to identifying anomalies is to first estimate the probability distribution of the data, and then identify observations that are unlikely to have come from that distribution. This is the approach taken by density-based methods, where anomalies are defined as observations of low probability.

## Surprisals {#sec-surprisals}

Any density-based method of anomaly detection first requires that we have a density estimate at each observation, which we will denote by $f(\bm{y}_i)$. This may be an assumed density, or estimated from the data; it may be a parametric function, or a nonparametric estimate; it may be a conditional density or a marginal density. Wherever it comes from, we will assume that $f$ represents a probability density function that is appropriate for the data set. When $\bm{y}_i$ takes values on a discrete space (such as the integers), then $f$ is a probability mass function, but we will call it a "density" for convenience when discussing methods that can apply to both discrete and continuous data. In many cases, $f(\bm{y}_i)$ is the estimated *likelihood* of the observation from a given model.

The "surprisal" [@Tribus1961] of an observation $\bm{y}_i$, is defined as $g_i = -\log f(\bm{y}_i)$, measuring the surprise in seeing the observation. So it is a measure of how anomalous (or surprising) that observation is, given the density $f$. A large value of $g_i$ indicates that $\bm{y}_i$ is not a likely value, and so is a potential anomaly. On the other hand, typical values will have low surprisals. In information theory, the average surprisal is known as the entropy of a random variable [@Cover2006;@Stone2022], and provides a measure of uncertainty.

Surprisals are commonly used in forecasting and prediction problems, where they known as "log scores", and are used to assess whether an estimated distribution $f$ provides a good description of the future values $y$ [@Gneiting2014]. In that context, the data $y$ are assumed to come from some distribution, and the surprisals are used to assess how well the estimated distribution $f$ matches that distribution. Here we are using surprisals in reverse --- we assume $f$ is a good description of the data, and then we use the surprisals to identify observations that are unlikely to have come from that distribution.

If the density $f$ has been estimated from the data, then it is sometimes useful to consider the leave-one-out (LOO) estimate given by $f_{-i}$. That is, $f_{-i}$ is the density estimate using all observations other than the $i$th observation. Then an unusual observation can't influence the density estimate, giving us a better measure of how anomalous it is. Then we will call the associated surprisal, a "LOO surprisal", given by $g_i = -\log f_{-i}(\bm{y}_i)$.

We can use surprisals to compute the probability of an observation being as anomalous as $y_i$ under the distribution $f$:
$$
p_i = \int f(u) ùüô(-\log f(u) > g_i) du,
$$ {#eq-pi-density}
where $ùüô(z)$ is an indicator function taking value 1 when $z$ is true and 0 otherwise. This probability is related to highest density regions discussed in @sec-hdr and @sec-mhdr. If $y_i$ lies on the boundary of the $100(1-\alpha)\%$ highest density region, then $p_i = 1-\alpha$. Therefore, if we want a false positive rate of $\alpha$, we can label $y_i$ as an anomaly if $p_i < \alpha$.

```{r}
#| label: fig-density-scores
#| fig-cap: "Top: Density with one observation shown at $y_i$. The shaded area shows the area where $f(y) < f(y_i)$, corresponding to $p_i$. Bottom: surprisals for different values of $y$. The dashed line shows the surprisal $g(y_i) = -\\log f(y_i)$, corresponding to $y_i$."
#| fig-asp: 1
#| echo: false
den <- dist_chisq(5)
hdr <- hdr(den, size = 90) |> unlist() %>% `[`(c("lower", "upper"))
falpha <- density(den, at = hdr["lower"])
df <- tibble(
  y = seq(0, 15, l = 501),
  fy = density(den, at = y)[[1]],
  g = -log(fy)
)
p1 <- df |>
  ggplot(aes(x = y, y = fy)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)") +
  geom_hline(aes(yintercept = falpha), col = "#D55E00", linetype = "dashed") +
  geom_polygon(
    fill = "#D55E00",
    data = df |>
      filter(y >= hdr[2]) |>
      bind_rows(
        tibble(y = c(hdr[2], hdr[2], max(df$y)), fy = c(0, falpha, 0))
      ) |>
      arrange(fy, y)
  ) +
  geom_polygon(
    fill = "#D55E00",
    data = df |>
      filter(y <= hdr["lower"]) |>
      bind_rows(
        tibble(y = c(hdr[1], hdr[1], 0), fy = c(0, falpha, 0))
      ) |>
      arrange(fy, desc(y))
  ) +
  scale_x_continuous(
    breaks = c(seq(0, 15, by = 5), hdr[2]),
    labels = latex2exp::TeX(c(seq(0, 15, by = 5), "$y_i$")),
    minor_breaks = NULL
  ) +
  scale_y_continuous(
    breaks = c(seq(0, 0.15, by = 0.05), falpha),
    labels = latex2exp::TeX(c(seq(0, 15, by = 5), "$f(y_i)$")),
    minor_breaks = NULL
  )
p2 <- df |>
  ggplot(aes(x = y, y = g)) +
  geom_line() +
  labs(x = "y", y = "surprisal: g(y) = -log f(y)") +
  geom_hline(aes(yintercept = -log(falpha)), col = "#D55E00", linetype = "dashed") +
  geom_line(
    data = tibble(x = rep(hdr[2], 2), y = c(0, falpha)),
    aes(x = x, y = y), col = "#D55E00", linetype = "dashed"
  ) +
  scale_x_continuous(
    breaks = c(seq(0, 15, by = 5), hdr[2]),
    labels = latex2exp::TeX(c(seq(0, 15, by = 5), "$y_i$")),
    minor_breaks = NULL
  ) +
  scale_y_continuous(
    breaks = c(seq(0, 7, by = 2), -log(falpha)),
    labels = latex2exp::TeX(c(seq(0, 7, by = 2), "$g(y_i)$")),
    minor_breaks = NULL
  )
patchwork::wrap_plots(p1, p2, ncol = 1)
```

The function `surprisal_prob()` function will return values of $p_i$ given values of the surprisals $g_i$, where the assumed density $f$ can be provided as a distribution object from the `distributional` package.

### Symmetric distributions

For any symmetric distribution, $p_i = 2[1 - F(|y_i|)]$, where $F$ is the corresponding distribution function. Therefore, under the normal distribution $N(\mu, \sigma^2)$, $p_i = 2[1-\Phi(|y_i-\mu|/\sigma)]$, where $\Phi$ is the standard normal distribution function. In this case, identifying anomalies as observations with $p_i < \alpha$ is equivalent to a z-score test (@sec-zscores) with the threshold set to the $1-\alpha/2$ quantile of the standard normal distribution.

### Generalized Pareto tail probabilities {#sec-gpd-tails}

If we do not know what the distribution of the data should be, or if we are unsure about the assumed distribution, we can use extreme value theory to estimate the tail probabilities.

We fit a Generalized Pareto Distribution (GPD) to the largest surprisals using the POT approach discussed in @sec-evt. For example, we may fit a GPD using the $90^{\text{th}}$ percentile as the threshold. Then we apply the fitted distribution to the surprisals to obtain the probability of each observation.

This may seem unnecessary, as we have already used a distribution in computing the surprisals, so why do we need to estimate another distribution? The main reason is that our assumed distribution $f$ may not be correct, or may be difficult to estimate in the extreme tails. But even under the incorrect distribution, the extreme surprisals will still follow a Generalized Pareto Distribution regardless of how accurately $f$ has been estimated. So it is a safe choice when we are not confident in the assumed density $f$.

The approach does require a relatively large number of surprisals to be available. We are only using the top 10% of observations in estimating the GPD, so we would normally need at least a few hundred observations to start with.

### Empirical tail probabilities

An alternative approach that does not require a GPD is to estimate $p_i$ as the proportion of surprisals larger than $g_i$. For large $n$, this provides a quick and convenient estimate of $p_i$ that is accurate even when the assumed distribution is not. Suppose $f$ is the true (unknown) density, and $f^*$ is the assumed (possibly incorrect) density. Then for the approximation to hold, we only need that the ordering of $-\log f(y_i)$ and $-\log f^*(y_i)$ is similar in the tails of the distribution.

### Example

For example, consider the 1000 values contained in the first column of `n01`. These were generated from a standard normal distribution. We can use `surprisal_prob()` to identify anomalies with probability less than 1%. This is equivalent to z-scores greater than `r sprintf("%.3f", qnorm(1-0.01/2))` in absolute value.

We will use all three methods described above using the correct density in computing the scores, and then repeat the calculation using an incorrect density. For the incorrect density, we will use a $t_3$ distribution, which has much longer tails than the true $N(0,1)$ distribution.

```{r}
#| label: surprisal_prob
#| code-fold: false
# Compute surprisal probabilities of v1 assuming a standard normal distribution and a t4 distribution
norm_prob <- tibble(
  y = n01$v1,
  p = surprisal_prob(y, dist_normal()),
  p_gpd = surprisal_prob(y, dist_normal(), GPD = TRUE),
  p_emp = rank(-surprisals(y, dist_normal())) / length(y),
  pt = surprisal_prob(y, dist_student_t(4)),
  pt_gpd = surprisal_prob(y, dist_student_t(4), GPD = TRUE),
  pt_emp = rank(-surprisals(y, dist_student_t(4)))/ length(y)
)
norm_prob |> arrange(p)
```

The most accurate values are in column `p`, as this uses the correct distribution applied to the correct surprisals. Notice that `p_gpd` and `p_emp` are both relatively accurate: the GPD and empirical approximations work. For the probabilities calculated using the incorrect distribution, `pt` is relatively inaccurate, with much larger probabilities than they should be, especially in the extreme tails. But `pt_gpd` and `pt_emp` are both reasonable estimates, despite being based on an incorrect distribution for computing the scores. In fact, `p_emp` and `pt_emp` are identical, because the ordering of the surprisals is unchanged despite the incorrect distribution being used.

The various estimates are displayed in @fig-surprisal_prob for values of $y$ greater than 2.5, showing how the estimates (other than `pt`) are particularly accurate in the extreme tails, where we need them for anomaly detection.

```{r}
#| label: fig-surprisal_prob
norm_prob |>
  filter(y > 2.5) |>
  tidyr::pivot_longer(p:pt_emp, names_to = "Estimate", values_to = "Probability") |>
  ggplot(aes(x = y, y = Probability, col = Estimate)) +
  geom_line()
```

In summary, even when we don't know the distribution of the data, we can use either a GPD or the empirical distribution to obtain good estimates of the tail probabilities of the surprisals.

## Linear regression {#sec-regression-log-scores}

Suppose we want to find anomalies amongst $n$ univariate observations $y_1,\dots,y_n$, and we have $p$ variables that we think might be useful for predicting $y$. Then we can write the conditional density as $f(y \mid \bm{x})$, where $\bm{x}$ is a $p$-dimensional vector of predictor variables. Anomalies in $y$ are identified as observations that are unlikely to have come from the conditional density $f$. This is commonly called a "regression model", regardless of the form of $f$, or whether the relationship with $\bm{x}$ is linear or not.

By far the most common type of regression model assumes that $f$ is a Normal distribution, and that the conditional mean is a linear function of $\bm{x}$. Note that this does *not* mean that $y$ is Normally distributed, or that $\bm{x}$ has a Normal distribution. The assumption is that the *conditional* distribution of $y$ given $\bm{x}$ is Normal, which can easily be checked by looking at the residuals from the regression model.

For a linear Normal regression model, with independent observations and homoscedastic errors, the conditional distribution is given by
$$
  y \mid \bm{x} \sim N(\bm{x}_+'\bm{\beta}, \sigma^2),
$$ {#eq-gaussian-regression}
where $\bm{x}_+ = [1, \bm{x}]'$ is a $(p+1)$-dimensional vector containing a 1 in the first position and the predictors in the remaining positions, and $\bm{\beta}$ is a $(p+1)$-dimensional vector of regression coefficients.

### Model estimation

The model can be written in matrix form as
$$
  \bm{y} \sim N(\bm{X}\bm{\beta}, \sigma^2\bm{I}),
$$
where $\bm{X}$ is an $n\times(p+1)$ matrix with the first column being a vector of 1s, and the other columns containing the predictor variables, or equivalently as
$$
  \bm{\varepsilon} = \bm{y} - \bm{X}\bm{\beta} \sim N(\bm{0}, \sigma^2\bm{I}).
$$ {#eq-error-distribution}
Provided $\bm{X}$ is of rank $p+1$, and the errors $\bm{\varepsilon}$ are independent of $\bm{X}$, the model can be estimated using ordinary least squares regression [@seberlee2003], resulting in the estimate
$$
  \hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}.
$$
The fitted values (i.e., predicted values for the training data) are given by
$$
  \hat{\bm{y}} = \bm{X}\hat{\bm{\beta}} = \bm{H}\bm{y},
$$
where $\bm{H} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$ is known as the "hat"-matrix because it creates the "y-hat" values $\hat{\bm{y}}$ from the data $\bm{y}$.

The diagonals of $\bm{H}$, given by $h_1,\dots,h_n$, take values between 0 and 1. These are known as the "leverage" values [@faraway2004linear, p69], and measure how much each observation influences the corresponding fitted value. High leverage values (close to 1) correspond to observations that have a large influence on the estimated coefficients, and so leaving those observations out will lead to very different values for the fitted values and residuals. On the other hand, small leverage values (close to 0) correspond to observations that have little influence on the estimated coefficients, and so leaving those observations out will lead to similar values for the fitted values and residuals.

### Residuals

The residuals from the model are given by
$$
  \bm{e} = \bm{y} - \hat{\bm{y}} = (\bm{I} - \bm{H})\bm{y}.
$$ {#eq-residual-distribution}
Note that the residuals have the distribution $\bm{e}\mid\bm{X} \sim N(\bm{0}, \sigma^2(\bm{I} - \bm{H}))$, which is not quite the same as the distribution of the errors given by @eq-error-distribution. However, the two distributions are asymptotically equivalent as $n\rightarrow\infty$. Often, we need standardized residuals, which are obtained by dividing each residual by its estimated standard deviation, giving
$$ r_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_i}}, \qquad i = 1,\dots, n,
$$
where
$$
\hat\sigma^2 = \frac{1}{n-p-1}\sum_{i=1}^n e_i^2
$$ {#eq-residual-variance}
is the estimated residual variance.

A linear model can be estimated in R using the `stats::lm()` function. The `broom::augment()` function will compute the residuals (named `.resid`), the standardized residuals (named `.std.resid`), and the leverage values (names `.hat`).

The surprisals under the Gaussian linear regression model @eq-gaussian-regression can be estimated using these standardized residuals, giving
$$
  g_i = -\log\phi(r_i),
$$ {#eq-regression-log-score}
where $\phi(u) = (2\pi)^{-1/2}e^{-u^2}$ is the standard normal density. This can be computed as follows, assuming that `fit` is the output from `stats::lm()`.

```r
broom::augment(fit) |>
  mutate(f_scores = -dnorm(.std.resid, log = TRUE))
```

Equivalently, the `surprisals()` function will compute them:

```r
surprisals(fit)
```

### LOO residuals

The leave-one-out residual for the $i$th observation is defined as the difference between $\bm{y}_i$ and the predicted value obtained using a model fitted to all observations except the $i$th observation. At first, this appears to involve a lot of computation --- estimating $n$ separate models. However, the leave-one-out residuals are easily obtained from a linear regression model without actually having to re-estimate the model many times. It can be shown [@Montgomery2012, Appendix C.7] that the leave-one-out (LOO) residuals are given by
$$
  e_{-i}  = e_{i}/(1-h_{i}),
$$ {#eq-loo-residuals}
where $e_{i}$ is the residual obtained from fitting the model to *all* observations. If we divide the LOO residuals by $\hat\sigma$ from @eq-residual-variance, we obtain the "standardized" residuals.

In the context of anomaly detection, it often makes more sense to standardize each LOO residual by the standard deviation estimated from the model fitted to all other observations, rather than by $\hat\sigma$. If we leave out the $i$th observation, and fit a regression model to the remaining observations, then the estimated variance of the residuals is given by [@Montgomery2012, Appendix C.8]
$$
  \hat\sigma_{-i}^2 = \frac{1}{n-p-2}\left[(n-p-1)\hat\sigma^2 - e_{i}^2/(1-h_i)\right].
$$ {#eq-loo-residual-variance}
These are computed by `broom::augment()` and the values of $\hat\sigma_{-i}$ are returned in the column `.sigma`.

If we standardize each residual using $\sigma_{-i}$, we obtain the "studentized" residuals
$$
  r_{-i} = \frac{e_{i}}{\hat\sigma_{-i}\sqrt{1-h_i}},
$$ {#eq-studentized-residuals}
from which we obtain the log-LOO regression scores given by
$$
  p_{-i} = -\log \phi(r_{-i})
$$ {#eq-log-loo-regression-scores}

If `fit` is the output from `stats::lm()`, then these quantities can be computed as follows.

```r
broom::augment(fit) |>
  mutate(
    studentized_residuals = .resid / (.sigma * sqrt(1 - .hat)),
    loo_fscores = -log(dnorm(studentized_residuals, log = TRUE))
  )
```

More simply, we can just use the `surprisals()` function again:

```r
surprisals(fit, type = "loo")
```

### Example: Shiraz reviews

For example, consider the wine reviews of Shiraz (aka Syrah), plotted in @fig-shiraz. We can fit a linear regression model to these data to obtain a conditional density estimate of price given the points awarded to each wine. Then, $\bm{X}$ contains just two columns: a column of 1s, and a column containing the points values. The vector $\bm{y}$ contains the log prices of the wines. The model can be fitted as follows.

```{r}
#| label: shiraz-data
#| eval: false
#| code-fold: false
wine_reviews <- fetch_wine_reviews()
```

```{r}
#| label: shiraz-regression
#| code-fold: false
shiraz <- wine_reviews |> filter(variety %in% c("Shiraz", "Syrah"))
fit_wine <- lm(log(price) ~ points, data = shiraz)
summary(fit_wine)
```

The fitted model can be written as
$$
  \log(\text{Price}) \sim N(`r sprintf("%.3f",coef(fit_wine)[1])` + `r sprintf("%.3f",coef(fit_wine)[2])` \times \text{Points}, `r sprintf("%.3f",sigma(fit_wine))`^2),
$$
and is depicted in @fig-shiraz-regression with 95% prediction intervals.

```{r}
#| label: fig-shiraz-regression
#| warning: false
#| fig.cap: Log price of Shiraz as a function of points, with 95% prediction intervals. The points are horizontally jitted to reduce overplotting. Points outside the prediction intervals are colored.
wine_aug <- broom::augment(fit_wine, data = shiraz, interval = "prediction") |>
  mutate(
    lwr = exp(.lower),
    upr = exp(.upper),
    location = case_when(
      price < lwr ~ "below",
      price > upr ~ "above",
      TRUE ~ "within"
    )
  )
wine_aug |>
  ggplot(aes(y = price, x = points, col = location)) +
  geom_jitter(height = 0, width = 0.1, alpha = 0.5) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "#cccccc", alpha = 0.25) +
  geom_line(aes(y = exp(.fitted)), color = "#666666") +
  scale_y_log10() +
  guides(fill = "none", col = "none") +
  scale_color_manual(values = c("#0072B2", "#D55E00", "#AAAAAA"))
```

The LOO surprisals obtained from this model are shown in @fig-shiraz-regression-scores, using the same colors as @fig-shiraz-regression to indicate whether the observation is below, within, or above, the 95% prediction interval.

```{r}
#| label: fig-shiraz-regression-scores
#| fig.cap: Residuals and surprisals for the Shiraz data using a linear regression model. Points are colored to match the 95% prediction intervals in @fig-shiraz-regression.
#| fig.height: 6
#| fig.asp: 0.8
wine_aug <- wine_aug |>
  mutate(
    Studentized_residuals = .resid / (.sigma * sqrt(1 - .hat)),
    fscores = -dnorm(.std.resid, log = TRUE),
    LOO_fscores = -dnorm(Studentized_residuals, log = TRUE)
  )
wine_aug |>
  select(points, Studentized_residuals, LOO_fscores, location) |>
  tidyr::pivot_longer(c(Studentized_residuals, LOO_fscores), names_to = "variable", values_to = "value") |>
  mutate(variable = factor(variable, levels = c("Studentized_residuals", "LOO_fscores"))) |>
  ggplot(aes(x = points, y = value, col = location)) +
  facet_grid(variable ~ ., scales = "free_y") +
  geom_jitter(height = 0, width = 0.1, alpha = 0.5) +
  geom_hline(yintercept = 0, color = "#666666") +
  labs(x = "Points", y = "") +
  guides(fill = "none", col = "none") +
  scale_color_manual(values = c("#0072B2", "#D55E00", "#AAAAAA"))
```

```{r}
#| label: shiraz-regression-scores
#| include: false
most_anomalous <- wine_aug |>
  filter(LOO_fscores > 17)
underpriced <- wine_aug |>
  filter(Studentized_residuals == min(Studentized_residuals))
good_buy <- wine_aug |>
  filter(points > 95) |>
  filter(Studentized_residuals == min(Studentized_residuals))
```

The over-priced wines under this model are shown in blue, while the under-priced wines are shown in orange. This shows that the most anomalous observations are the two with LOO surprisals above 17, and studentized residuals close to 6. The largest LOO surprisal is for the most over-priced wine (under this model), a 2009 Shiraz from the Henschke winery in the Eden Valley region of South Australia, with `r most_anomalous$points[1]` points and a price of $`r most_anomalous$price[1]`.

```{r}
#| code-fold: false
wine_aug |>
  filter(LOO_fscores == max(LOO_fscores)) |>
  select(country:winery, year, points, price, Studentized_residuals, LOO_fscores)
```

The largest LOO surprisal corresponding to an under-priced wine is for the wine with the lowest residual value, with `r underpriced$points[1]` points and a price of $`r underpriced$price[1]`. Another good buy, at the higher quality end, is the 2007 Syrah from the Rulo winery in the Columbia Valley in Washington State, USA:

```{r}
#| code-fold: false
wine_aug |>
  filter(points > 95) |>
  filter(Studentized_residuals == min(Studentized_residuals)) |>
  select(country:winery, year, points:price, Studentized_residuals, LOO_fscores)
```

This corresponds to the only orange point in @fig-shiraz-regression that has a point value above 95 and a price below the 95% prediction interval.

### Score tail probabilities

We will compute the tail probabilities for these scores using the empirical distribution.

```{r}
#| code-fold: false
wine_aug |>
  mutate(tail_prob = rank(-LOO_fscores) / NROW(wine_aug)) |>
  select(country:winery, year, points, price, LOO_fscores, tail_prob) |>
  arrange(tail_prob)
```

Those with the ten smallest tail probabilities are all wines that appear to be over-priced given their points values.

```{r}
#| code-fold: false
shiraz <- shiraz |>
  mutate(prob = surprisal_prob(fit_wine))
shiraz |>
  select(country:winery, year, points, price, prob) |>
  arrange(prob)
```

@fig-shiraz-surprisal shows the relationship between points and price, with the points colored according to the surprisal probability. The six blue points are observations with surprisal probabilities less than 0.02.

```{r}
#| label: fig-shiraz-surprisal
#| fig.cap: Price vs points for Shiraz wines. Points are colored according to the surprisal probability. The six blue points are observations with surprisal probabilities less than 0.02.
shiraz |>
  ggplot(aes(x = points, y = price, color = prob < 0.02)) +
  geom_jitter(height = 0, width = 0.2) +
  scale_y_log10()
```


## GAM surprisals

In some applications, it is not appropriate to assume the conditional density is Gaussian, or that the relationships are linear. One useful model that allows for non-Gaussian densities, and non-linear relationships, is a generalized additive model or GAM. Under this model, the conditional density is given by [@Wood2017gam]
$$
  y\mid\bm{x} \sim f(\mu), \qquad \ell(\mu) = \sum_{k=1}^p g_k(x_{k}),
$$
where $\mu = \text{E}(y | \bm{x})$ denotes the conditional mean, $\ell()$ is a link function, and each $g_k$ function is smooth. If $f$ is Normal, $\ell$ is the identity, and $g_i(u) = \beta_i u$, then this reduces to the linear Gaussian model (@eq-gaussian-regression).

```{r}
#| include: false
df_no <- cricket_batting |>
  filter(Innings > 0) |>
  mutate(prop_no = NotOuts / Innings)
p <- sum(df_no$NotOuts) / sum(df_no$Innings)
```

Consider the number of "not outs" for each batter in the `cricket_batting` data set. A "not out" occurs when a batsman has not been dismissed at the end of the team's innings. Let's consider if there are some batters who have an unusually high proportion of not outs. The data set contains results from `r sum(df_no$Innings)` innings, of which `r sum(df_no$NotOuts)` were not outs. So the overall proportion of not outs is $`r sum(df_no[["NotOuts"]])` / `r sum(df_no[["Innings"]])` = `r sprintf("%.3f", p)`$.

@fig-notouts shows the proportion of not outs for each batter as a function of the number of innings they played. There is some overplotting that occurs due to batters having the same numbers of not-outs and innings, which results in the higher color density of the corresponding plotted points. The unusual structure on the left of each plot is due to the discrete nature of the data. Batters who have played only a smaller number of innings tend to have a higher proportion of not outs on average, and a higher variance, than those who have played a large number of innings.

```{r}
#| label: fig-notouts
#| fig.cap: "Proportion of not outs for each batter as a function of the number of innings they played."
#| message: false
#| code-fold: false
df_no <- cricket_batting |>
  filter(Innings > 0) |>
  mutate(prop_no = NotOuts / Innings)
df_no |>
  ggplot(aes(x = Innings, y = NotOuts / Innings)) +
  geom_point(alpha = 0.15)
```

This suggests that we can construct a GAM for the number of not outs for each batter as a function of the number of innings they played. It is natural to use a Binomial distribution with a logit link function:
$$
  \text{NotOuts} \mid \text{Innings} \sim \text{Binomial}(n=\text{Innings},~ p),
$$
where
$$
  \log(p / (1- p)) = g(\text{Innings})
$$
We can fit this model using the `mgcv` package.

```{r}
#| label: fig-notouts-gam
#| message: false
#| code-fold: false
#| fig.cap: "Proportion of not outs for each batter as a function of the number of innings they played, with a GAM fit using a Binomial distribution. The blue line shows the probability of a batter being not out as a function of the number of Innings they have played."
fit_notouts <- mgcv::gam(prop_no ~ s(Innings),
  data = df_no,
  family = binomial(link = logit), weights = Innings
)
notouts_aug <- broom::augment(fit_notouts, data = df_no, type.predict = "response")
notouts_aug |>
  ggplot(aes(x = Innings, y = prop_no)) +
  geom_point(alpha = 0.2) +
  geom_line(aes(y = .fitted), color = "#0072B2") +
  geom_ribbon(
    aes(
      ymin = .fitted - 2 * .se.fit,
      ymax = .fitted + 2 * .se.fit
    ),
    fill = "#0072B2", alpha = 0.2
  ) +
  labs(y = "Proportion of not outs")
```

Now we can use the fitted model to compute the surprisals from the Binomial distribution, and find the most anomalous batters. Unfortunately, there is not a convenient way to compute loo surprisals for GAM models, so we will only consider surprisals in this example.

```{r}
#| code-fold: false
#| warning: false
notouts_aug <- notouts_aug |>
  mutate(
    fscores = surprisals(fit_notouts),
    prob = surprisal_prob(fit_notouts)
  ) |>
  select(Player:Country, Innings:NotOuts, prop_no:.fitted, fscores:prob) |>
  arrange(desc(fscores))
notouts_aug
```

The most anomalous batters are all "tail-enders" (i.e., not skilled batters) who played for a long time (so they have a large number of innings). Because they batted last, or nearly last, they are more likely to be not out at the end of the team's innings.

The `.fitted` value is the expected proportion of not outs for each player given the number of innings they have played, while `prop_no` gives the actual proportion of not outs they have had. The largest surprisal is for English batter Jimmy Anderson, who has had `r notouts_aug$NotOuts[1]` not outs in `r notouts_aug$Innings[1]` innings, which is much higher than the expected number of not outs of $`r notouts_aug[["Innings"]][1]` \times `r sprintf("%.3f", notouts_aug[[".fitted"]][1])` = `r sprintf("%.1f", notouts_aug[[".fitted"]][1]*notouts_aug[["Innings"]][1])`$. This anomaly is also seen in @fig-notouts-gam, as being somewhat unusual for that part of the data. Although Jimmy Anderson was not a great batter, he was good at defence, and was able to bat for a long time without being dismissed, leaving the other batter time to score runs.

We have identified an anomaly that is not anomalous in the proportion of not-outs, or in the number of innings, and the difference between the actual proportion and the predicted proportion is not anomalous either compared to some of the other values. However, because we have used a statistical model, we have been able to account for the particular features of this data set, such as the discrete nature of the data, and the changing variance, to identify an observation that is anomalous in the context of the model.


## KDE scores {#sec-kdescores}

Suppose, instead of a regression or a GAM, we estimate $f$ using a kernel density estimate. Then we call the resulting surprisals "kde scores". The kernel density estimate at each observation is (@eq-mkde)
$$
  f_i = \hat{f}(\bm{y}_i) = \frac{1}{n} \sum_{j=1}^n K_H(\bm{y}_i-\bm{y}_j),
$$ {#eq-kdescores}
and so the "**kde score**" at each observation as
$$
  p_i = -\log(f_i).
$$
The largest possible score occurs when an observation has no other observations nearby. Then $f_i \approx K_H(\bm{0})/n$ because $K_H(\bm{y}_i-\bm{y}_j)\approx 0$ when $\|\bm{y}_i-\bm{y}_j\|$ is large. So the largest possible kde score, when using a Gaussian kernel, is
$$
  -\log(K_H(\bm{0})/n) \approx \log(n) + \frac{d}{2}\log(2\pi) + \frac{1}{2}\text{log det}(\bm{H}),
$$
where $\bm{H}$ is now the bandwidth matrix. For univariate data, when $d=1$, this simplifies to
$$
  -\log(K_h(0)/n) \approx \log(nh\sqrt{2\pi}).
$$

### Leave-one-out kde scores

The contribution of the $i$th point to the kernel density estimate at that point is $K_H(\bm{0})/n$. Therefore, we can compute leave-one-out kde scores as
$$
  f_{-i} = \left[nf_i - K_H(\bm{0})\right]/(n-1),
$$ {#eq-lookkde}
where $f_i$ is the kde estimate at $\bm{y}_i$ using all data. Thus, we can compute the leave-one-out kernel surprisals without needing to re-estimate the density many times.

### The lookout algorithm {#sec-lookout}

The **"lookout" algorithm** (standing for Leave-One-Out Kernel density estimates for OUTlier detection) was proposed by @lookout2021 and uses surprisal probabilities to find the probability of each observation being an anomaly. Unlike the KDE discussed earlier, it uses a TDA method of bandwidth selection.

### Example: Old Faithful eruption durations

For the Old Faithful eruption duration data, we obtain the following LOO kde scores.

```{r}
#| label: ofscores
#| code-fold: false
of_scores <- oldfaithful |>
  mutate(
    score = surprisals(duration, loo = FALSE),
    loo_score = surprisals(duration, loo = TRUE),
    lookout = lookout_prob(duration)
  )
of_scores |> arrange(desc(loo_score))
```

The two infinite LOO scores correspond to the extreme 2 hour duration, and the tiny 1 second duration. These are so improbable given the rest of the data, that the scores are effectively infinite. The regular kde scores (with `loo = FALSE`) are at their maximum values.

@fig-ofpot shows an HDR boxplot of the data (other than the maximum), with those points identified as lookout anomalies highlighted in red.

```{r}
#| label: fig-ofpot
#| fig.cap: "HDR boxplot of the Old Faithful eruption durations, with the lookout anomalies highlighted in red."
#| dependson: ofpot2
#| fig.asp: 0.2
#| code-fold: false
oldfaithful |>
  filter(duration < 7200) |>
  gg_hdrboxplot(duration, show_anomalies = TRUE)
```

### More examples

Let's apply the lookout algorithm to the six examples introduced in @sec-examples.

```{r}
#| label: cricket-lookout
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  mutate(lookout = lookout_prob(Average)) |>
  filter(lookout < 0.05) |>
  select(Player, Average, lookout)
```

Here Bradman is a clear anomaly (with a very low lookout probability), and no-one else is identified as a possible anomaly.

The same algorithm is easily applied in two dimensions. Here we use the `oldfaithful` data set, and consider the distribution of Duration and Waiting time, ignoring those observations that are greater than 2 hours in either dimension.

```{r}
#| label: lookout2
#| code-fold: false
of <- oldfaithful |>
  select(duration, waiting) |>
  filter(duration < 7200, waiting < 7200)
of |>
  mutate(
    loo_scores = surprisals(of, loo = TRUE),
    lookout = lookout_prob(of)
  ) |>
  filter(lookout < 0.05) |>
  arrange(lookout, duration)
```

Now, `r english::words(sum(surprisal_prob(of) < 0.05))` anomalies are identified, with `r english::words(sum(surprisals(of, loo = TRUE) == Inf))` of them having infinite LOO surprisals. We can visualize them in an HDR scatterplot, shown in @fig-ofpot2.

```{r}
#| label: fig-ofpot2
#| fig.cap: "HDR scatterplot of the Old Faithful eruption durations and waiting times, with the lookout anomalies highlighted in red."
of |>
  gg_hdrboxplot(duration, waiting, scatterplot = TRUE, show_lookout = TRUE)
```

Next we consider some artificial examples. First, we consider the first 48 rows of the second variable in the `n01` data, along with the values 4.0 and 4.5.

```{r}
#| label: lookout3
#| code-fold: false
n01b <- tibble(y = c(n01$v2[1:48], 4, 4.5))
n01b |>
  mutate(lookout = surprisal_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout)
```

As expected, only the two genuine anomalies have been identified.

Finally, we consider 1000 simulated observations from each of the distributions, N(0,1), $\text{t}_3$ and $\chi^2_4$.

```{r}
#| label: lookout4
#| code-fold: false
n01 |>
  select(v1) |>
  mutate(lookout = surprisal_prob(v1)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, v1)
set.seed(1)
tibble(y = rt(1000, df = 3)) |>
  mutate(lookout = surprisal_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, y)
tibble(y = rchisq(1000, df = 4)) |>
  mutate(lookout = surprisal_prob(y)) |>
  filter(lookout < 0.05) |>
  arrange(lookout, y)
```

The algorithm has found a small number of spurious anomalies in each case, out of the 1000 observations included. Notably, the results do not appear to deteriorate with the heavier-tailed or skewed distributions.

## Other density-based methods
