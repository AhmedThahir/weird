# Multivariate probability distributions {#sec-multivariate}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

## Joint probability distributions {#sec-multivariate}

While we will cover anomaly detection in univariate data, most of the methods we will discuss are for multivariate data. We will therefore need to understand some basic concepts of multivariate probability distributions.

Suppose $\bm{Y} = [Y_1,\dots,Y_n]'$ is a random variable taking values in $\mathbb{R}^d$, the $d$-dimensional real numbers. Then the joint distribution of $\bm{Y}$ is defined by the joint cdf
$$
F(\bm{y}) = \text{Pr}(\bm{Y} \le \bm{y}),
$$
while the joint density function is given by
$$
f(\bm{y}) = \frac{\partial^n F(\bm{y})}{\partial y_1 \dots \partial y_d}.
$$

The marginal cdfs are defined by $F_i(y) = \text{Pr}(Y_i \le y)$, with corresponding marginal pdfs given by $f_i(y) = F_i'(y)$, $i=1,\dots,d$.

If the variables are independent, then the joint pdf is the product of the marginal pdfs, $f(\bm{y}) = \prod_{i=1}^d f_i(y_i)$.

The expected value of $\bm{y}$ is given by
$$
\text{E}(\bm{Y}) = \int_{\mathbb{R}^d} \bm{y} f(\bm{y})d\bm{y},
$$
and the covariance matrix is given by
$$
\text{Var}(\bm{Y}) = \text{E}[(\bm{Y}-\text{E}(\bm{Y}))(\bm{Y}-\text{E}(\bm{Y}))'].
$$
The covariance matrix is a $d\times d$ matrix, with $(i,j)$th element given by $\text{Cov}(Y_i,Y_j) = \text{E}[(Y_i-\text{E}(Y_i))(Y_j-\text{E}(Y_j))]$. The diagonal elements are the variances of the individual variables, while the off-diagonal elements are the covariances between the variables.

### Multivariate Normal distribution

If random variable $\bm{Y}$ has a multivariate Normal distribution, we write $\bm{Y} \sim \text{N}(\bm{\mu}, \bm{\Sigma})$, where $\bm{\mu}$ is the mean and $\bm{\Sigma}$ is the covariance matrix.

The multivariate Normal distribution has pdf given by
$$
f(\bm{y}; \bm{\mu}, \bm{\Sigma}) = (2\pi)^{-d/2}|\bm{\Sigma}|^{-1/2} \exp\left\{-\frac{1}{2}(\bm{y}-\bm{\mu})'\bm{\Sigma}^{-1}(\bm{y}-\bm{\mu})\right\}.
$$
The notation $|\bm{\Sigma}|$ denotes the determinant of the matrix $\bm{\Sigma}$.

Multivariate Normal distributions have the interesting property that the marginal distributions are also Normal.

### Further reading

A good reference on multivariate probability distributions is @JKmulticontinuous1.

## Highest density regions

As with univariate distributions, a **highest density region** for a multivariate distribution is defined as the region of the sample space where the density is higher than a given threshold. Suppose we have a multivariate random variable $\bm{Y}$ with a smooth, continuous density function $f$. Then the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{\bm{y}: f(\bm{y}) \ge f_\alpha\}
$$
where $P(\bm{Y} \in R_\alpha) = 1-\alpha$.

HDRs are equivalent to level sets of the density function, and so can be plotted as contours for bivariate density functions. For example, the bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ is shown in @fig-bivariate as a series of HDR contours, each containing an additional 10% of the probability mass.

```{r}
#| label: fig-bivariate
#| message: false
#| fig-cap: Bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
library(mvtnorm)
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
# Random sample from the distribution
samp <- rmvnorm(10000, mu, Sigma)
# Define the density function on a grid
grid <- seq(-5.5, 5.5, l = 999)
den <- list(x = grid, y = grid)
den$z <- dmvnorm(expand_grid(grid, grid), mu, Sigma) |>
  matrix(nrow = length(grid), ncol = length(grid), byrow = TRUE)
# Find the HDRs with probability 0.1, 0.2, ..., 0.9
prob <- seq(9) / 10
hdr <- hdrcde::hdr.2d(x = samp[, 1], y = samp[, 2], den = den, prob = prob)
# Plot the contours
  expand_grid(
    x = seq(-3, 3, l = 199),
    y = seq(-3, 3, l = 199)
  ) |>
  mutate(z = dmvnorm(cbind(x, y), mu, Sigma)) |>
  ggplot(aes(x = x, y = y, z = z)) +
  geom_contour_filled(breaks = rev(c(hdr$falpha, 100))) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  ) +
  scale_fill_manual(
    values = rev(viridisLite::viridis(10)),
    labels = rev(paste0(100 * rev(prob), "%"))
  ) +
  guides(fill = guide_legend(title = "HDR coverage"))
```

Similarly, we can obtain HDRs for a mixture distribution. Suppose we had two bivariate Normal distributions with means $(0,0)$ and $(3,1)$, and covariance matrices equal to  $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and the identity matrix $\bm{I}_2$ respectively. Then the HDRs for an equal mixture of these two distributions is shown in @fig-bivariate-mixture.

```{r}
#| label: fig-bivariate-mixture
#| message: false
#| fig-cap: Bivariate mixture distribution of two equally weighted Normal components with means $(0,0)$ and $(3,1)$, and covariance matrices $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and $\bm{I}_2$ respectively. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
library(mvtnorm)
p1 <- 0.5
p2 <- 0.5
mu1 <- c(0, 0)
mu2 <- c(3, 1)
Sigma1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
Sigma2 <- diag(2)
n1 <- round(10000 * p1)
n2 <- round(10000 * p2)
# Random sample from mixture distribution
samp <- rbind(rmvnorm(n1, mu1, Sigma1), rmvnorm(n2, mu2, Sigma2))
# Define the density function on a grid
minx <- round(min(samp[, 1]) - 0.5, 1)
maxx <- round(max(samp[, 1]) + 0.5, 1)
miny <- round(min(samp[, 2]) - 0.5, 1)
maxy <- round(max(samp[, 2]) + 0.5, 1)
den <- list(x = seq(minx, maxx, l = 999), y = seq(miny, maxy, l = 999))
den$z <- matrix(
  p1 * dmvnorm(expand_grid(den$x, den$y), mu1, Sigma1) +
  p2 * dmvnorm(expand_grid(den$x, den$y), mu2, Sigma2),
  nrow = length(den$x), ncol = length(den$y)
)
# Find the HDRs with probability 0.1, 0.2, ..., 0.9
prob <- seq(9) / 10
hdr <- hdrcde::hdr.2d(x = samp[, 1], y = samp[, 2], den = den, prob = prob)
# Plot the contours
mixture_density <- expand_grid(
    x = seq(minx, maxx, by = 0.05),
    y = seq(miny, maxy, by = 0.05)
  ) |>
  mutate(
    z = p1 * dmvnorm(cbind(x, y), mu1, Sigma1) +
      p2 * dmvnorm(cbind(x, y), mu2, Sigma2)
  )
mixture_plot <- mixture_density |>
  ggplot(aes(x = x, y = y, z = z)) +
  geom_contour_filled(breaks = rev(c(hdr$falpha, 100))) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  ) +
  scale_fill_manual(
    values = rev(viridisLite::viridis(10)),
    labels = rev(paste0(100 * rev(prob), "%"))
  )
mixture_plot +
  guides(fill = guide_legend(title = "HDR coverage"))
```

Here, the 10% and 20% HDRs contain disconnected regions, but for the larger HDRs, there is just one region for each.

## Multivariate quantiles

Unlike the univariate case, there is no unique definition of a multivariate quantile. There are many different definitions, and each has its own advantages and disadvantages. In this book, we are mostly concerned with sample multivariate quantiles, and we will defer a more detailed discussion of multivariate quantiles to @sec-boxplots.

## Multivarate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate version of @eq-kde is given by [@Scott2015]
$$
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_H(\bm{y} - \bm{y}_i),
$$ {#eq-mkde}
where $K_H$ is a multivariate probability density with covariance matrix $\bm{H}$. Whenever we estimate a multivariate kernel density estimate, we will use a multivariate Gaussian kernel given by
$$
  K_H(\bm{u}) = (2\pi)^{-d/2} |\bm{H}|^{-1/2} \exp \{-\textstyle\frac12 \bm{u}'\bm{H}^{-1}\bm{u} \}.
$$

```{r}
#| label: bivariatebandwidths
#| include: false
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  mutate(eruption = row_number())

h1 <- MASS::bandwidth.nrd(of2021$duration)
h2 <- MASS::bandwidth.nrd(of2021$waiting)
```

We will illustrate the idea using a simple bivariate example of 10 observations: the same 10 eruption durations discussed above, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by $\bm{H} = \left[\begin{array}{cc}`r round(h1,0)` & 0 \\ 0 & `r round(h2,0)`\end{array}\right]$.

```{r}
#| label: fig-ofdw
#| dependson: of2021
#| warning: false
#| fig-cap: Contours of bivariate kernels centered over the observations.
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  mutate(eruption = row_number())
h <- c(13, 300)
k <- expand_grid(
    x = seq(-3 * h[1], 3 * h[1], l = 100),
    y = seq(-3 * h[2], 3 * h[2], l = 100)
  ) |>
  mutate(z = dnorm(x, 0, h[1]) * dnorm(y, 0, h[2]))
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration, y = y + waiting)
ggplot() +
  geom_contour(data = of2021kde, aes(x = x, y = y, group = eruption, z = z),
               bins = 4, col = "gray") +
  geom_point(data = of2021, mapping = aes(x = duration, y = waiting)) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

If we add the bivariate kernel functions as in @eq-mkde, we obtain the bivariate kernel density estimate shown below.

```{r}
#| label: fig-ofbivariate1
#| dependson: of2021
#| fig-cap: Bivariate kernel density estimate formed by summing the kernels shown in @fig-ofdw.
of2021 |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point() +
  geom_density_2d(color = "#0072B2") +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

Now we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors).

We will use the `geom_density_2d()` function, which by default uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by @eq-nrr.

```{r}
#| label: fig-ofbivariate2
#| fig.cap: "Bivariate kernel density estimate with default bandwidths."
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d(color = "#0072B2") +
  labs(x = "Duration (seconds)")
```

Here we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There is one anomalous eruption where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.

If $\bm{H}$ is diagonal with values $h_1,\dots,h_d$, then the estimator is consistent when
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh^d = \infty,
$$
where $h = (h_1h_2\dots,h_d)^{1/d}$ is the geometric mean of the diagonal values of $\bm{H}$. The default values for `geom_density_2d()` satisfy this property.

Note that the diagonal values $h_1,\dots,h_d$ tend to be larger than the values used in the corresponding univariate density estimates, as the convergence properties of the estimate are slower for larger $d$. Consequently, the default bandwidths for `geom_density_2d()` tend to be too small. Further, because we are interested in the tails of the distribution, we usually want larger bandwidths than would be suitable for obtaining good estimates of the density function.

@fig-ofbivariate3 shows a bivariate kernel density estimate where the bandwidths are double the default values.

```{r}
#| label: fig-ofbivariate3
#| fig.cap: "Bivariate kernel density estimate with double the default bandwidths."
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d(adjust = 2, color = "#0072B2") +
  labs(x = "Duration (seconds)")
```

### Further reading

There is a rich literature on kernel density estimation. A good starting point is @Scott2015 or @chacon2018multivariate.

## Conditional probability distributions

A fundamental concept in statistics is a **conditional probability distribution**; that is, the distribution of a random variable conditional on the values of other random variables. Almost all statistical modelling involves the estimation of conditional distributions. For example, a regression is a model for the conditional distribution of a response variable given the values of a set of predictor variables. In its simplest form, we assume the conditional distribution is normal, with constant variance, and mean equal to a linear function of the predictor values. Generalized linear models allow for non-normal conditional distributions, while generalized additive models allow for non-linear relationships between the response and the predictors.

The conditional cdf of $Y$ given $X_1,\dots,X_n$ is defined by the conditional cdf
$$
 F(y\mid x_1,\dots,x_n) = \text{Pr}(Y \le y \mid  X_1 = x_1,\dots,X_n = x_n).
$$
The conditional pdf is given by
$$
f(y \mid  x_1, \dots, x_n) = \frac{f(y,x_1,\dots,x_n)}{f(x_1,\dots,x_n)}.
$$
The conditional pdf can be thought of as slices of the joint pdf, with the values of $x_1,\dots,x_n$ fixed, rescaled to ensure the conditional pdfs integrate to 1. For example, $f(y_1 | y_2)$ is equal to a scaled slice of the joint pdf $f(y_1,y_2)$ at $y_2$. @fig-conditional shows some examples for the distribution shown at @fig-bivariate-mixture at several values of $y_2$. The left plot shows the joint density, with horizontal lines indicating where conditioning (or slicing) occurs at different values of $y_2$. The right plot shows the resulting conditional density functions.

```{r}
#| label: fig-conditional
#| message: false
#| depends: fig-bivariate-mixture
#| fig-cap: Conditional distribution of $Y_2|Y_1$, where $(Y_1,Y_2)$ has the joint distribution plotted in @fig-bivariate-mixture. The left plot shows the joint density with the values of $y_2$ where we will condition, while the right plot shows conditional density functions at different values of $y_2$.
# Conditioning points
y2 <- -2:3
# Scaling factor for each density
scale_cond_density <- mixture_density |>
  filter(round(y, 2) %in% round(y2, 2)) |>
  summarise(scale = sum(z), .by = y) |>
  transmute(
    scale = scale / max(scale),
    y2 = factor(y, levels = y2)
  )
# Joint density plot
plot1 <- mixture_plot +
  guides(fill = "none") +
  scale_y_continuous(breaks = y2, minor_breaks = NULL) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6)) +
  geom_hline(aes(yintercept = y2), data = tibble(y2 = y2), color = "#8ab3f6")
# Conditional density plots
plot2 <- mixture_density |>
  filter(round(y, 2) %in% round(y2, 2)) |>
  mutate(y2 = factor(y, levels = y2)) |>
  left_join(scale_cond_density, by = "y2") |>
  mutate(
    z = z / scale,
    z = z / max(z) * 0.9
  ) |>
  ggplot(aes(x = x, y = z + y, group = y2)) +
  geom_ribbon(aes(ymin = y, ymax = z + y, xmin = -2, xmax = 4),
              col = "#8ab3f6", fill = "#8ab3f6") +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Conditional densities: $f(y_1|y_2)$")
  ) +
  scale_y_continuous(minor_breaks = NULL, breaks = y2) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6))
# Show plots side by side
plot1 | plot2
```

Another neat property of Normal distributions is that the conditional distribution of a subset of variables is also Normal. For example, suppose $\bm{Y} = (Y_1,Y_2,Y_3)$ is a multivariate Normal random variable. Then the conditional distribution of $Y_1$ given $Y_2$ and $Y_3$ is also Normally distributed.

### Kernel conditional density estimation


## Extreme value theory {#sec-evt}

Extreme Value Theory is used to model rare, extreme events and is useful in anomaly detection. Suppose we have $n$ independent and identically distributed random variables $Y_1, \dots, Y_n$ with a cdf $F(y) = P\{Y \leq y\}$. Then the maximum of these $n$ random variables is $M_n = \max \{Y_1, \dots, Y_n\}$. If $F$ is known, the cdf of $M_n$ is given by $P\{M_n \leq z \} = \left(F(z)\right)^n$. However, $F$ is usually not known in practice. This gap is filled by Extreme Value Theory, which studies approximate families of models for $F^n$ so that extremes can be modeled and their uncertainty quantified.

It is well known, due to the [central limit theorem](02-tools.html#sec-clt), that the average of a set of iid random variables will converge to the normal distribution if the mean and variance both exist and are finite. The Fisher-Tippett-Gnedenko (FTG) Theorem provides an analogous result for the maximum. It was developed in a series of papers by @Frechet1927, @Fisher1928, and @Gnedenko1943. Independently, @Mises1936 proposed a similar result. The FTG Theorem states that if the maximum can be scaled so that it converges, then the scaled maximum will converge to either a Gumbel, Fréchet or Weibull distribution [@coles2001introduction, 46]; no other limits are possible.

### Fisher-Tippett-Gnedenko Theorem {-}

If there exist sequences $\{a_n\}$ and $\{b_n\}$ such that
$$
  P\left\{ \frac{(M_n - a_n)}{b_n} \leq z \right\} \rightarrow G(z) \quad \text{as} \quad n \to \infty,
$$
where $G$ is a non-degenerate cumulative distribution function, then $G$ belongs to one of the following families:
\begin{align*}
  &\text{Gumbel} :  && G(z) = \exp\left(-\exp \left[- \Big(\frac{z-b}{a}\Big) \right] \right), \quad -\infty < z < \infty , \\
  &\text{Fréchet} : && G(z) =
    \begin{cases}
      0 ,                                                           & z \leq b  , \\
      \exp \left( - \left( \frac{z-b}{a}\right)^{-\alpha} \right) , & z > b    ,
    \end{cases}                     \\
  &\text{Weibull} : && G(z) =
    \begin{cases}
      \exp \left( - \left(- \left[\frac{z-b}{a}\right]\right)^{\alpha} \right) , & z < b  ,    \\
      1 ,                                                                        & z \geq b  ,
    \end{cases}
\end{align*}
for parameters $a, b$ and $\alpha$ where $a, \alpha >0$.

These three families of distributions can be further combined into a single family by using the following cdf known as the Generalized Extreme Value (GEV) distribution,
$$
  G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\} ,
$$ {#eq-EVT4}
where the domain of the function is $\{z: 1 + \xi (z - \mu)/\sigma >0 \}$. The location parameter is $\mu\in\mathbb{R}$, $\sigma>0$ is the scale parameter, while $\xi\in\mathbb{R}$ is the shape parameter. When $\xi = 0$ we obtain a Gumbel distribution with exponentially decaying tails. When $\xi < 0$ we get a Weibull distribution with a finite upper end, and when $\xi > 0$ we get a Fréchet family of distributions with heavy tails including polynomial tails.

If we take the negative of the random variables $Y_1,\dots,Y_n$, it becomes clear that a similar result holds for the minimum.

The three types of limits correspond to different forms of the tail behaviour of $F$.

  * When $F$ has a finite upper bound, such as with a uniform distribution, then $G$ is a Weibull distribution.
  * When $F$ has exponential tails, such as with a normal distribution or a Gamma distribution, then $G$ is a Gumbel distribution.
  * When $F$ has heavy tails including polynomial decay, then $G$ is a Fréchet distribution. One example is when $F$ itself is a Fréchet distribution with $F(y)= e^{-1/y}$, $y>0$.

To illustrate, suppose $F$ is a standard normal distribution N(0,1) and we have $n=1000$ observations, $Y_1,\dots,Y_n$. Then we know that the distribution of the maximum is given by
$$
  P\left\{ \max \{Y_1, \dots, Y_n\} \leq z \right\} = \left[ \Phi(z) \right]^n,
$$
so the density of the maximum is given by
$$
  f(z) = n \left[ \Phi(z) \right]^{n-1} \phi(z).
$$ {#eq-maxn01}

According to the FTG theorem, the distribution of the maximum can be approximated by a Gumbel distribution. We can find the parameters of the Gumbel distribution by estimating them from simulated maximums. We simulate 10000 maximums (each from 1000 N(0,1) draws), and fit a Gumbel distribution to the resulting data. The resulting Gumbel distribution is shown in @fig-evdexample (in blue), along with the true distribution of the maximum given by @eq-maxn01 (in black). The approximation is so good, that it is hard to distinguish the two distributions.

```{r}
#| label: "fig-evdexample"
#| fig.cap: "Distribution of the maximum of 1000 N(0,1) draws. Here we have simulated 10000 such maximums and shown the estimated GEV distribution (in blue), along with the true distribution (in black)."
m <- 10000 # Number of simulations
n <- 1000 # Number of observations in each data set
maximums <- numeric(m)
for (i in seq(m)) {
  maximums[i] <- max(rnorm(n))
}
gev <- evd::fgev(maximums)$estimate
max_density <- tibble(y = seq(2, 6, l = 100)) |>
  mutate(
    fy = n * pnorm(y)^(n-1) * dnorm(y),
    estfy = evd::dgev(y, loc = gev["loc"], scale = gev["scale"], shape = gev["shape"])
  )
max_density |>
  ggplot(aes(x = y, y = fy)) +
  geom_line() +
  geom_line(aes(y = estfy), col = "#0072B2")
```

### The Generalized Pareto Distribution {-}

The Peaks Over Threshold (POT) approach regards extremes as observations greater than a threshold $u$. The probability distribution of *exceedances* above a specified threshold $u$ can be expressed as the conditional distribution
$$
  H(y) = P\left \{Y \leq u + y \mid Y > u \right \}
  = \frac{ F(u+y) - F(u)}{1 - F(u)}.
$$ {#eq-POT3}
When the distribution $F$ satisfies the FTG theorem, then [@coles2001introduction, 75] $H$ is a  **Generalized Pareto Distribution** (GPD) defined by
  $$
    H(y) \approx 1 - \Big( 1 + \frac{\xi y}{\sigma_u} \Big)^{-1/\xi} ,
  $$ {#eq-POT}
where the domain of $H$ is $\{y: y >0 \text{ and } (1 + \xi y)/\sigma_u >0 \}$, and $\sigma_u = \sigma + \xi(u- \mu)$. The GPD parameters are determined from the associated GEV parameters. In particular, the shape parameter $\xi$ is the same in both distributions.

Continuing the previous example, we now look at the probability distribution of exceedances above 3 from a N(0,1) distribution. We simulate 1 million N(0,1) values and only keep those above 3. Then two GPD estimates are drawn. The orange GPD uses the parameters obtained previously from the GEV estimate, while the blue GPD estimates the parameters from the exceedances.

```{r}
#| label: fig-pot
#| dependson: evdexample
#| fig.cap: "Conditional distribution of exceedances above 3 from N(0,1) draws. Here we have simulated 1 million values and only kept those above 3. The density implied by the GEV distribution is shown in orange, and an estimated GPD distribution is shown in blue."
df <- tibble(y = rnorm(1e6)) |>
  filter(y > 3)
gpd <- evd::fpot(df$y, 3)$estimate
max_density <- max_density |>
  mutate(
    hy = evd::dgpd(y, loc = 3, scale = gev["scale"], shape = gev["shape"]),
    hy2 = evd::dgpd(y, loc = 3, scale = gpd["scale"], shape = gpd["shape"])
  )
df |>
  ggplot(aes(x = y)) +
  geom_line(data = max_density, aes(x = y, y = hy), col = "#D55E00") +
  geom_line(data = max_density, aes(x = y, y = hy2), col = "#0072B2")
```

The orange estimate is slightly better because it is based on more information (10000 maximums rather than `r NROW(df)` exceedances).

### Further reading

A good introduction to extreme value theory is @coles2001introduction.
