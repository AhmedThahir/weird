# Multivariate probability distributions {#sec-multivariate}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

While we will cover anomaly detection in univariate data, most of the methods we will discuss are for multivariate data. We will therefore need to understand some basic concepts of multivariate probability distributions.

## Hiding in high-dimensional space {#sec-hiding}

When first thinking about multivariate probability distributions, it can take some time to develop an intuition for how they behave. One way to start developing this intuition is to plot data with increasing numbers of dimensions, starting with the univariate case.

Let's take the `n01` data set, which contains 10 variables, each from independent standard normal distributions. We will consider only the first three variables, but to make the example more interesting, we will consider cumulative sums of these variables. We will also add a couple of anomalies, and then see if we can find them using some data visualizations.

```{r}
#| label: demo_data
#| code-fold: false
# Construct example data set
df <- n01 |>
  transmute(
    V1 = v1,
    V2 = v1 + v2,
    V3 = v1 + v2 + v3
  )
# Add anomalies
df[999, ] <- tibble(V1 = -2, V2 = 3.4, V3 = 1.3)
df[1000, ] <- tibble(V1 = 1.2, V2 = -1.8, V3 = 1.1)
df$anomaly <- c(rep(FALSE, 998), TRUE, TRUE)
```

First, we can do a strip plot of each variable to visualize the data from a univariate perspective.

```{r}
#| label: fig-demo_strip
#| code-fold: false
#| fig-asp: 0.3
#| fig-cap: Strip plots of each of the three variables, with anomalies shown in orange.
df |>
  pivot_longer(V1:V3) |>
  ggplot(aes(x = value, y = 0, col = anomaly)) +
  geom_jitter(width = 0) +
  facet_grid(name ~ .) +
  scale_y_discrete() +
  labs(x = "", y = "") +
  scale_color_manual(values = c("#777777", "#D55E00"))
```

The only obvious feature of these graphs is that the variance increases in successive panels. This is because the variance of the sum of independent random variables is the sum of their variances. The two anomalies are plotted in each panel, but they are not obviously different from the other observations.

Now we will plot pairwise scatterplots of the four variables, giving the bivariate perspective.

```{r}
#| label: fig-demo-pairs
#| fig-cap: Pairwise scatterplots of the three variables, with anomalies shown in orange.
#| code-fold: false
#| warning: false
#| message: false
GGally::ggpairs(df[, 1:3],
    lower = list(
      continuous = GGally::wrap("points"),
      mapping = aes(color = df$anomaly)
    )
  ) +
  scale_color_manual(values = c("#777777", "#D55E00"))
```

This shows that there are positive relationships between the variables, with the strongest between `V2` and `V3`. One of the anomalies is now clearly separate from the other observations in the plot of `V2` vs `V1` (the top left scatterplot). For this anomaly, the value of `V1` is -2, which is not particularly unusual compared to the other observations. Similarly, the value of `V2` is 3.4, which is also not particularly unusual. But because of their positive correlation, the combination of these two values is unusual, so it shows up in the 2-dimensional scatterplot. The other anomaly does not look particularly unusual in any of the plots.

To visualise trivariate relationships, we need to use a 3d-scatterplot. It is easiest to see this if it can spin. Try dragging the plot around with your mouse to view if from different angles. See if you can find a viewpoint where the second anomaly is clearly separated from the other observations.

```{r}
#| label: rglhook
#| include: false
#| cache: false
knitr::knit_hooks$set(webgl = rgl::hook_webgl)
```

::: {#fig-rgl}

::: {.figure-content}

```{r}
#| webgl: true
#| code-fold: false
rgl::plot3d(df[, 1:3], size = 5,
  col = c(rep("#777777", 998), rep("#D55E00", 2))
)
```

:::

3d scatterplot of the three variables, with anomalies shown in orange.

:::

This second anomaly is not particularly unusual in any combination of two variables, but it is unusual in the combination of all three variables.

When looking for anomalies, it is important to consider all the variables together, rather than looking at each variable in isolation, or even looking at all the pairwise relationships. It is possible for an observation to be unusual in $d$ dimensions, but not unusual in any of the lower-dimensional subsets of variables. In other words, there are more places for anomalies to hide in higher dimensions.

## The curse of dimensionality {#sec-curse}

The "curse of dimensionality" refers to the increasing sparseness of space as the number of dimensions increases.

Suppose we observed a high-dimensional data set of independent U(-1,1) variables, and consider the proportion of points that lie outside the unit sphere (in the corners of the space) as the number of dimensions increases.
First, let's visualise what this means in 1, 2 and 3 dimensions. In 1 dimension, the unit sphere is just the interval [-1,1], so no points lie outside this interval. In 2 dimensions, the unit sphere is the circle with radius 1, sitting snugly within a square of side length 2, as shown in @fig-curse2d.

```{r}
#| label: fig-curse2d
#| fig-cap: 2000 simulated observations from two independent U(-1,1) distributions, with points within the unit circle shown in grey, and points outside the unit circle shown in orange.
#| fig-height: 5
#| fig-width: 5
#| out-width: 70%
tibble(
    x = runif(2000, -1, 1),
    y = runif(2000, -1, 1),
    within_sphere = (x^2 + y^2) < 1
  ) |>
  ggplot(aes(x=x, y=y, color = within_sphere)) +
  geom_point() +
  coord_fixed() +
  scale_color_manual(values = c("#D55E00","#777777")) +
  guides(col = "none")
```

The proportion of points lying outside the circle in the corners of the space (shown in orange) is $`r sprintf("%.1f", 100*(1-pi/4))`$%.

The equivalent plot in 3-dimensions is shown in @fig-curse3d. The unit sphere is now a ball sitting within a box, and $`r sprintf("%.1f", 100*(1-pi/6))`$% of points lie outside the ball.

::: {#fig-curse3d}

::: {.figure-content}

```{r}
#| webgl: true
df <- tibble(
  x = runif(2000, -1, 1),
  y = runif(2000, -1, 1),
  z = runif(2000, -1, 1),
  within_sphere = (x^2 + y^2 + z^2) < 1,
  color = if_else(within_sphere, "#777777", "#D55E00")
)
rgl::plot3d(df[, 1:3], size = 5,
  col = df$color)
```

:::

2000 simulated observations from three independent U(-1,1) distributions, with points within the unit sphere shown in grey, and points outside the unit sphere shown in orange.

:::

As the number of dimensions grows, it becomes increasingly likely that points will lie in the corners of the space. In fact, the proportion of points lying outside the unit sphere in $d$ dimensions is 1 minus the ratio of the volume of the unit sphere to the volume of the whole space. This is given by
$$
1 - \frac{\pi^{d/2}}{2^d\Gamma(d/2 + 1)}
$$
where $\Gamma$ is the gamma function, and is plotted in @fig-curse.

```{r}
#| label: fig-curse
#| fig-cap: For observations from a $d$-dimensional U(-1,1) distribution, the percentage of points lying outside the unit sphere in the corners of the space.
tibble(d = seq(10)) |>
  mutate(outside = 100 * (1 - (pi^(d / 2) / (2^d * gamma(d / 2 + 1))))) |>
  ggplot(aes(x = d, y = outside)) +
  geom_line() +
  labs(x = "Dimensions (d)", y = "Percentage of points outside unit sphere") +
  scale_x_continuous(breaks = seq(10), minor_breaks = NULL)
```

Remarkably, by 10 dimensions, almost all points lie outside the unit sphere, and live in the corners of the space. Almost no points are in the centre of the space. This occurs because, with enough variables, at least one of the univariate observations is going to lie in the tails of the distribution, and so the multivariate observation will not lie near the centre of the space.

Another way to think about this is to consider hypercubes that contain 50% of the observations. These must have sides of length $2^{1-1/d}$, in order for the volume of the hypercube to be $2^{d-1}$, exactly half of the entire space. For $d=1$, the hypercube is a unit interval. For $d=2$, it is a square of side length $\sqrt{2} \approx 1.41$ as shown in @fig-hypercube. For $d=3$, the central cube must have side length $2^{2/3} \approx 1.59$, and so on.

```{r}
#| label: fig-hypercube
#| fig-cap: 2000 simulated observations from two independent U(-1,1) distributions, with the central square containing 50% of the observations.
#| fig-height: 5
#| fig-width: 5
#| out-width: 70%
tibble(
    x = runif(2000, -1, 1),
    y = runif(2000, -1, 1),
    within_cube = (abs(x) < 2^(-1/2)) & (abs(y) < 2^(-1/2))
  ) |>
  ggplot(aes(x=x, y=y, color = within_cube)) +
  geom_point() +
  coord_fixed() +
  scale_color_manual(values = c("#D55E00","#777777")) +
  guides(col = "none")
```

As the dimension $d$ increases, the size of the hypercube containing 50% of the observations must also increase.
@fig-hypercubes shows the side length of these hypercubes plotted against the dimension $d$.

```{r}
#| label: fig-hypercubes
#| fig-cap: For observations from a $d$-dimensional U(-1,1) distribution, the side length of a hypercube which contains 50% of points.
tibble(d = seq(20)) |>
  mutate(length = 2^(1-1/d)) |>
  ggplot(aes(x = d, y = length)) +
  geom_line() + geom_point() +
  labs(x = "Dimensions (d)", y = "Side length of hypercube containing 50% of points") +
  scale_x_continuous(breaks = seq(2, 20, by=2), minor_breaks = NULL) +
  ylim(1,2)
```

The side lengths of these hypercubes increasingly approaches the maximum of 2 as the number of dimensions increases. So the size of the neighbourhood containing 50% of the observations becomes almost as large as the whole space. Therefore, we cannot use methods that rely on local neighbourhoods in high dimensions, as these neighbourhoods must become so large as to no longer be "local".

## Joint probability distributions {#sec-joint}

Now we have a sense of what it means to have data in many dimensions, let's consider how we can describe the joint probability distribution of multiple variables.

### Statistical definitions

Suppose $\bm{Y} = [Y_1,\dots,Y_n]'$ is a random variable taking values in $\mathbb{R}^d$, the $d$-dimensional real numbers. Then the joint distribution of $\bm{Y}$ is defined by the joint cdf
$$
F(\bm{y}) = \text{Pr}(\bm{Y} \le \bm{y}),
$$
while the joint density function is given by
$$
f(\bm{y}) = \frac{\partial^n F(\bm{y})}{\partial y_1 \dots \partial y_d}.
$$

The marginal cdfs are defined by $F_i(y) = \text{Pr}(Y_i \le y)$, with corresponding marginal pdfs given by $f_i(y) = F_i'(y)$, $i=1,\dots,d$.

If the variables are independent, then the joint pdf is the product of the marginal pdfs, $f(\bm{y}) = \prod_{i=1}^d f_i(y_i)$.

The expected value of $\bm{y}$ is given by
$$
\text{E}(\bm{Y}) = \int_{\mathbb{R}^d} \bm{y} f(\bm{y})d\bm{y},
$$
and the covariance matrix is given by
$$
\text{Var}(\bm{Y}) = \text{E}[(\bm{Y}-\text{E}(\bm{Y}))(\bm{Y}-\text{E}(\bm{Y}))'].
$$
The covariance matrix is a $d\times d$ matrix, with $(i,j)$th element given by $\text{Cov}(Y_i,Y_j) = \text{E}[(Y_i-\text{E}(Y_i))(Y_j-\text{E}(Y_j))]$. The diagonal elements are the variances of the individual variables, while the off-diagonal elements are the covariances between the variables.

### Multivariate Normal distribution

If random variable $\bm{Y}$ has a multivariate Normal distribution, we write $\bm{Y} \sim \text{N}(\bm{\mu}, \bm{\Sigma})$, where $\bm{\mu}$ is the mean and $\bm{\Sigma}$ is the covariance matrix.

The multivariate Normal distribution has pdf given by
$$
f(\bm{y}; \bm{\mu}, \bm{\Sigma}) = (2\pi)^{-d/2}|\bm{\Sigma}|^{-1/2} \exp\left\{-\frac{1}{2}(\bm{y}-\bm{\mu})'\bm{\Sigma}^{-1}(\bm{y}-\bm{\mu})\right\}.
$$
The notation $|\bm{\Sigma}|$ denotes the determinant of the matrix $\bm{\Sigma}$.

Multivariate Normal distributions have the interesting property that the marginal distributions are also Normal.

### Further reading

A good reference on multivariate probability distributions is @JKmulticontinuous1.

## Highest density regions

As with univariate distributions, a **highest density region** for a multivariate distribution is defined as the region of the sample space where the density is higher than a given threshold. Suppose we have a multivariate random variable $\bm{Y}$ with a smooth, continuous density function $f$. Then the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{\bm{y}: f(\bm{y}) \ge f_\alpha\}
$$
where $P(\bm{Y} \in R_\alpha) = 1-\alpha$.

HDRs are equivalent to level sets of the density function, and so can be plotted as contours for bivariate density functions. For example, the bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ is shown in @fig-bivariate as a series of HDR contours, each containing an additional 10% of the probability mass.

```{r}
#| label: fig-bivariate
#| message: false
#| fig-cap: Bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
grid <- seq(-3, 3, by = 0.05)
den <- expand_grid(y1 = grid, y2 = grid)
den$density <- mvtnorm::dmvnorm(den, mu, Sigma)
as_kde(den) |>
  autoplot(fill = TRUE) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  )
```

Similarly, we can obtain HDRs for a mixture distribution. Suppose we had two bivariate Normal distributions with means $(0,0)$ and $(3,1)$, and covariance matrices equal to  $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and the identity matrix $\bm{I}_2$ respectively. Then the HDRs for an equal mixture of these two distributions is shown in @fig-bivariate-mixture.

```{r}
#| label: fig-bivariate-mixture
#| message: false
#| fig-cap: Bivariate mixture distribution of two equally weighted Normal components with means $(0,0)$ and $(3,1)$, and covariance matrices $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and $\bm{I}_2$ respectively. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
p1 <- p2 <- 0.5
mu1 <- c(0, 0)
mu2 <- c(3, 1)
Sigma1 <- rbind(c(1, 0.5), c(0.5, 1))
Sigma2 <- diag(2)
# Define the density function on a grid
mixture_density <- expand_grid(y1 = seq(-3, 6, by = 0.05), y2 = seq(-3, 4, by = 0.05))
mixture_density$density <- p1 * mvtnorm::dmvnorm(mixture_density, mu1, Sigma1) +
               p2 * mvtnorm::dmvnorm(mixture_density, mu2, Sigma2)
# Plot the contours
mixture_plot <- as_kde(mixture_density) |>
  autoplot(fill = TRUE) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  )
mixture_plot
```

Here, the 10% and 20% HDRs contain disconnected regions, but for the larger HDRs, there is just one region for each.

## Multivariate quantiles

Unlike the univariate? case, there is no unique definition of a multivariate quantile. There are many different definitions, and each has its own advantages and disadvantages.

In this book, we are mostly concerned with sample multivariate quantiles, and one useful definition for sample quantiles is based on data depth. We will discuss this approach in @sec-depth.


## Robust covariance estimation

The sample covariance matrix is a useful measure of the spread of a multivariate distribution, given by
$$
\bm{S} = \frac{1}{n-1} \sum_{i=1}^n (\bm{y}_i - \bar{\bm{y}})(\bm{y}_i - \bar{\bm{y}})',
$$ {#eq-cov}
However, it is sensitive to outliers, and so is not suitable for our purposes. There have been many robust estimators of covariance proposed in the literature, but we will discuss only one, relatively simple, estimator known as the "orthogonalized Gnanadesikan/Kettenring" (OGK) estimator [@GK72;@MZ02].

Suppose we have two random variables  $X$ and $Y$. Then the variance of their sum and difference is given by
\begin{align*}
  \text{Var}(X+Y) &= \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y) \\
  \text{Var}(X-Y) &= \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X,Y).
\end{align*}
The difference between these two expressions is
$$
  \text{Var}(X+Y) - \text{Var}(X-Y) = 4\text{Cov}(X,Y),
$$
so that the covariance can be expressed as
$$
\text{Cov}(X,Y) = \frac{1}{4} \left[ \text{Var}(X+Y) - \text{Var}(X-Y)\right].
$$
Now we can use a robust estimate of variance, such as one based on the IQR (@sec-robust-univariate), to estimate the two variances on the right hand side, giving
$$
\hat{s}(X,Y) = \frac{1}{4} \left[ s_{\text{IQR}}^2(X+Y) - s_{\text{IQR}}^2(X-Y)\right].
$$
We can repeat this for each pair of variables, to obtain a robust estimate of the covariance matrix, $\bm{S}^*$. The diagonals can be obtained using the same robust measure of variance. This is known as the Gnanadesikan-Kettenring estimator. The resulting matrix is symmetric, but not necessarily positive definite, which is a requirement of a covariance matrix. So some additional iterative steps are applied to "orthogonalize" it.

1. Compute the eigenvector decomposition of $\bm{S^*}$, so that $\bm{S}^* = \bm{U}\bm{\Lambda}\bm{U}^{-1}$.
2. Project the data onto the basis eigenvectors
3. Estimate the variances (robustly) in the coordinate directions.
4. Then the robust covariance matrix is given by
  $$
    \bm{S}_{\text{OGK}} = \bm{U}\bm{\Lambda}^*\bm{U}^{-1},
  $$ {#eq-ogk}
  where $\bm{\Lambda}^*$ is a diagonal matrix with the robust variances on the diagonal.

These orthogonalization steps are usually repeated one more time.

This procedure is implemented in the `covOGK` function in the `robustbase` package [@robustbase].

## Multivariate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate version of @eq-kde is given by [@Scott2015]
$$
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_H(\bm{y} - \bm{y}_i),
$$ {#eq-mkde}
where $K_H$ is a multivariate probability density with covariance matrix $\bm{H}$. In this book, we will use a multivariate Gaussian kernel given by
$$
  K_H(\bm{u}) = (2\pi)^{-d/2} |\bm{H}|^{-1/2} \exp \{-\textstyle\frac12 \bm{u}'\bm{H}^{-1}\bm{u} \}.
$$

### Bivariate kernel density estimation

```{r}
#| label: bivariatebandwidths
#| include: false
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  select(duration, waiting) |>
  mutate(eruption = row_number())
H <- kde_bandwidth(of2021[,1:2])
```

To illustrate the idea, consider a simple bivariate example of 10 observations: the first 10 eruption durations from 2021 that are in the `oldfaithful` data set, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by $\bm{H} = \left[\begin{array}{rr}`r sprintf("%.f", H[1,1])` & `r sprintf("%.f", H[1,2])` \\ `r sprintf("%.f", H[2,1])` & `r sprintf("%.f", H[2,2])`\end{array}\right]$.

```{r}
#| label: fig-ofdw
#| dependson: bivariatebandwidths
#| echo: false
#| warning: false
#| fig-cap: Contours of bivariate kernels centred over the first ten observations in 2021 from the `oldfaithful` data set.
h <- sqrt(diag(H))
k <- expand_grid(
  x = seq(-3 * h[1], 3 * h[1], l = 100),
  y = seq(-3 * h[2], 3 * h[2], l = 100)
) |>
  mutate(z = mvtnorm::dmvnorm(x = cbind(x,y), sigma = H))
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration, y = y + waiting)
ggplot() +
  geom_contour(
    data = of2021kde, aes(x = x, y = y, group = eruption, z = z),
    bins = 4, col = "gray"
  ) +
  geom_point(data = of2021, mapping = aes(x = duration, y = waiting)) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)")
```

If we add the bivariate kernel functions as in @eq-mkde, we obtain the bivariate kernel density estimate shown below. The contours shown correspond to the 10%, 20%, $\dots$, 90% highest density regions (HDRs) of the density estimate. With 10 observations, one of them must lie outside the 90% HDR.

```{r}
#| label: fig-ofbivariate1
#| dependson: fig-ofdw
#| echo: false
#| fig-cap: Bivariate kernel density estimate formed by summing the kernels shown in @fig-ofdw.
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  select(duration, waiting)
of2021 |>
  kde(H = kde_bandwidth(of2021)) |>
  autoplot() +
  geom_point(data = of2021, aes(x = duration, y = waiting)) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)")
```

Now we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors).

We will use the `kde()` function, which uses a bivariate Gaussian kernel, and the bandwidth matrix given by the `kde_bandwidth()` function.

```{r}
#| label: fig-ofbivariate2
#| code-fold: false
#| fig.cap: "Bivariate kernel density estimate with default bandwidths."
of <- oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  select(duration, waiting)
of_density <- of |>
  kde(H = kde_bandwidth(of))
of_density |>
  autoplot() +
  geom_point(aes(duration, waiting), data = of, alpha=0.15) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)")
```

Here we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There are two anomalous eruptions: the one with the 1 second duration, and one where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.

### Bandwidth matrix selection

The optimal bandwidth matrix (minimizing the mean integrated squared error between the true density and its estimate) is of the order $n^{-2/(d+4)}$. If such a bandwidth matrix is used, then the estimator converges at rate $n^{-4/(d+4)}$, implying that kernel density estimation becomes increasingly difficult as the dimension $d$ increases. This is to be expected given the curse of dimensionality (@sec-curse), as the number of observations required to obtain a good estimate increases exponentially with the dimension. In practice, we rarely attempt to estimate a density in more than $d=3$ dimensions.

If the underlying density is Normal with mean $\bm{\mu}$ and variance $\bm{\Sigma}$, then the optimal bandwidth matrix is given by
$$
  \bm{H} = \left(\frac{4}{d+2}\right)^{2/(d+4)} n^{-2/(d+4)} \bm{\Sigma}.
$$ {#eq-gaussianH}
Notice that in the univariate case, when $d=1$, this rule gives the same bandwidth as the rule of thumb given by @eq-nrr.

Replacing $\bm{\Sigma}$ by the sample covariance $\bm{S}$ (@eq-cov), we obtain the normal reference rule, calculated by `Hns()`. If we use the robust covariance matrix $\bm{S}_{\text{OGK}}$ (@eq-ogk) instead, we obtain a robust normal reference rule, calculated by `kde_bandwidth()`. @fig-ofbivariate2 shows a bivariate kernel density estimate computed using this approach.

A more data-driven approach, is the "plug-in" estimator [@chacon2018multivariate] implemented by `Hpi()`, which is a generalization of the plug-in bandwidths popular for univariate kernel density estimation [@sec-kde]. While this typically leads to better estimates of the density as a whole, it can lead to worse estimates in the tails of the distribution where anomalies tend to lie.

A common approach is to specify $\bm{H}$ to be diagonal, with elements equal to the squares of univariate bandwidth rules. (The univariate bandwidths are usually defined as the standard deviation of the kernel, while multivariate bandwidth matrices correspond to the covariance matrix of the kernel. Hence, the univariate bandwidths need to be squared if used in a bandwidth matrix.) That is, if $h_1,\dots,h_d$ are the univariate bandwidths for each of the variables, then the corresponding diagonal bandwidth matrix is given by $\bm{H} = \text{diag}(h_1^2,\dots,h_d^2)$. For example, `geom_density_2d()` uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by the squares of @eq-nrd. However, this approach leads to bandwidths that are too small, as it ignores the convergence properties of the multivariate estimator. Additionally, any diagonal bandwidth matrix implicitly assumes that the variables are uncorrelated, and leads to more biased estimators.

In general, we will use the robust normal reference rule, as implemented by `kde_bandwidth()`, unless otherwise stated.

### Further reading

There is a rich literature on multivariate kernel density estimation. Good starting points are @Scott2015 or @chacon2018multivariate.

## Conditional probability distributions

A fundamental concept in statistics is a **conditional probability distribution**; that is, the distribution of a random variable conditional on the values of other (possibly random) variables.

Almost all statistical modelling involves the estimation of conditional distributions. For example, a regression is a model for the conditional distribution of a response variable given the values of a set of predictor variables. In its simplest form, we assume the conditional distribution is normal, with constant variance, and mean equal to a linear function of the predictor values. Generalized linear models allow for non-normal conditional distributions, while generalized additive models allow for non-linear relationships between the response and the predictors.

The conditional cdf of $Y$ given $X_1,\dots,X_n$ is defined by the conditional cdf
$$
 F(y\mid x_1,\dots,x_n) = \text{Pr}(Y \le y \mid  X_1 = x_1,\dots,X_n = x_n).
$$
The conditional pdf is given by
$$
f(y \mid  x_1, \dots, x_n) = \frac{f(y,x_1,\dots,x_n)}{f(x_1,\dots,x_n)}.
$$
The conditional pdf can be thought of as slices of the joint pdf, with the values of $x_1,\dots,x_n$ fixed, rescaled to ensure the conditional pdfs integrate to 1. For example, $f(y_1 | y_2)$ is equal to a scaled slice of the joint pdf $f(y_1,y_2)$ at $y_2$. @fig-conditional shows some examples for the distribution shown at @fig-bivariate-mixture at several values of $y_2$. The left plot shows the joint density, with horizontal lines indicating where conditioning (or slicing) occurs at different values of $y_2$. The right plot shows the resulting conditional density functions.

```{r}
#| label: fig-conditional
#| message: false
#| depends: fig-bivariate-mixture
#| fig-cap: Conditional distribution of $Y_2|Y_1$, where $(Y_1,Y_2)$ has the joint distribution plotted in @fig-bivariate-mixture. The left plot shows the joint density with the values of $y_2$ where we will condition, while the right plot shows conditional density functions at different values of $y_2$.
# Conditioning points
y2_slice <- -2:3
# Make mixture_density a tibble and grab only slices
mixture_density <- mixture_density |>
  filter(round(y2, 2) %in% round(y2_slice, 2))
# Scaling factor for each density
scale_cond_density <- mixture_density |>
  summarise(scale = sum(density), .by = y2) |>
  mutate(scale = scale / max(scale))
# Scale each conditional density
mixture_density <- mixture_density |>
  left_join(scale_cond_density, by = "y2") |>
  mutate(
    density = density / scale,
    density = density / max(density) * 0.9
  )
# Joint density plot
plot1 <- mixture_plot +
  guides(fill = "none") +
  scale_y_continuous(breaks = y2_slice, minor_breaks = NULL) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6)) +
  geom_hline(aes(yintercept = y2), data = tibble(y2 = y2_slice), color = "#4c93bb")
# Conditional density plots
plot2 <- mixture_density |>
  ggplot(aes(x = y1, y = density + y2, group = y2)) +
  geom_ribbon(aes(ymin = y2, ymax = density + y2, xmin = -2, xmax = 4),
    col = "#4c93bb", fill = "#4c93bb") +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Conditional densities: $f(y_1|y_2)$")
  ) +
  scale_y_continuous(minor_breaks = NULL, breaks = y2_slice) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6))
# Show plots side by side
patchwork::wrap_plots(plot1, plot2, nrow = 1)
```

Another neat property of Normal distributions is that the conditional distribution of a subset of variables is also Normal. For example, suppose $\bm{Y} = (Y_1,Y_2,Y_3)$ is a multivariate Normal random variable. Then the conditional distribution of $Y_1$ given $Y_2$ and $Y_3$ is also Normally distributed.

## Topological data analysis

Topological data analysis (TDA) uses tools from topology to study data. Using TDA, we can infer high-dimensional structure from low-dimensional representations of data such as individual points. For example, one concept from topology is "persistent homology": a method for computing  topological features of a space at different spatial resolutions. Features that persist for a wider range of spatial resolutions represent important, intrinsic features of the data, while features that sporadically change are more likely due to random noise.

### Simplicial complexes {-}

Suppose we have a set of bivariate observations. These observations can be used to construct a graph where the individual points are considered vertices and the edges are determined by the distance between the points. Given a proximity parameter $\varepsilon$, two vertices are connected by an edge if the distance between these two points is less than or equal to $\varepsilon$. Starting from this graph, a simplicial complex --- a space built from simple pieces --- is constructed. A simplicial complex is a finite set of $k$-simplices, where $k$ denotes the dimension; for example, a point is a 0-simplex, an edge a 1-simplex, a triangle a 2-simplex, and a tetrahedron a 3-simplex. Suppose $S$ denotes a simplicial complex that includes a $k$-simplex. Then all non-empty subsets of the $k$-simplex are also included in $S$. For example, if $S$ contains a triangle $pqr$, then the edges $pq$, $qr$ and $rs$, and the vertices $p$, $q$ and $r$, are also in $S$.

The *Vietoris-Rips* complex is one type of $k$-simplicial complex. Given a set of points and a proximity parameter $\varepsilon > 0$, $k+1$ points within a distance of $\varepsilon$ to each other form a $k$-simplex. For example, consider the five points $p$, $q$, $r$, $s$ and $t$ shown on the left of @fig-tetrahedron, and suppose we choose $\varepsilon=0.5$. Then the distance between any two points other than $t$ is less than $\varepsilon$, and the distance between $t$ and any other point is greater than $\varepsilon$. Then we can construct the edges $pq$, $pr$, $ps$, $qr$, $qs$ and $rs$. From the edges $pq$, $qr$ and $rp$ we can construct the triangle $pqr$; from $pq$, $qs$ and $sp$ the triangle $pqs$; and so on, because the distance between any two points $p$, $q$, $r$ and $s$ is bounded by $\varepsilon$. By constructing the four triangles $pqr$, $qrs$, $rsp$ and $spq$ we can construct the tetrahedron $pqrs$. The vertex $t$ is not connected to this 3-simplex because the distance between $t$ and the other vertices is greater than $\varepsilon$. The simplicial complex resulting from these five points consists of the tetrahedron $pqrs$ and all the subset $k$-simplices and the vertex $t$.

```{r}
#| label: fig-tetrahedron
#| fig.cap: "Two examples of Vietoris-Rips complexes. Left: points $p$, $q$, $r$, $s$ and $t$, with a proximity parameter $\\varepsilon = 0.5$. The resulting complex consists of the tetrahedron $pqrs$, triangles $pqr$, $qrs$, $prs$, $pqs$, edges $pq$, $qr$, $rs$, $sp$, $qs$, $pr$, and vertices $p$, $q$, $r$, $s$ and $t$. Right: eight points with $\\varepsilon=1.5$. The resulting complex consists of the triangles $ade$, $acd$, edges $ad$, $ae$, $de$, $ac$, $cd$, $ch$, $bg$, and vertices $a,\\dots,h$."
#| message: false
#| warning: false
library(ggtda)
d <- tibble(
  x = c(0, 0.2, 0, 0.1, 0.75),
  y = c(0, 0.2, 0.2, 0, 0.75),
  point = letters[16:20]
)
prox <- 0.5
p1 <- ggplot(d, aes(x = x, y = y)) +
  coord_fixed() +
  stat_disk(radius = prox/2, fill = "#56B4E9") +
  stat_vietoris0() +
  stat_vietoris1(diameter = prox, col="#D55e00") +
  stat_vietoris2(diameter = prox, fill = "#D55E00") +
  geom_text(aes(label = point), nudge_x = 0.03, nudge_y = 0.03)

set.seed(2021)
X <- tibble(
  x = rnorm(8),
  y = rnorm(8),
  point = letters[1:8]
)
prox <- 1.3
p2 <- ggplot(X, aes(x = x, y = y)) +
  coord_fixed() +
  stat_disk(radius = prox/2, fill = "#56B4E9") +
  stat_vietoris0() +
  stat_vietoris1(diameter = prox, col="#D55e00") +
  stat_vietoris2(diameter = prox, fill = "#D55E00") +
  geom_text(aes(label = point), nudge_x = -0.1, nudge_y = 0.1)
patchwork::wrap_plots(p1, p2, ncol=2)
```

A second example is shown on the right of @fig-tetrahedron, where there are eight points, and  $\varepsilon=1.3$. Here, $f$ is a vertex, disconnected from all other points because it is further than $\varepsilon$ from any point. The pair $g$ and $b$ are connected to each other, but not to any other points. The points $ade$ and $adc$ form connected triangles (but not a tetrahedron), while $h$ is connected to them via $c$.

### Persistent homologies {-}

Given a point cloud of data, the resulting Vietoris-Rips complex depends on the value of the proximity parameter $\varepsilon$. As we increase $\varepsilon$, topological features such as connected components and holes appear and disappear.

Taking the small example on the right of @fig-tetrahedron, we explore what happens as $\varepsilon$ increases from 0.5 to 3.5. On the left-hand side, with a small value of $\varepsilon$, all points are disconnected, and the Vietoris-Rips complex consists of 8 vertices. As $\varepsilon$ increases, more points are connected to each other, and eventually, the complex will consist of a single connected component containing all possible connections up to the 8-simplex.

```{r}
#| label: fig-persistence
#| fig.cap: "Vietoris-Rips complexes resulting from different $\\varepsilon$ values."
#| warning: false
#| fig.width: 8
#| fig.asp: 1
#| out.width: "100%"
prox <- c(0.5, 1.5, 2.5, 3.5)
p <- list()
for(i in seq_along(prox)) {
  p[[i]] <- ggplot(X, aes(x = x, y = y)) +
    coord_fixed() +
    stat_disk(radius = prox[i]/2, fill = "#56B4E9") +
    stat_vietoris0()
  if(prox[i] > 0.5) {
    p[[i]] <- p[[i]] + stat_vietoris1(diameter = prox[i], col="#D55e00")
  }
  p[[i]] <- p[[i]] +
    stat_vietoris2(diameter = prox[i], fill = "#D55E00") +
    geom_text(aes(label = point), nudge_x = -0.1, nudge_y = 0.1) +
    labs(subtitle = latex2exp::TeX(sprintf('$\\epsilon$ = %.1f', prox[i]))) +
    xlim(-4,3) + ylim(-4,3.5)
}
patchwork::wrap_plots(p, nrow = 2)
```

To take a larger, more interesting, example, in @fig-annulus, we start with a point cloud of 50 points sampled uniformly from an annulus. As $\varepsilon$ increases from 0.005 to 1.4, the number of connected components decreases from 50 to 1. At $\varepsilon=0.005$, each point is disconnected from all others, and the Vietoris-Rips complex consists of 50 vertices. As $\varepsilon$ increases, the points start to connect to each other, and the number of connected components decreases. By $\varepsilon=0.7$, the connected components have merged, and the complex consists of a single connected component in the shape of the annulus. As $\varepsilon$ increases further, the hole disappears, and the complex is now in the shape of a ball.

```{r}
#| label: fig-annulus
#| fig.cap: "Vietoris-Rips complexes resulting from different $\\varepsilon$ values."
#| warning: false
#| fig.width: 8
#| fig.asp: 1
#| out.width: "100%"
outer_radius <- 1
inner_radius <- 0.7
n <- 50
rho <- sqrt(runif(n, inner_radius^2, outer_radius^2))
theta <- runif(n, 0, 2 * pi)
X <- tibble(
  x = rho * cos(theta),
  y = rho * sin(theta)
)
prox <- c(0.005, 0.3, 0.7, 1.4)
p <- list()
for(i in seq_along(prox)) {
  p[[i]] <- ggplot(X, aes(x = x, y = y)) +
    coord_fixed() +
    stat_disk(radius = prox[i]/2, fill = "#56B4E9") +
    stat_vietoris0() +
    stat_vietoris1(diameter = prox[i], col="#D55e00") +
    stat_vietoris2(diameter = prox[i], fill = "#D55E00") +
    labs(subtitle = latex2exp::TeX(sprintf('$\\epsilon$ = %.3f', prox[i]))) +
    xlim(-1.75,1.75) + ylim(-1.75,1.75)
}
patchwork::wrap_plots(p, nrow = 2)
```

The appearances and disappearances of these topological features are referred to as births and deaths, and can be illustrated using a *barcode* or a *persistence diagram*.

```{r}
#| include: false
#| label: longblueline
# Pre-computed for the caption of fig-barcodeandpersistence.
library(TDAstats)
ph1 <- calculate_homology(as.matrix(X), dim = 1)
longblueline <- ph1 |>
  as_tibble() |>
  filter(dimension==1) |>
  head(1)
```

```{r}
#| label: fig-barcodeandpersistence
#| dependson: "longblueline"
#| fig-cap: !expr 'sprintf("Left: the barcode of the point cloud in @fig-annulus. Each line denotes a feature, and spans from its birth diameter to its death diameter. The length of the line indicates the persistence of the feature. The 0-dimensional features (in orange) are connected components. The long blue line denotes the 1-dimensional hole that is born at $\\varepsilon = %.2f$ and disappears at $\\varepsilon = %.2f$. Right: the corresponding persistence diagram. Each point denotes a feature, and its coordinates indicate its birth and death diameters. Points far from the diagaonal are the most persistent.", longblueline$birth, longblueline$death)'
#| fig-width: 8
#| fig-height: 3
#| out-width: "100%"
library(TDAstats)
# compute the persistent homology
ph1 <- calculate_homology(as.matrix(X), dim = 1)
# plot topological barcode
bcd <- plot_barcode(ph1)
# plot persistence diagram
pst <- plot_persist(ph1) + xlab("Birth") + ylab("Death")
patchwork::wrap_plots(bcd, pst, nrow = 1)
```

@fig-barcodeandpersistence shows the barcode and the persistence diagram of the point cloud shown in @fig-annulus. The barcode comprises a set of horizontal line segments, each denoting a feature that starts at its birth diameter and ends at its death diameter. These line segments are grouped by their dimension. The orange lines in @fig-barcodeandpersistence denote the connected components, and the blue lines denote holes. The same information is shown in the persistence diagram, where each point corresponds to one line, with the coordinates of the point equal to the birth and death diameters of the feature.

If there are $n$ observations, then there are $n$ connected components (or 0-dimensional features), each born at diameter 0, and which die when the corresponding observation is connected to one or more other observations. The first death occurs when the closest two points merge, so the corresponding two features have the same birth and the same death diameters. Rather than have a repeated bar, only $n-1$ bars are shown. As $\varepsilon$ increases, the connected components disappear one by one, as they merge with neighbouring features, and eventually the number of connected components decreases to 1 when all observations are connected.

The 1-dimensional features are born when a hole appears in the point cloud, and die when the hole disappears. The long blue line in @fig-barcodeandpersistence is born at `r sprintf("%.2f",longblueline$birth)` and dies at `r sprintf("%.2f",longblueline$death)`, and corresponds to the hole at the centre of the point cloud in @fig-annulus.

Features that continue for a large range of $\varepsilon$ represent structural properties of the data that are of interest to us. These points lie well above the diagonal in the persistence diagram, while points closer to the diagonal are probably perturbations related to noise. In this plot, the triangle near the top represents the same feature as the long blue line in the left plot.

### Kernel bandwidth selection using TDA {-}

These topological concepts can be used to determine a bandwidth for a kernel density estimate designed for anomaly detection. First we construct the barcode of the data cloud for dimension zero using Vietoris-Rips complexes with increasing diameter $\varepsilon$. From the barcode we obtain the sequence of death diameters $\{d_i\}_{i=1}^n$ for the connected components.

Consider the example shown in @fig-tdaknn, comprising $n=1000$ observations where most points lie on an annulus, with a few points near the centre. The left panel shows a scatterplot of the data, while the barcodes for the connected components are shown in the centre. The right panel displays only the first 20 barcodes, with the dashed line drawn at the second largest death diameter.

```{r}
#| label: fig-tdaknn
#| fig-cap: "Left: A scatterplot of 1000 observations with most points falling on an annulus and some points near the centre. The other panels show the barcodes for the connected components, with the dashed line drawn at the second largest death diameter. The right panel is a zoomed-in version of the top few barcodes from the centre panel."
#| fig-height: 3
#| fig-width: 8
#| fig-asp: 0.35
oo <- 10
outer_radius <- 1
inner_radius <- 0.7
n <- 1000 - oo
rho <- sqrt(abs(rnorm(n, mean = 5, sd = 1)))
theta <- runif(n, 0, 2 * pi)
X <- bind_rows(
  tibble(
    x = rho * cos(theta),
    y = rho * sin(theta)
  ),
  tibble(
    x = rnorm(oo, mean = 0, sd = 0.2),
    y = rnorm(oo, mean = 0, sd = 0.2)
  )
)
phom <- calculate_homology(X, dim = 0)
top2 <- tail(phom[,"death"],2)

g1 <- ggplot(X, aes(x, y)) +
  geom_point()
g2 <- plot_barcode(phom) +
  geom_vline(xintercept = top2[1], lty = 2) +
  guides(col = "none")
g3 <- plot_barcode(tail(phom, 20)) +
  geom_vline(xintercept = top2[1], lty = 2) +
  guides(col = "none")

patchwork::wrap_plots(g1, g2, g3, nrow = 1)
```

The plot on the right in @fig-tdaknn shows the largest 20 death diameters (out of the `r NROW(phom)` diameters shown in the centre panel). A vertical dashed line is drawn at diameter $h^* = `r sprintf("%.3f",top2[1])`$, the second largest death diameter. The largest death diameter is `r sprintf("%.3f",top2[2])`. Any diameter between these two values gives the same number of connected components. For this data set, $(`r sprintf("%.3f",top2[1])`, `r sprintf("%.3f",top2[2])`)$ is the largest diameter range for which the number of components stays the same. Thus, it signifies a global structural property of the point cloud, and we want a bandwidth that will help us detect this structure. In this example, an appropriate choice would be $\bm{H} = h_*^2\bm{I}$, which ensures that points within a distance of $h_*$ contribute to the kernel density estimate.

The simplest procedure is to first normalize the data matrix $\bm{Y}$ to obtain a new data set $\bm{Y}^* = \bm{S}_{\text{OGK}}^{-1/2} \bm{Y}$, where $\bm{S}_{\text{OGK}}$ is the robust covariance matrix estimate from the OGK algorithm given in @eq-ogk. This normalized data set will have a covariance matrix that is approximately equal to the identity matrix. From $\bm{Y}^*$, we can compute the Vietoris-Rips death diameters, $d_1,\dots,d_n$. These are then ordered to give $d_{(1)},\dots,d_{(n)}$, and we find the largest interval by computing successive differences
$\Delta_i = d_{(i+1)} - d_{(i)}$, for $i=1,\dots,n-1$. Following @lookout2021, we choose $h_* = d_{(i)}$ to be the diameter corresponding to the largest $\Delta_i$. Then we can define the bandwidth matrix to be
$$
  \bm{H} = h_*\bm{S}_{\text{OGK}}.
$$ {#eq-lookoutH}

@fig-kdetda shows the resulting kernel density estimate using this bandwidth. This has effectively captured the annulus structure of the data.

```{r}
#| label: fig-kdetda
#| code-fold: false
#| fig.cap: "Bivariate kernel density estimate with bandwidth chosen using TDA"
annulus_density <- X |>
  kde(H = kde_bandwidth(X, method = "lookout"))
annulus_density |>
  autoplot() +
  geom_point(aes(x, y), data = X, alpha=0.15)
```

We can repeat the bivariate kde for the Old Faithful data, shown in @fig-ofbivariate2, but this time using the bandwidth matrix given by @eq-lookoutH. The resulting density estimate is shown in @fig-kdeogktda. This has captured the two modes of the data, and makes it clear that one is much larger the other; it also highlights the outliers.

```{r}
#| label: fig-kdeogktda
#| code-fold: false
#| fig.cap: "Bivariate kernel density estimate with bandwidth chosen using TDA"
of <- oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  select(duration, waiting)
of_density <- of |>
  kde(H = kde_bandwidth(of, method = "lookout"))
of_density |>
  autoplot(prob = c(0.5, 0.90, 0.99), fill = TRUE, 
           show_points = TRUE, show_mode = TRUE, show_lookout = TRUE) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)")
```

The code above also demonstrates some of the arguments of the `autoplot()` function for `kde` objects. The `prob` argument specifies the probability contours to be drawn, while `fill` determines whether the contours are filled or not. The `show_points`, `show_mode` and `show_lookout` arguments determine whether the data points, mode and anomalies are shown on the plot. We will discuss how the `lookout` anomalies are determined in @sec-lookout.
