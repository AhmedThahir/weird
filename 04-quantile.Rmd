# Quantile and depth-based measures {#ch-quantile}


## Sample quantiles

A quantile is a point that divides the sample space into two based on the probability of observations falling below or above the quantile value. For example, if the quantile corresponding to probability 0.8 is denoted by $q_{0.8}$, and $y$ is a new observation, then $P(y \le q_{0.8}) = 0.8$ and $P(y > q_{0.8}) = 0.2$.

Suppose we have data on a single variable, $\{y_1,\dots,y_n\}$, and we want to estimate a sample quantile. There are a surprising number of ways this can be done, and there is no accepted standard approach. The R function `quantile()` includes nine variations, based on @HF96. Fortunately, which variation you use makes little difference except for tiny data sets where $n$ is very small. So we will simply describe the default approach used by `quantile()` (which is *not* the one recommended by Hyndman & Fan!).

Let $y_{(k)}$ denote the $k$th largest observation, $k=1,\dots,n$. These ordered values are known as "**order statistics**". Then the sample quantile $\hat{q}_p$ is given by linear interpolation between the points $(p_k, y_{(k)})$ where $p_k = (k-1)/(n-1)$. These points divide the range of the data into $n-1$ intervals, and exactly $100p$% of the intervals lie to the left of $\hat{q}_p$ and $100(1-p)$% of the intervals lie to the right of $\hat{q}_p$.

Equivalently, let $k^+ = (n-1)p+1$ and
$$
  \hat{q}_{p} = (1-\gamma) y_{(k)} + \gamma y_{(k+1)},
$$
where $k = \lfloor k^+ \rfloor$ is the integer part of $k^+$ and $\gamma = k^+ - k$ is the remainder.

A few special cases give some well-known summary statistics.

  * When $p=0$, then $k=1$ and $\gamma = 0$, so $\hat{q}_{0}$ is the minimum.
  * When $p=1$, then $k=n$ and $\gamma = 0$, so $\hat{q}_{1}$ is the maximum.
  * When $p=0.5$ and $n$ is even, then $k=n/2$ and $\gamma = 1/2$, so $\hat{q}_{0.5}$ is the average of the middle two observations.
  * When $p=0.5$ and $n$ is odd, then $k=(n+1)/2$ and $\gamma = 0$, so $\hat{q}_{0.5}$ is the middle observation.

### Quartiles, depths and letter values {-}

The sample quantiles $\hat{q}_{0.25}$ and $\hat{q}_{0.75}$ are known as the **quartiles**, and the difference between them is the **IQR** or "interquartile range". The latter is often used as an alternative to the standard deviation for estimating the spread of the data as it is unaffected by any anomalies in the data. When used in this way, the IQR is usually divided by 1.34 to make it an estimate of the standard deviation assuming a normal distribution. The scaling factor will be different for other distributions.

Closely related are the "**fourths**" (or "hinges") introduced by @Tukey1977-xd. These are alternative (simpler) estimates of the quartiles given by
$$
  L_F = y_{(\ell)}
  \qquad\text{and}\qquad
  U_F = y_{(u)},
$$
where $\ell = (\lfloor n/2 \rfloor + 1)/2$ and $u = n+1 - (\lfloor n/2 \rfloor + 1)/2$. These order statistics are based on depth values.

For univariate data, the **depth** of an observation is a measure of how deeply buried it is when all observations are ordered. That is, how far would you need to count from either the smallest or largest observation until you encountered the observation of interest. So the minimum and maximum both have depth 1, while the median has the largest depth of $(n+1)/2$.

Tukey's fourths are at half the depth of the median. In other words, $L_F$ is the median of the observations below the median, while $L_U$ is the median of the observations above the median.

**Letter values** [@Tukey1977-xd;@hoaglin1983letter] are a generalization of fourths. These are order statistics with specific depths, defined recursively starting with the median. The depth of the median is $d_1 = (1+n)/2$. The depths of successive letter values are defined recursively as $d_i = (1+\lfloor d_{i-1}\rfloor)/2$, $i=2,3,\dots$. The corresponding letter values are defined as
$$
  L_i = y_{(\lfloor d_i\rfloor)}
  \qquad\text{and}\qquad
  U_i = y_{(\lfloor n-d_i+1\rfloor)}
$$
when the depth is an integer. Otherwise the depth is an integer plus 1/2, and the letter values are given by
$$
  L_i = (y_{(\lfloor d_i\rfloor)} + y_{(\lfloor d_i\rfloor+1)})/2
  \qquad\text{and}\qquad
  U_i = (y_{(\lfloor n-d_i+1\rfloor)} + y_{(\lfloor n-d_i+1\rfloor+1)})/2 .
$$
Rather than label these using integers ($L_2,L_3,\dots$), Tukey proposed using letters ($L_F,L_E,L_D,\dots$) where $F=$ fourths, $E=$ eighths, $D=$ sixteenths, and so on.

Because each depth is roughly half the previous depth, the lower letter values provide estimates of the quantiles with probabilities $p=\frac{1}{2},\frac14,\frac18,\dots$, while the upper letter values provide estimates of the quantiles with probabilities $p=\frac{1}{2},\frac34,\frac78,\dots$.

### Example: Test cricket batting averages {-}

```{r battingsubset, echo=FALSE}
batave <- cricket_batting %>% filter(Innings > 20)
n <- NROW(batave)
lv <- lvplot::lvtable(batave$Average, k=4)
d <- lv[4:7,"depth"]
quartiles <- sprintf("%2.2f", quantile(batave$Average, seq(0,1,by=0.25)))
kplus <- (n-1)*seq(0,1, by=0.25) + 1
k <- trunc(kplus)
gamma <- kplus - k
```

We will use the batting averages discussed in the previous section as an example. There are $n=`r n`$ observations with a minimum of $\hat{q}_0 = y_{(1)} = `r quartiles[1]`$ and a maximum of $\hat{q}_1 = y_{(`r n`)} = `r quartiles[5]`$. Because there are an odd number of observations, the median is the middle value in the ordered data, which is $\hat{q}_{0.5} = y_{(`r (n+1)/2`)} = `r quartiles[3]`$. The quartiles are given by
$$\hat{q}_{0.25} = 0.5 (y_{(`r k[2]`)} + y_{(`r k[2]+1`)}) = `r quartiles[2]`$$
and
$$\hat{q}_{0.75} = 0.5 (y_{(`r k[4]`)} + y_{(`r k[4]+1`)}) = `r quartiles[4]`.$$

The depths of the first four letter values are given by
$$
  d_1 = `r d[1]`,\quad
  d_2 = `r d[2]`,\quad
  d_3 = `r d[3]`,\quad\text{and}\quad
  d_4 = `r d[4]`,
$$
so that the corresponding letter values are given by
\begin{align*}
 L_1 &= U_1 = y_{(`r d[1]`)} = `r sprintf("%2.2f", lv[4,"LV"])` &\\
 L_F &= (y_{(`r trunc(d[2])`)} + y_{(`r trunc(d[2]+1)`)})/2 = `r sprintf("%2.2f", lv[3,"LV"])` \qquad
 &U_F &= (y_{(`r trunc(n-d[2]+1)`)} + y_{(`r trunc(n-d[2]+1) + 1`)})/2 = `r sprintf("%2.2f", lv[5,"LV"])` \\
 L_E &= y_{(`r trunc(d[3])`)} = `r sprintf("%2.2f", lv[2,"LV"])` \qquad
 &U_E &= y_{(`r trunc(n-d[3]+1)`)} = `r sprintf("%2.2f", lv[6,"LV"])` \\
 L_F &= (y_{(`r trunc(d[4])`)} + y_{(`r trunc(d[4]+1)`)})/2 = `r sprintf("%2.2f", lv[1,"LV"])` \qquad
 &U_F &= (y_{(`r trunc(n-d[4]+1)`)} + y_{(`r trunc(n-d[4]+1) + 1`)})/2 = `r sprintf("%2.2f", lv[7,"LV"])`
\end{align*}
In this example, the fourths are identical to the quartiles, but that is not always the case.

### Anomalies using quantiles

One of the simplest approaches that is sometimes used to find anomalies in univariate data is to identify the points that are above or below a specified quantile value. For example, we might identify anomalies as points above the 0.999 quantile or below the 0.001 quantile. More generally, an anomaly could be defined as points above $\hat{q}_{1-p}$ or below $\hat{q}_{p}$. The problem with that approach is that you need at least $1/p$ observations to find any anomaly, and even with a clean data set you will find $2p$ "anomalous" observations. Further, it doesn't allow us to find points that are inliers but otherwise unusual. So we don't recommend this approach.

## Boxplots

Boxplots were invented by John Tukey as a quick summary of medium sized data sets. They are widely used to identify anomalies, which are shown as separate points in the plot.

Figure \@ref(fig:cricketbox) shows a boxplot of the cricket batting average data, previously shown in  \@ref(fig:cricket1).

```{r cricketbox, fig.asp=0.2, fig.cap="(ref:cricket1)"}
cricket_batting %>%
  filter(Innings > 20) %>%
  ggplot(aes(x = Average, y = 1)) +
  geom_boxplot() +
  scale_y_discrete() +
  labs(y = "", x = "Career batting average")
```

The middle line in the box shows the median, and the ends of the box are the fourths ($L_F$ and $U_F$). Half of all observations lie within the box. The width of the box is called the "interquartile range", defined by $IQR = U_F - L_F$. Any points more than $1.5*IQR$ outside the box are shown as anomalies and appear as separate points in the plot. The "whiskers" that extend out each side of the box show the range of the remaining points.

Originally, Tukey proposed two levels of outliers --- those more than $1.5*IQR$ beyond the box were labelled "outside" values, while those more than $3*IQR$ beyond the box were labelled "far out" values. Most software implementations of boxplots do not distinguish between these groups.

In this cricket batting example, the boxplot works well because the data set is not too large or small, and the distribution of points other than the anomaly is unimodal.

However, boxplots can be misleading, and are they are limited in at least two respects.

1. For large data sets, boxplots show too many points as anomalies, and it is hard to distinguish them.
2. Boxplots assume that the distribution of the data is unimodal.

```{r standardnormal, echo=FALSE}
q1 <- qnorm(0.25)
q3 <- qnorm(0.75)
iqr <- q3 - q1
whisker1 <- q1 - 1.5 * iqr
whisker2 <- q3 + 1.5 * iqr
```

To better understand the first problem, imagine the data comprised $n$ observations from a standard normal distribution N(0,1). Figure \@ref(fig:normalboxplot) shows an example with 10000 points.

```{r normalboxplot, fig.asp=0.2, fig.cap="Boxplot of 10000 draws from a standard normal distribution."}
tibble(x = rnorm(10000)) %>%
  ggplot(aes(x = x, y = 1)) +
  geom_boxplot() +
  scale_y_discrete() +
  labs(y = "")
```

Many anomalies are shown, but since all observations come from a simple distribution, none of them are actually anomalies. For this distribution, Q1 $= `r round(q1,2)`$, Q3 $= `r round(q3,2)`$, IQR $= `r round(iqr,2)`$, and so any observations less than $`r round(q1,2)` - 1.5\times `r round(iqr,2)` = `r round(whisker1, 2)`$ or greater than $`r round(q3,2)` + 1.5\times `r round(iqr,2)` = `r round(whisker2, 2)`$ would be identified as "anomalous" by a boxplot and plotted as separate points. The probability of a standard normal observation being at least $`r round(whisker2, 2)`$ in absolute value is $`r sprintf("%.5f", 1-pnorm(whisker2))`$. So with 10000 observations, we would have $`r round(10000*(1-pnorm(whisker2)))`$ anomalies identified, none of which would be a genuine anomaly.

The second problem is demonstrated using the Old Faithful eruption duration data, shown in Figure \@ref(fig:oldfaithful1). As before, we will omit the largest value so we can see the details in the remaining data.

```{r oldfaithful2a, fig.asp=0.2, fig.cap="Boxplot of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
oldfaithful %>%
  filter(duration < 6000) %>%
  ggplot(aes(x = duration, y = 1)) +
  geom_boxplot() +
  labs(y = "", x = "Duration (seconds)") +
  scale_y_discrete()
```

Quite a few anomalies are shown, including the one-second eruption we identified earlier. But the remaining "anomalies" are not particularly unusual observations. All the points below 180 seconds are identified as anomalies, even though we know that observations in the region between 100 and 140 are not unusual for this geyser. Because the boxplot does not allow for more than one mode, all the points in the second smaller cluster are identified as anomalies. The points around 300 seconds are also not really anomalies --- these are just values in the upper tail of the distribution for eruptions.

## Modified IQR method

```{r modifiediqr, echo=FALSE}
q1 <- qnorm(0.25)
q3 <- qnorm(0.75)
iqr <- q3 - q1
tukey1 <- 2 * (1 - pnorm(q3 + 1.5 * iqr))
tukey2 <- 2 * (1 - pnorm(q3 + 3 * iqr))
```


Under Tukey's boxplot approach to identifying outliers, a regular outlier is more than 1.5IQR beyond the quartiles, while an extreme outlier is more than 3IQR beyond the quartiles. For a normal distribution, the probability of genuine observations lying beyond these thresholds is `r sprintf("%.4f",tukey1)` and `r sprintf("%.7f",tukey2)` respectively, so for very large sample sizes, many spurious anomalies will be identified. Even with 1000 observations, Tukey's approach will find spurious anomalies in a normal distribution with probability `r 1 - (1-tukey1)^1000`.

@Barbato2011 proposed a modification to the boxplot approach to identifying outliers, where the IQR in these thresholds is replaced with IQR$[1+0.1\log(n/10)]$. This allows the limits to increase with the sample size, in a way that controls the probability of false anomalies. Figure \@ref(fig:probanomaly) shows the probability of a regular or extreme anomaly using Tukey's boxplot approach compared to those obtained using the modified IQR approach of @Barbato2011.

```{r probanomaly, echo=FALSE, fig.cap="Probability of anomaly based on the boxplot approach of Tukey, and the modified IQR approach of Barbato et al."}
tibble(n = exp(seq(log(3), log(1e6), l = 100))) %>%
  mutate(
    Tukey1 = 2 * (1 - pnorm(q3 + 1.5 * iqr)),
    Tukey2 = 2 * (1 - pnorm(q3 + 3 * iqr)),
    Barbato1 = 2 * (1 - pnorm(q3 + 1.5 * iqr * (1 + 0.1 * log(n / 10)))),
    Barbato2 = 2 * (1 - pnorm(q3 + 3 * iqr * (1 + 0.1 * log(n / 10))))
  ) %>%
  pivot_longer(Tukey1:Barbato2, names_to = "method", values_to = "probability") %>%
  mutate(
    level = stringr::str_extract(method, "\\d"),
    level = if_else(level == "1", "Regular outlier", "Extreme outlier"),
    method = stringr::str_extract(method, "[A-Za-z]*"),
    level = factor(level, levels = c("Regular outlier", "Extreme outlier")),
    method = factor(method, levels = c("Tukey", "Barbato")),
    probability = 1 - (1 - probability)^n
  ) %>%
  ggplot(aes(x = n, y = probability)) +
  geom_line() +
  facet_grid(level ~ method) +
  labs(y = "Probability of an anomaly in a normal distribution", x = "Sample size") +
  scale_x_log10(
    limits = c(3, 2e6),
    breaks = 10^(1:6),
    minor_breaks = NULL,
    labels = format(10^(1:6), scientific = FALSE, trim = TRUE)
  )
```

Even with a huge sample size, the probability of spotting a genuine anomaly using this modified approach is less than 1/2.

Let's apply this approach to our four examples. First we will write a short function to implement the idea.

```{r barbato4}
barbato_anomaly <- function(y, extreme = FALSE) {
  n <- length(y)
  q1 <- quantile(y, 0.25, na.rm = TRUE)
  q3 <- quantile(y, 0.75, na.rm = TRUE)
  threshold <- (1.5 + 1.5 * extreme) * (q3 - q1) * (1 + log(n / 10))
  return(y > q3 + threshold | y < q1 - threshold)
}
cricket_batting %>%
  filter(Innings > 20) %>%
  filter(barbato_anomaly(Average))
oldfaithful %>% filter(barbato_anomaly(duration))
n01a %>% filter(barbato_anomaly(y))
n01b %>% filter(barbato_anomaly(y))
```

Only the extreme outlier in the duration data is identified as an anomaly.

## Letter value plots

The problem that boxplots have with large data sets was addressed by @Hofmann2017-hz who introduced "letter-value" plots, a variation of boxplots that replace the whiskers with a variable number of letter values. In these plots, each pair of letter values marks the boundaries of a box. The box bounded by the fourths is the same as the box of a boxplot; the additional boxes extend to successive letter values until the quantiles corresponding to the letter values can no longer be estimated sufficiently accurately from the available data.

These can be produced using the `lvplot` package.

```{r cricketboxlv, fig.asp=0.2, fig.cap="Letter value plot of career batting averages for all men and women who played test cricket and batter more than 20 times."}
library(lvplot)
cricket_batting %>%
  filter(Innings > 20) %>%
  ggplot(aes(x = 1, y = Average)) +
  geom_lv(aes(fill = ..LV..)) +
  scale_x_discrete() +
  coord_flip() +
  labs(x = "", y = "Career batting average") +
  theme(legend.key.height = unit(.2, "cm"))
```

```{r lv, echo=FALSE}
lv <- cricket_batting %>%
  filter(Innings > 20) %>%
  stat_lv(mapping = aes(y = Average))
```

Here the median is given by M, the fourths by F, and so on. So the middle box (F) is bounded by the fourths and contains all but 2/4 the data; the next box (E) is bounded by the eighths and contains all but 2/8 of the data; then D is bounded by the sixteenths and contains all but 2/16 of the data; and so on.

In this example, the most extreme box (labelled Z) is bounded by the points that fall within the 1/256 letter values. So it contains all but 2/256 of the data, and shows $2 / 256 \times `r NROW(filter(cricket_batting, Innings > 20))` = `r round(2/256*NROW(filter(cricket_batting, Innings > 20)))`$ points as anomalies.

The stopping rule used in the letter value plot is to show the boxes up to letter value $k$, where
$$
  0.5\sqrt{2d_k} z_{1-\alpha/2} > d_{k+1}
$$
and $z_{1-\alpha/2}$ is the $1-\alpha/2$ quantile of a standard Gaussian distribution. This choice is based on the idea that the edges of the boxes are quantile estimates, and the confidence interval for each  quantile estimate that is displayed should not overlap the subsequent quantile estimate. The Gaussian distribution arises because the quantile estimate has an approximate Gaussian distribution due to the Central Limit Theorem. This stopping rule means that, on average, there should be fewer than $2z^2_{1-\alpha/2}$ legitimate observations in the tails. By default, $\alpha=0.05$, so that, on average, there should be fewer than $2 \times (`r qnorm(1-0.05/2)`)^2 = `r round(2* qnorm(1-0.05/2)^2,1)`$ legitimate observations in the tails, regardless of the size of the data set.

Letter value plots were not designed to detect anomalies, but to be a useful data visualization tool for univariate distributions with large numbers of observations. So the display of legitimate observations in the tails of the distribution is by design, not a flaw.

In this cricketing example, it looks like there is one true anomaly (Don Bradman) and the remaining 8 observations displayed directly are simply in the tails of the distribution of the remaining data.

When applied to the remaining examples, we see approximately 10--20 observations shown as individual points in each case. The code to produce these is very similar to that shown for Figure \@ref(fig:cricketboxlv), so is not reproduced here.

```{r lvplots1, echo=FALSE, fig.asp=0.2, fig.cap="Letter value plot of Old Faithful eruption durations, omitting the very long 2 hour duration."}
oldfaithful %>%
  filter(duration < 7000) %>%
  ggplot(aes(x = 1, y = duration)) +
  geom_lv(aes(fill = ..LV..)) +
  scale_x_discrete() +
  coord_flip() +
  labs(x = "", y = "Eruption durations (seconds)") +
  theme(legend.key.height = unit(.2, "cm"))
```

```{r lvplots2, echo=FALSE, fig.asp=0.2, fig.cap="Letter value plot of 1000 N(0,1) observations."}
n01a %>%
  ggplot(aes(x = 1, y = y)) +
  geom_lv(aes(fill = ..LV..)) +
  scale_x_discrete() +
  coord_flip() +
  labs(x = "") +
  theme(legend.key.height = unit(.2, "cm"))
```

```{r lvplots3, echo=FALSE, fig.asp=0.2, fig.cap="Letter value plot of 19 N(0,1) observations with an anomaly at 4."}
n01b %>%
  ggplot(aes(x = 1, y = y)) +
  geom_lv(aes(fill = ..LV..)) +
  scale_x_discrete() +
  coord_flip() +
  labs(x = "")
```

In this last example, because there are only 20 observations, there is not enough data to estimate the quantiles beyond the fourths. So only the middle box is shown.


## Depth measures


## Bagplots
