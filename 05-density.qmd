# Density-based methods {#sec-density-methods}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

## Kernel density estimation {#sec-kde}

Unlike the examples discussed in @sec-univariate and @sec-multivariate, we will not normally know what distribution our data come from, and they will almost never be from a standard parametric distribution such as a Normal, t, $\chi^2$ or Gamma distribution. Instead, we will need to estimate the probability density function from the data. Kernel density estimation is the most popular method for nonparametric estimation of a probability density function.

Suppose we have $n$ univariate observations, $\{y_1,\dots,y_n\}$, which are independent draws from a probability distribution, and we want to estimate the underlying probability density function. The kernel estimate [see @WJ1995] is given by
$$
  \hat{f}(y) = \frac{1}{n} \sum_{i=1}^n K_h(y-y_i),
$$ {#eq-kde}
where $K_h$ is a "kernel" function and $h$ is a bandwidth to be determined. We will use kernel functions that are themselves probability density functions with mean 0 and standard deviation $h$. Thus, a kernel density estimate is a mixture distribution with $n$ components, each of which is a kernel function centred at one of the observations. For example, the "Gaussian" kernel is $K_h(u) = \exp(-u^2/h^2)/(h\sqrt{2\pi})$, equal to the Normal density function with mean zero and standard deviation $h$. (When we use the Normal density function as a kernel, we will call it a "Gaussian" kernel, to distinguish it from the Normal distribution.) Another popular kernel is the quadratic Epanechnikov kernel given by
$K_h(u) = [1-u^2/(5h^2)]_+ / (h4\sqrt{5}/3)$, where $x_+ = \max(x,0)$.

```{r}
#| label: of2021
#| include: false
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  mutate(eruption = row_number())
s <- sd(of2021$duration)
iqr <- IQR(of2021$duration)
h <- 0.9 * min(s, iqr / 1.349) * NROW(of2021)^(-1 / 5)
```

Now we will apply @eq-kde to the first ten Old Faithful eruption durations from 2021 that are in the `oldfaithful` data set. The kernel density estimate can be visualized as a sum of kernel functions centered over each observation with width determined by $h$, and height given by $K_h(0)/n$.

Let's suppose $K_h$ is a Gaussian kernel and let $h = `r round(h, 1)`$ (I will explain this choice below). Then we get the following set of kernel functions.

```{r}
#| label: fig-kde1
#| fig.asp: 0.2
#| depends: of2021
#| fig-cap: Kernel functions centered over the observations.
k <- tibble(x = seq(-3 * h, 3 * h, l = 1000)) |>
  mutate(y = dnorm(x, 0, h) / 10)
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration)
ggplot() +
  geom_line(data = of2021kde, aes(x = x, y = y, group = eruption), col = "gray") +
  geom_rug(
    data = of2021, mapping = aes(x = duration, y = 0),
    linewidth = 1, length = unit(0.06, "npc"), sides = "b"
  ) +
  labs(x = "y = Duration (seconds)", y = latex2exp::TeX("$K_h(y - y_i)/n$"))
```

The vertical ticks show the location of the ten observations, while the grey lines show $\frac{1}{n}K_h(y - y_i)$ for $i=1,\dots,n$.

These functions are then added together, as in @eq-kde, to give the density estimate.

```{r}
#| label: fig-kde2
#| fig.asp: 0.5
#| dependson: kde1
#| fig-cap: Density estimate formed by summing the kernel functions centered on the observations.
ggplot() +
  geom_line(data = of2021kde, aes(x = x, y = y, group = eruption), col = "gray") +
  geom_rug(data = of2021, mapping = aes(x = duration, y = 0), sides = "b", linewidth = 1) +
  geom_density(data = of2021, mapping = aes(x = duration), bw = h) +
  labs(x = "y = Duration (seconds)", y = "Density")
```

We made two choices when producing this estimate: the value of $h$ and the type of kernel $K_h$. If either was replaced with a different choice, the estimate would be different. For large data sets, it does not make much difference which kernel function is used.

The choice of $h$ is more difficult and will change the shape of the density estimate substantially. Here are three versions of the density estimate with bandwidths given by $h=5$, $h=15$ and $h=40$.

```{r}
#| label: fig-kde3
#| fig.asp: 0.5
#| dependson: of2021
#| fig-cap: Kernel density estimates based on different bandwidth values $h$.
of2021 |>
  ggplot(aes(x = duration)) +
  geom_rug(sides = "b", linewidth = 1) +
  geom_density(bw = 5, col = "#D55E00") +
  geom_density(bw = 15, col = "#0072B2") +
  geom_density(bw = 40, col = "#009E73") +
  labs(x = "y = Duration (seconds)", y = "Density") +
  xlim(120, 320) +
  geom_label(x = 260, y = .02, label = "h = 5", col = "#D55E00") +
  geom_label(x = 275, y = .01, label = "h = 15", col = "#0072B2") +
  geom_label(x = 320, y = .004, label = "h = 40", col = "#009E73")
```

When $h$ is too small, the density estimate is very rough with many peaks and troughs. When $h$ is too large, the density estimate is too smooth and we fail to see any features in the data. A popular choice for $h$ uses Silverman's "rule-of-thumb" [@Silverman1986,p48]:
$$
  h = 0.9 \min(s, \text{IQR}/1.349) n^{-1/5},
$$  {#eq-ruleofthumb}
where $s$ is the sample standard deviation and $\text{IQR} = \hat{Q}(0.75) - \hat{Q}(0.25)$. This tends to work reasonably well for most data sets. For the 10 observations in the example above, it gives $h = `r round(h, 1)`$, which is the value we used.

This example using only 10 observations was purely to illustrate the method. Let's now estimate a kernel density estimate for the full data set (other than that one pesky duration of 2 hours).

We will use the `geom_density()` function, which uses the Gaussian kernel and Silverman's rule-of-thumb by default, although other choices are available.

```{r}
#| label: fig-oldfaithful4
#| fig.asp: 0.45
#| fig.cap: "Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."
oldfaithful |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration)) +
  geom_density() +
  geom_rug() +
  labs(x = "Duration (seconds)")
```

```{r}
#| label: ofbw2
#| include: false
of <- oldfaithful |>
  filter(duration < 7000)
s <- sd(of$duration)
iqr <- IQR(of$duration)
h <- 0.9 * min(s, iqr / 1.349) * NROW(of)^(-1 / 5)
```

As this is a much bigger data set (with `r NROW(of)` observations), the selected bandwidth is smaller and is now $h = `r round(h, 2)`$. Here the estimate has clearly identified the two groups of eruptions, one much larger than the other. The extreme observation of 1 second, and the unusual observations between 140 and 180 seconds are in the areas of low density. Later we will use density estimates at each observation to identify anomalous points.

### Statistical properties {-}

The statistical properties of the kernel density estimator @eq-kde have been extensively studied, and are described in several books including @WJ1995 and @Scott2015.

An important asymptotic result is that the mean square error (MSE) of $\hat{f}(y)$ is
$$
  \text{E}\left[(\hat{f}(y) - f(y))^2\right] \approx
  \frac{1}{4}h^4[f''(y)]^2 + \frac{f(y)R(K)}{nh} ,
$$ {#eq-mse}
where $R(K) = \int K^2(u)du$ is the "roughness" of the kernel function. An estimator is "consistent" if the MSE goes to zero as the sample size $n$ goes to infinity. So @eq-kde gives a consistent estimator of the underlying density $f$ when both terms in @eq-mse go to zero. That is,
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh = \infty,
$$ {#eq-asymptotickde}
and so $h$ should decrease slowly as $n$ increases. Note that these conditions hold for @eq-ruleofthumb.

If we integrate the MSE given by @eq-mse over $y$ (assuming $f$ is sufficiently smooth for the integral to exist), we obtain the mean integrated squared error (MISE) given by
$$
  \text{E}\int \left[(\hat{f}(y) - f(y))^2\right] dy \approx
  \frac{1}{4}h^4R(f'') + \frac{R(K)}{nh} ,
$$ {#eq-mise}
where $R(f'') = \int [f''(y)]^2 dy$ is the roughness of the second derivative of the underlying density.

The optimal overall bandwidth is obtained by minimizing the MISE. This can be calculated by differentiating @eq-mise with respect to $h$ and setting the derivative to zero, yielding
$$
  h = \left(\frac{R(K)}{R(f'')n}\right)^{1/5}.
$$ {#eq-opth}
So the optimal $h$ is proportional to $n^{-1/5}$, which clearly satisfies the conditions @eq-asymptotickde.

However, this value of $h$ depends on the underlying density $f$ which we don't know. For a Normal density $g$ with variance $\sigma^2$, $R(g) = \frac{1}{2\sqrt{\pi}}\sigma^{-1}$ and $R(g'') = \frac{3}{8\sqrt{\pi}}\sigma^{-5}$. So if we use a Gaussian kernel, and assume the underlying density has the same roughness as a Normal density, we obtain the "normal reference rule":
$$
  h = \sigma\left(\frac{4}{3n}\right)^{1/5} = 1.06\sigma n^{-1/5},
$$ {#eq-nrr}
where $\sigma$ is the standard deviation of the underlying density. This is often too large as a Normal distribution is relatively smooth (and so has a relatively low $R(f'')$ value). Silverman proposed replacing 1.06 by 0.9 and $\sigma$ by a robust estimate given by $\min(s, IQR/1.349)$, giving his rule-of-thumb @eq-ruleofthumb.

Another popular bandwidth choice is the Sheather-Jones "plug-in" bandwidth [@SJ91], obtained by replacing $R(f'')$ in @eq-opth by an estimate based on the data.  To use this bandwidth, just set `bw = "SJ"` in `geom_density()`.

Bandwidths obtained in this way are designed to give a good overall estimate of the underlying density, but may not be optimal for any particular point of the density. Our goal is to find anomalies in the data, rather than find a good representation for the rest of the data, and so we are interested in the regions of low density.

If we optimized MSE @eq-mse rather than MISE @eq-mise, we would obtain
$$
h = \left(\frac{f(y)R(K)}{n[f''(y)]^2}\right)^{1/5}.
$$
This shows that larger bandwidths are required when $f(y)/[f''(y)]^2$ is relatively large, which occurs in the extreme tails of a distribution. Often the usual bandwidth selection methods result in bandwidths that are too small and can cause the kernel density estimates of observations in the tails to be confused with anomalies. So bandwidths for anomaly detection tend to be a little larger than bandwidths for other purposes.

We can adjust the bandwidth using the argument `adjust` as follows.

```{r}
#| label: fig-oldfaithful5
#| fig.asp: 0.45
#| fig.cap: "Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."
oldfaithful |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration)) +
  geom_density(adjust = 2) +
  geom_rug() +
  labs(x = "Duration (seconds)")
```

Here we have doubled the default bandwidth obtained using @eq-ruleofthumb.

### Boundaries

When a Gaussian kernel is used, a kernel density estimate assumes that the underlying density $f$ is smooth and non-zero on the whole real line. This will cause problems when the true density is actually zero for some regions of the sample space. For example, if all data must be positive, then $f(u)=0$ for $u<0$.

There are modified estimators which deal with this situation, but we won't concern ourselves with them here as this situation will not come up very often in the context of anomaly detection.

### Multivarate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate version of @eq-kde is given by [@Scott2015]
$$
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_H(\bm{y} - \bm{y}_i),
$$ {#eq-mkde}
where $K_H$ is a multivariate probability density with covariance matrix $\bm{H}$. Whenever we estimate a multivariate kernel density estimate, we will use a multivariate Gaussian kernel given by
$$
  K_H(\bm{u}) = (2\pi)^{-d/2} |\bm{H}|^{-1/2} \exp \{-\textstyle\frac12 \bm{u}'\bm{H}^{-1}\bm{u} \}.
$$

```{r}
#| label: bivariatebandwidths
#| include: false
h1 <- MASS::bandwidth.nrd(of2021$duration)
h2 <- MASS::bandwidth.nrd(of2021$waiting)
```

We will illustrate the idea using a simple bivariate example of 10 observations: the same 10 eruption durations discussed above, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by $\bm{H} = \left[\begin{array}{cc}`r round(h1,0)` & 0 \\ 0 & `r round(h2,0)`\end{array}\right]$.

```{r}
#| label: fig-ofdw
#| dependson: of2021
#| warning: false
#| fig-cap: Contours of bivariate kernels centered over the observations.
h <- c(13, 300)
k <- expand_grid(
    x = seq(-3 * h[1], 3 * h[1], l = 100),
    y = seq(-3 * h[2], 3 * h[2], l = 100)
  ) |>
  mutate(z = dnorm(x, 0, h[1]) * dnorm(y, 0, h[2]))
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration, y = y + waiting)
ggplot() +
  geom_contour(data = of2021kde, aes(x = x, y = y, group = eruption, z = z),
               bins = 4, col = "gray") +
  geom_point(data = of2021, mapping = aes(x = duration, y = waiting)) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

If we add the bivariate kernel functions as in @eq-mkde, we obtain the bivariate kernel density estimate shown below.

```{r}
#| label: fig-ofbivariate1
#| dependson: of2021
#| fig-cap: Bivariate kernel density estimate formed by summing the kernels shown in @fig-ofdw.
of2021 |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point() +
  geom_density_2d(color = "#0072B2") +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

Now we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors).

We will use the `geom_density_2d()` function, which by default uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by @eq-nrr.

```{r}
#| label: fig-ofbivariate2
#| fig.cap: "Bivariate kernel density estimate with default bandwidths."
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d(color = "#0072B2") +
  labs(x = "Duration (seconds)")
```

Here we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There is one anomalous eruption where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.

If $\bm{H}$ is diagonal with values $h_1,\dots,h_d$, then the estimator is consistent when
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh^d = \infty,
$$
where $h = (h_1h_2\dots,h_d)^{1/d}$ is the geometric mean of the diagonal values of $\bm{H}$. The default values for `geom_density_2d()` satisfy this property.

Note that the diagonal values $h_1,\dots,h_d$ tend to be larger than the values used in the corresponding univariate density estimates, as the convergence properties of the estimate are slower for larger $d$. Consequently, the default bandwidths for `geom_density_2d()` tend to be too small. Further, because we are interested in the tails of the distribution, we usually want larger bandwidths than would be suitable for obtaining good estimates of the density function.

@fig-ofbivariate3 shows a bivariate kernel density estimate where the bandwidths are double the default values.

```{r}
#| label: fig-ofbivariate3
#| fig.cap: "Bivariate kernel density estimate with double the default bandwidths."
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d(adjust = 2, color = "#0072B2") +
  labs(x = "Duration (seconds)")
```


### Further reading

There is a rich literature on kernel density estimation. A good starting point is @Scott2015 or @chacon2018multivariate.
## Highest density regions

For data sets of dimension one or two, it is often helpful to visualize the density using highest density regions.

A **highest density region** is defined as the region of the sample space where the density is higher than a given threshold [@HDR96]. Suppose we have a multivariate random variable $\bm{Y}$ with a smooth, continuous density function $f$. Then the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{\bm{y}: f(\bm{y}) \ge f_\alpha\}
$$
where $P(\bm{Y} \in R_\alpha) = 1-\alpha$.

Let's illustrate the idea with some simple examples.

```{r}
#| label: fig-hdr-faithful
#| fig.asp: 0.2
#| warning: false
#| message: false
#| fig-cap: HDR boxplot of the Old Faithful durations
library(gghdr)
oldfaithful |>
  filter(duration < 7000) |>
  ggplot() +
  geom_point(aes(x = 0, y = duration)) +
  geom_hdr_boxplot(aes(y = duration), prob = c(0.5, 0.99)) +
  labs(x = "", y = "Duration (seconds)") +
  coord_flip()
```

Points outside the 99% region are shown separately and indicate possible outliers.

## KDE scores {#kdescores}

A popular way of defining anomalies is that they are observations of low probability. The kernel density estimate at each observation is @eq-mkde
$$
  f_i = \hat{f}(\bm{y}_i) = \frac{1}{n} \sum_{j=1}^n K_H(\bm{y}_i-\bm{y}_j),
$$
and we define the "**kde score**" at each observation as
$$
  s_i = -\log(f_i).
$$
These provide a measure of how anomalous each point is --- anomalies are points where the kde scores are relatively large. The largest possible score occurs when an observation has no other observations nearby. Then $f_i \approx K_H(\bm{0})/n$ because $K_H(\bm{y}_i-\bm{y}_j)\approx 0$ when $|\bm{y}_i-\bm{y}_j|$ is large. So the largest possible kde score, when using a Gaussian kernel, is
$$
  -\log(K_H(\bm{0})/n) \approx \log(n) + \frac{d}{2}\log(2\pi) + \frac{1}{2}\text{log det}(\bm{H}).
$$
For univariate data, when $d=1$, this simplifies to
$$
  -\log(K_h(0)/n) \approx \log(nh\sqrt{2\pi}).
$$

For the Old Faithful eruption duration data, we obtain the following kde scores.

```{r, fig.cap="KDE scores for the Old Faithful eruption durations."}
#| label: fig-ofscores
of_scores <- oldfaithful |>
  mutate(score = kde_scores(duration))
of_scores |> arrange(desc(score))
of_scores |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration, y = score)) +
  geom_point()
```

The two large scores correspond to the extreme 2 hour duration, and the tiny 1 second duration. In both cases, the kde score is at its maximum. The lowest kde scores correspond to the mode of the estimated density.

For data sets of low dimension (say $d \le 3$), this approach works quite well in identifying anomalies. However, as $d$ increases, it becomes increasingly difficult to estimate the density, and so it does not work so effectively for large values of $d$.

## Lookout algorithm

A variation on the idea of kde scores, is to consider estimates of the density at each observation, after removing that observation from the calculation. This is known as "leave-one-out density estimation".

Recall that the kernel density estimate at each observation is (@eq-mkde)
$$
  f_i = \hat{f}(\bm{y}_i) = \frac{1}{n} \sum_{j=1}^n K_H(\bm{y}_i-\bm{y}_j),
$$
and we define the "**kde score**" at each observation as $s_i = -\log(f_i)$. These provide a measure of how anomalous each point is --- anomalies are points where the kde scores are relatively large.

We also define the "leave-one-out" kernel density estimate --- the estimate of $f(\bm{y}_i)$ obtained using all data other than $\bm{y}_i$. Note that the contribution of the $i$th point to the kernel density estimate at that point is $K_H(\bm{0})/n$, so the leave-one-out kernel density estimate is simply $f_{-i} = \left[nf_i - K_H(\bm{0})\right]/(n-1)$.

The "lookout" algorithm (standing for Leave-One-Out Kernel density estimates for OUTlier detection) was proposed by @lookout2021 and uses these kde scores to find the probability of each observation being an anomaly.

In this procedure, we fit a Generalized Pareto Distribution to $s_i$ using the POT approach discussed in @sec-evt, with the $90^{\text{th}}$ percentile as the threshold.

```{r}
#| label: ofpot
#| code-fold: false
of_scores <- oldfaithful |>
  mutate(
    score = kde_scores(duration),
    looscore = kde_scores(duration, loo = TRUE),
  )
threshold <- quantile(of_scores$score, prob = 0.90)
gpd <- evd::fpot(of_scores$score, threshold = threshold)$estimate
```

Now we apply the fitted distribution to the leave-one-out scores to obtain the probability of each observation based on the distribution of the remaining observations.

```{r}
#| label: ofpot2
#| code-fold: false
of_scores |>
  mutate(
    pval = evd::pgpd(looscore,
      loc = threshold,
      scale = gpd["scale"], shape = gpd["shape"], lower.tail = FALSE
    )
  ) |>
  arrange(pval)
```

Low probabilities indicate likely outliers and high probabilities indicate normal points. This procedure has identified the minimum and maximum eruptions as clear outliers, with several other values around 90--93 seconds, 155 seconds and 304--305 seconds as likely outliers as well. These latter observations are in the regions of low density that we have previous noted.

The above code was introduced to illustrate each step of the procedure, but it is simpler to use the `lookout_prob` function.

```{r}
#| label: lookout
#| code-fold: false
cricket_batting |>
  filter(Innings > 20) |>
  mutate(lookout = lookout_prob(Average)) |>
  filter(lookout < 0.05) |>
  select(Player, Average, lookout)
oldfaithful |>
  mutate(lookout = lookout_prob(duration)) |>
  filter(lookout < 0.05)
n01 |>
  mutate(lookout = lookout_prob(v1)) |>
  filter(lookout < 0.05)
n01b <- n01 |>
  select(v2) |>
  head(20)
n01b$v2[20] <- 4
n01b |>
  mutate(lookout = lookout_prob(v2)) |>
  filter(lookout < 0.05)
```

The algorithm has found 6 of the 1000 N(0,1) observations to be possible anomalies, as well as the one true anomaly in the fourth example.

The `lookout` package also implements this method, but uses the Epanechnikov kernel rather than the Gaussian kernel, and selects the bandwidth using a different approach. This idea will be discussed in Chapter ??
