# Distance-based methods {#sec-distance-methods}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
```

## Multivariate distances {#sec-distances}

Euclidean distance

Mahalanobis distance

Manhattan:
: Absolute distance between the two vectors:
$$\|\bm{y}_i - \bm{y}_j\| = \sum_{k=1}^d |y_{ik}-y_{jk}|.$$
This is also known as the $L_1$ distance.  It is called the Manhattan distance as it gives the shortest path  between the corners of city blocks (denoted as points on a grid) when those blocks are rectangular, as they mostly are in Manhattan. For the same reason, it is also sometimes called the "taxicab" distance or the "city block" distance.

Minkowski:
: This generalizes the Manhattan and Euclidean distances to use powers of $p$ to define the $L_p$ distance:
$$\|\bm{y}_i - \bm{y}_j\| = \left(\sum_{k=1}^d (y_{ik}-y_{jk})^p\right)^{1/p}.$$
The value of $p$ is specified using the `p` argument. It is named after the German mathematician Hermann Minkowski.

Maximum:
: Maximum distance between any components of $\bm{y}_i$ and $\bm{y}_j$:
$$\|\bm{y}_i - \bm{y}_j\| = \max_k |y_{ik}-y_{jk}|.$$

Canberra:
: $$\|\bm{y}_i - \bm{y}_j\| = \sum_{k=1}^d \frac{|y_{ik}-y_{jk}|}{|y_{ik}|+|y_{jk}|}.$$
Terms with zero numerator and denominator are omitted from the sum. This distance was introduced by two Australian scientists, Godfrey Lance and Bill Williams, who named it after their home city of Canberra. It scales the distances between components by the size of the components, so there is an inbuilt scaling which avoids problems with variables on different units. However, if there are components which are close to zero, but not identical to zero, the measure is numerically unstable. So it is better to scale the data explicitly before computing a distance, unless the variables cannot take small values.

Binary:
: This is designed for binary vectors where each element is either 0 or 1. The vectors are regarded as binary bits, so non-zero elements are ‘on’ and zero elements are ‘off’. The distance is the proportion of bits in which only one is on amongst those in which at least one is on. This may be useful for distances between logical variables.


## Pairwise distances

Many anomaly detection algorithms are based on pairwise distances between observations. If there are $n$ observations, then there are $n(n-1)/2$ pairwise distances to compute, so this is an $O(n^2)$ operation which can take a very long time for large $n$.

Suppose our observations are denoted by $\bm{y}_1,\dots,\bm{y}_n$. When we have numerical data, we will usually want to use a Euclidean distance, named after the famous Greek mathematician Euclid. That is, the distance between points $\bm{y}_i = (y_{i1},\dots,y_{id})'$ and $\bm{y}_j=(y_{j1},\dots,y_{jd})'$ is given by
$$\|\bm{y}_i - \bm{y}_j\| = \sqrt{\sum_{k=1}^d (y_{ik}-y_{jk})^2}.$$
This is also known as the $L_2$ distance.

For $d=1$ or $d=2$, this is the physical distance between the points when plotted on a strip plot or a scatterplot (provided there is no jittering used).

The `dist()` function will return a distance matrix containing all pairwise distances computed in this way. Here is an example using only the first five observations of the `old_faithful` data set (omitting the time stamp). Because the distances are symmetric, only the lower triangle of the matrix is computed.

```{r}
#| label: dist-1
#| code-fold: false
oldfaithful |>
  select(-time) |>
  head(5) |>
  dist()
```

When the variables have very different scales, the variables with the largest ranges will dominate the distance measures. In this example, durations are much longer than waiting times, and so the duration variable is dominating the calculation of neighbours. Consequently, it is often preferable to scale the data before computing distances.

```{r}
#| label: dist-2
#| code-fold: false
oldfaithful |>
  select(-time) |>
  scale() |>
  head(5) |>
  dist()
```

The `scale()` function subtracts the mean and divides by the standard deviation for each column of data. Therefore, all variables in the resulting scaled data have mean zero and standard deviation equal to 1.

The `dist()` function will also compute other types of distances (discussed in @sec-distances) which may be more appropriate for some kinds of data. These are specified using the `method` argument.

For more information about pairwise distances, see @Borg2005.

## Nearest neighbours

Some algorithms only compute the pairwise distances of the $k$ nearest observations, although finding those observations requires some additional distances to be computed. For some types of distances, efficient solutions are available using kd trees [@Bentley1975;@Arya1998] that find the $k$ nearest neighbours to each observation in $O(n\log(n))$ time.

The calculation of $k$ nearest neighbours is useful for more than anomaly detection problems. It is also the basis of a popular classification method due to the Berkeley statisticians Evelyn Fix and Joe Hodges [@knn] which is often known as the "kNN algorithm".

Suppose we use the Old Faithful data to find eruptions that are neighbours in the (duration, waiting) space. The `dbscan` package uses kd trees to quickly identify the $k$ nearest observations to each eruption. As noted earlier, we will scale the data before computing any distances.

```{r}
#| label: knn
#| code-fold: false
# Find 5 nearest neighbours to each eruption
knn <- oldfaithful |>
  select(duration, waiting) |>
  scale() |>
  dbscan::kNN(k = 5)
# First eruption in the data set
oldfaithful[1, ]
# Five closest observations
oldfaithful[knn$id[1, ], ]
```

For very large data sets, approximations are available which speed up the computation even more, but are less accurate in finding the $k$ nearest neighbours. The `approx` argument specifies a distance tolerance which makes the process faster for large data sets, although the neighbours returned may not be the exact nearest  neighbours.

```{r}
#| label: ann
#| code-fold: false
# Find 5 approximate nearest neighbours to each eruption
kann <- oldfaithful |>
  select(duration, waiting) |>
  scale() |>
  dbscan::kNN(k = 5, approx = 2)
# Five closest observations
oldfaithful[kann$id[1, ], ]
```

Here the fifth closest observation has been omitted, and another nearby observation has been included instead, but otherwise the approximation has identified four of the five nearest observations.

## Local outlier factors

A popular way of using $k$-nearest neighbours for anomaly detection is via local outlier factors [@lof]. This is similar to the idea discussed in \@ref(kdescores) of finding points with low probability density estimates, in that it is designed to find points in areas with few surrounding observations.

Suppose we write the distance between observations $\bm{y}_i$ and $\bm{y}_j$ as $\|\bm{y}_i - \bm{y}_j\|$, and let $d_{i,k}$ be the distance between observation $\bm{y}_i$ and its $k$th nearest neighbour.

Let $N_{i,k}$ be the set of $k$ nearest neighbours within $d_{i,k}$ of $\bm{y}_i$. (If there are multiple observations all exactly $d_{i,k}$ from $\bm{y}_i$, then $N_{i,k}$ may contain more than $k$ observations, but we will ignore that issue here.)

Note that an observation $\bm{y}_j$ may be within $N_{i,k}$ while $\bm{y}_i$ does not fall within $N_{j,k}$, as shown in the diagram below.

```{r}
#| label: fig-njk
#| fig.cap: "A synthetic data set of 11 observations. Nearest neighbourhoods containing five observations each are shown for $\\bm{y}_1$ (in orange) and $\\bm{y}_2$ (in blue). The neighbourhood of $\\bm{y}_1$ contains $\\bm{y}_2$ in its nearest five observations, but $\\bm{y}_2$ does not include $\\bm{y}_1$ in its nearest five observations."
#| warning: false
#| echo: false
set.seed(2)
df <- tibble(
  x = c(rnorm(5), rnorm(6, 5, 1)),
  y = c(rnorm(5), rnorm(6, 5, 1))
)
chosen <- c(3, 11)
knn <- dbscan::kNN(df, 5)$id[chosen, ]
df <- df |>
  mutate(
    d = dbscan::kNNdist(df, 5),
    n15 = if_else(row_number() %in% knn[1, ], "N15", "NO"),
    n25 = if_else(row_number() %in% knn[2, ], "N25", "NO")
  )
df |>
  ggplot() +
  ggforce::geom_circle(aes(x0 = x, y0 = y, r = d, col = c("N15", "N25"), fill = c("N15", "N25")),
    alpha = 0.2, data = df[chosen, ]
  ) +
  geom_point(aes(x = x, y = y), size = 3) +
  # geom_point(aes(x=x, y=y), size=3, data=df[chosen,], col=cols) +
  annotate("text",
    x = df$x[chosen] + 0.3, y = df$y[chosen] + 0.3,
    label = latex2exp::TeX(paste0("$y_", 1:2, "$"), bold = TRUE), col = discrete_colors[1:2]
  ) +
  annotate("text",
    x = c(-0.5, 3.6), y = c(4.2, 6),
    label = latex2exp::TeX(paste0("$N_{", 1:2, ",5}$")), col = discrete_colors[1:2]
  ) +
  geom_segment(aes(x = x, y = y, xend = c(7.75, 8.75), yend = y),
    arrow = arrow(length = unit(0.02, "npc")),
    data = df[chosen, ], col = discrete_colors[1:2]
  ) +
  annotate("text",
    x = c(4.7, 7), y = c(-0.8, 4.1),
    label = latex2exp::TeX(paste0("$d_{", 1:2, ",5}$")), col = discrete_colors[1:2]
  ) +
  coord_fixed(xlim = c(-4, 8.5), ylim = range(df$y)) +
  guides(col = "none", fill = "none") +
  theme_void()
```

Let $r_k(i,j) = \max(d_{j,k}, \|\bm{y}_i-\bm{y}_j\|)$ be the "reachability" of $\bm{y}_i$ from $\bm{y}_j$. Thus, $r_k(i,j)$ is the distance between observations $\bm{y}_i$ and $\bm{y}_j$ when they are far from each other, but is equal to $d_{j,k}$ if $\bm{y}_i$ is one of the $k$ nearest neighbours of $\bm{y}_j$. The reachability is a truncated variant of the usual Euclidean distance $\|\bm{y}_i-\bm{y}_j\|$ so that it is not less than $d_{j,k}$.

The average reachability of $\bm{y}_i$ *from* its nearest neighbours is given by
$$
  \bar{r}_{i,k} = k^{-1} \sum_{\bm{y}_j \in N_{i,k}} r_k(i,j)
$$
This is not the same as the average reachability of the neighbours *from* $\bm{y}_i$ which, by definition, would be $d_{i,k}$. An observation will have high average reachability if it is far from its neighbours, whereas it will have low average reachability if it has many close neighbours.

Then the local outlier factor for observation $\bm{y}_i$ is given by
$$
\ell_{i,k} = \frac{\bar{r}_{i,k}}{k}\sum_{\bm{y}_j \in N_{i,k}} \bar{r}^{-1}_{j,k}.
$$
Thus, it is the ratio between $\bar{r}_{i,k}$ and the average value of $\bar{r}^{-1}_{j,k}$ for points within its neighbourhood.

This provides a relative measure of the density of each observation compared to its neighbours. An observation is regarded as an anomaly if it is in a low-density region (far from its neighbours) while its neighbours are in higher density regions (with many neighbours nearby). A value of $\ell_{i,k}$ much greater than 1 shows that the observation has much larger average reachability compared to its neighbours, and so is regarded as an anomaly.

One problem with this definition is that $\ell_{i,k}$ can be infinite. If the data set contains at least $k+1$ identical observations, then the average reachability $\bar{r}_k(j,k)$ will be 0 if $\bm{y}_j$ is one of the group of identical observations. Consequently, $\ell_{i,k} = \infty$ if $\bm{y}_i$ is a neighbour to one of the group of identical observations.  In this book we use a slightly different definition from that used elsewhere and replace these infinite values with zeros.

Another problem is that it can be difficult to determine an appropriate value for $k$.

### Example: Old Faithful data

```{r}
#| label: fig-ofscores
#| fig-cap: TBC
of_scores <- oldfaithful |>
  mutate(lof = lof_scores(duration, k = 150))
of_scores |> arrange(desc(lof))
of_scores |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration, y = lof)) +
  geom_point()
```

Again, the two large scores correspond to the extreme 2 hour duration and the tiny 1 second duration. The value of $k=150$ has been chosen by trial and error.


## Stray and HDoutliers algorithms

```{r}
#| label: fig-stray
of_scores <- of_scores |>
  mutate(stray = stray_scores(duration))
of_scores |> arrange(desc(stray))
of_scores |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration, y = stray)) +
  geom_point()
```

## Clustering methods

