# Tools

This chapter covers various statistical and computational tools that will be used in subsequent chapters, collected here for easy reference.

## Sample quantiles

A quantile is a point that divides the sample space into two based on the probability of observations falling below or above the quantile value. For example, if the quantile corresponding to probability 0.8 is denoted by $q_{0.8}$, and $y$ is a new observation, then $P(y \le q_{0.8}) = 0.8$ and $P(y > q_{0.8}) = 0.2$.

Suppose we have data on a single variable, $\{y_1,\dots,y_n\}$, and we want to estimate a sample quantile. There are a surprising number of ways this can be done, and no accepted standard approach. The R function `quantile()` includes nine variations, based on @HF96. Fortunately, it makes little difference except for tiny data sets where $n$ is very small. So we will simply describe the default approach used by `quantile()` (which is *not* the one recommended by Hyndman & Fan!).

Let $y_{(k)}$ denote the $k$th largest observation, $k=1,\dots,n$. These ordered values are known as "**order statistics**". Then the sample quantile $\hat{q}_p$ is given by linear interpolation between the points $(p_k, y_{(k)})$ where $p_k = (k-1)/(n-1)$. These points divide the range of the data into $n-1$ intervals, and exactly $100p$% of the intervals lie to the left of $\hat{q}_p$ and $100(1-p)$% of the intervals lie to the right of $\hat{q}_p$.

Equivalently, let $k^+ = (n-1)p+1$ and
$$
  \hat{q}_{p} = (1-\gamma) y_{(k)} - \gamma y_{(k+1)},
$$
where $k = \lfloor k^+ \rfloor$ is the integer part of $k^+$ and $\gamma = k^+ - k$ is the remainder.

A few special cases give some well-known summary statistics.

  * When $p=0$, then $k=1$ and $\gamma = 0$, so $\hat{q}_{0}$ is the minimum.
  * When $p=1$, then $k=n$ and $\gamma = 0$, so $\hat{q}_{1}$ is the maximum.
  * When $p=0.5$ and $n$ is even, then $k=n/2$ and $\gamma = 1/2$, so $\hat{q}_{0.5}$ is the average of the middle two observations.
  * When $p=0.5$ and $n$ is odd, then $k=(n+1)/2$ and $\gamma = 0$, so $\hat{q}_{0.5}$ is the middle observation.

### Quartiles, depths and letter values {-}

The sample quantiles $\hat{q}_{0.25}$ and $\hat{q}_{0.75}$ are known as the **quartiles**. Closely related are the "**fourths**" (or "hinges") introduced by @Tukey1977-xd. These are alternative (simpler) estimates of the quartiles given by
$$
  L_F = y_{(\ell)}
  \qquad\text{and}\qquad
  U_F = y_{(u)},
$$
where $\ell = (\lfloor n/2 \rfloor + 1)/2$ and $u = n+1 - (\lfloor n/2 \rfloor + 1)/2$. These order statistics are based on depth values.

For univariate data, the **depth** of an observation is a measure of how deeply buried it is when all observations are ordered. That is, how far would you need to count from either the smallest or largest observation until you encountered the observation of interest. So the minimum and maximum both have depth 1, while the median has the largest depth of $(n+1)/2$.

Tukey's fourths are at half the depth of the median. In other words, $L_F$ is the median of the observations below the median, while $L_U$ is the median of the observations above the median.

**Letter values** [@Tukey1977-xd;@hoaglin1983letter] are a generalization of fourths. These are order statistics with specific depths, defined recursively starting with the median. The depth of the median is $d_1 = (1+n)/2$. The depths of successive letter values are defined recursively as $d_i = (1+\lfloor d_{i-1}\rfloor)/2$, $i=2,3,\dots$. The corresponding letter values are defined as
$$
  L_i = y_{(\lfloor d_i\rfloor)}
  \qquad\text{and}\qquad
  U_i = y_{(\lfloor n-d_i+1\rfloor)}
$$
when the depth is an integer. Otherwise the depth is an integer plus 1/2, and the letter values are given by
$$
  L_i = (y_{(\lfloor d_i\rfloor)} + y_{(\lfloor d_i\rfloor+1)})/2
  \qquad\text{and}\qquad
  U_i = (y_{(\lfloor n-d_i+1\rfloor)} + y_{n-(\lfloor d_i\rfloor+1)+1})/2 .
$$
Rather than label these using integers ($L_2,L_3,\dots$), Tukey proposed using letters ($L_F,L_E,L_D,\dots$) where $F=$ fourths, $E=$ eighths, $D=$ sixteenths, and so on.

Because each depth is roughly half the previous depth, the letter values provide estimates of the quantiles with probabilities $p=1/2,1/4,1/8,\dots$.

## Kernel density estimation {#sec:kde}

Kernel density estimation is the most popular method for nonparametric estimation of a probability density function.

### Univariate density estimation {-}

Suppose we have $n$ univariate observations, $\{y_1,\dots,y_n\}$ and we want to estimate their probability density function. The kernel estimate is given by
\begin{equation}
  \hat{f}(y) = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{y-y_i}{h}\right),
  (\#eq:kde)
\end{equation}
where $h$ is a bandwidth to be determined and $K$ is a "kernel" function --- a non-negative symmetric function which integrates to 1. That is, we require $K$ to satisfy the properties
\begin{equation}
  K(u) \ge 0, \qquad K(u) = K(-u), \qquad \int K(u) du = 1.
  (\#eq:kernels)
\end{equation}

```{r include=FALSE}
of2021 <- oldfaithful %>%
  filter(as.Date(time) > "2021-01-01")  %>%
  head(10) %>%
  mutate(eruption = row_number())
```

Now we will apply \@ref(eq:kde) to the first ten Old Faithful eruption durations from 2021 that are in the `oldfaithful` data set. The kernel density estimate can be visualized as a sum of kernel functions centered over each observation with width determined by $h$, and height given by $K(0)/nh$.

Let's suppose $K$ is the standard normal density function given by
$$
 K(x) = \frac{1}{\sqrt{2\pi}} e^{x^2},
$$
and let $h=15$. Then we get the following set of kernel functions.

```{r kde1, fig.asp=0.2, echo=FALSE}
h <- 15
k <- tibble(x = seq(-3*h, 3*h, l = 1000)) %>%
  mutate(y = dnorm(x, 0, h)/10)
of2021kde <- of2021 %>%
  mutate(k = list(k)) %>%
  unnest(k) %>%
  mutate(x = x + duration)
ggplot() +
  geom_line(data = of2021kde, aes(x=x,y=y,group=eruption), col='gray') +
  geom_rug(data = of2021, mapping=aes(x=duration, y=0),
           size=1, length=unit(0.06,"npc"), sides='b') +
  labs(x="y = Duration (seconds)", y = latex2exp::TeX("$K((y - y_i)/h)/(nh)$"))
```

The vertical ticks show the location of the ten observations, while the grey lines show $\frac{1}{nh}K((y - y_i)/h)$ for $i=1,\dots,n$.

These functions are then added together, as in \@ref(eq:kde), to give the density estimate.


```{r kde2, fig.asp=0.5, echo=FALSE}
ggplot() +
  geom_line(data = of2021kde, aes(x=x,y=y,group=eruption), col='gray') +
  geom_rug(data = of2021, mapping=aes(x=duration, y=0), sides='b', size=1) +
  geom_density(data = of2021, mapping=aes(x=duration), bw=h) +
  labs(x="y = Duration (seconds)", y = "Density")
```

We made two choices when producing this estimate: the value of $h$ and the type of kernel $K$. If either was replaced with a different choice, the estimate would be different. Any symmetric kernel function that is, itself, a probability density function can be used as it will satisfy the properties \@ref(eq:kernels). For large data sets, it does not make much difference which kernel function is used, and it is common to use a Gaussian kernel or the quadratic Epanechnikov kernel given by
$$
  K(u) = \frac{3}{4}(1-u^2)_+
$$
where $(x)_+ = \max(x,0)$. The `geom_density()` function uses the Gaussian kernel by default, although other choices are available.

The choice of $h$ is more difficult and will change the shape of the density plot substantially. Here are three versions of the density estimate with bandwidths given by $h=5$, $h=15$ and $h=40$.

```{r kde3, fig.asp=0.5, echo=FALSE}
of2021 %>%
  ggplot(aes(x=duration)) +
  geom_rug(sides='b', size=1) +
  geom_density(bw=5, col="#E69F00") +
  geom_density(bw=15, col="#56B4E9") +
  geom_density(bw=40, col="#009E73") +
  labs(x="y = Duration (seconds)", y = "Density") +
  xlim(120,320) +
  geom_label(x=260, y=.02, label="h = 5", col="#E69F00") +
  geom_label(x=275, y=.01, label="h = 15", col="#56B4E9") +
  geom_label(x=320, y=.004, label="h = 40", col="#009E73")
```

When $h$ is too small, the density estimate is very rough with many peaks and troughs. When $h$ is too large, the density estimate is too smooth and we fail to see any features in the data. The default choice in `geom_density()` function uses Silverman's "rule-of-thumb" [@Silverman1986,48]:
$$
  h = 0.9 \min(s, \text{IQR}/1.34) n^{-1/5},
$$
where $s$ is the sample standard deviation and $\text{IQR} = \hat{q}_{0.75} - \hat{q}_{0.25}$. This tends to work reasonably well for most data sets.

The preceding example using only 10 observations was purely to illustrate the method. Let's now estimate a kernel density estimate for the full data set (other than that one pesky duration of 2 hours).

```{r oldfaithful3, fig.asp=0.45, fig.cap="Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
oldfaithful %>%
  filter(duration < 6000) %>%
  ggplot(aes(x=duration)) +
  geom_density() +
  geom_rug() +
  labs(x="Duration (seconds)")
```

Here the estimate has clearly identified the two groups of eruptions, one much larger than the other. The extreme observation of 1 second, and the unusual observations between 140 and 180 seconds are in the areas of low density. Later we will use density estimates at each observation to identify anomalous points.

### Multivarate density estimation {-}

## Depth measures

## Pairwise distances and nearest neighbours

 * Approximate NN

## Extreme value theory

## Principal component analysis

## Multi-dimensional scaling

