# Tools

This chapter covers various statistical and computational tools that will be used in subsequent chapters, collected here for easy reference.

## Sample quantiles

A quantile is a point that divides the sample space into two based on the probability of observations falling below or above the quantile value. For example, if the quantile corresponding to probability 0.8 is denoted by $q_{0.8}$, and $y$ is a new observation, then $P(y \le q_{0.8}) = 0.8$ and $P(y > q_{0.8}) = 0.2$.

Suppose we have data on a single variable, $\{y_1,\dots,y_n\}$, and we want to estimate a sample quantile. There are a surprising number of ways this can be done, and no accepted standard approach. The R function `quantile()` includes nine variations, based on @HF96. Fortunately, which variation you use makes little difference except for tiny data sets where $n$ is very small. So we will simply describe the default approach used by `quantile()` (which is *not* the one recommended by Hyndman & Fan!).

Let $y_{(k)}$ denote the $k$th largest observation, $k=1,\dots,n$. These ordered values are known as "**order statistics**". Then the sample quantile $\hat{q}_p$ is given by linear interpolation between the points $(p_k, y_{(k)})$ where $p_k = (k-1)/(n-1)$. These points divide the range of the data into $n-1$ intervals, and exactly $100p$% of the intervals lie to the left of $\hat{q}_p$ and $100(1-p)$% of the intervals lie to the right of $\hat{q}_p$.

Equivalently, let $k^+ = (n-1)p+1$ and
$$
  \hat{q}_{p} = (1-\gamma) y_{(k)} - \gamma y_{(k+1)},
$$
where $k = \lfloor k^+ \rfloor$ is the integer part of $k^+$ and $\gamma = k^+ - k$ is the remainder.

A few special cases give some well-known summary statistics.

  * When $p=0$, then $k=1$ and $\gamma = 0$, so $\hat{q}_{0}$ is the minimum.
  * When $p=1$, then $k=n$ and $\gamma = 0$, so $\hat{q}_{1}$ is the maximum.
  * When $p=0.5$ and $n$ is even, then $k=n/2$ and $\gamma = 1/2$, so $\hat{q}_{0.5}$ is the average of the middle two observations.
  * When $p=0.5$ and $n$ is odd, then $k=(n+1)/2$ and $\gamma = 0$, so $\hat{q}_{0.5}$ is the middle observation.

### Quartiles, depths and letter values {-}

The sample quantiles $\hat{q}_{0.25}$ and $\hat{q}_{0.75}$ are known as the **quartiles**, and the difference between them is the **IQR** or "interquartile range". The latter is often used as an alternative to the standard deviation for estimating the spread of the data as it is unaffected by any outliers in the data. When used in this way, the IQR is usually divided by 1.34 to make it an estimate of the standard deviation assuming a normal distribution. The scaling factor will be different for other distributions.

Closely related are the "**fourths**" (or "hinges") introduced by @Tukey1977-xd. These are alternative (simpler) estimates of the quartiles given by
$$
  L_F = y_{(\ell)}
  \qquad\text{and}\qquad
  U_F = y_{(u)},
$$
where $\ell = (\lfloor n/2 \rfloor + 1)/2$ and $u = n+1 - (\lfloor n/2 \rfloor + 1)/2$. These order statistics are based on depth values.

For univariate data, the **depth** of an observation is a measure of how deeply buried it is when all observations are ordered. That is, how far would you need to count from either the smallest or largest observation until you encountered the observation of interest. So the minimum and maximum both have depth 1, while the median has the largest depth of $(n+1)/2$.

Tukey's fourths are at half the depth of the median. In other words, $L_F$ is the median of the observations below the median, while $L_U$ is the median of the observations above the median.

**Letter values** [@Tukey1977-xd;@hoaglin1983letter] are a generalization of fourths. These are order statistics with specific depths, defined recursively starting with the median. The depth of the median is $d_1 = (1+n)/2$. The depths of successive letter values are defined recursively as $d_i = (1+\lfloor d_{i-1}\rfloor)/2$, $i=2,3,\dots$. The corresponding letter values are defined as
$$
  L_i = y_{(\lfloor d_i\rfloor)}
  \qquad\text{and}\qquad
  U_i = y_{(\lfloor n-d_i+1\rfloor)}
$$
when the depth is an integer. Otherwise the depth is an integer plus 1/2, and the letter values are given by
$$
  L_i = (y_{(\lfloor d_i\rfloor)} + y_{(\lfloor d_i\rfloor+1)})/2
  \qquad\text{and}\qquad
  U_i = (y_{(\lfloor n-d_i+1\rfloor)} + y_{n-(\lfloor d_i\rfloor+1)+1})/2 .
$$
Rather than label these using integers ($L_2,L_3,\dots$), Tukey proposed using letters ($L_F,L_E,L_D,\dots$) where $F=$ fourths, $E=$ eighths, $D=$ sixteenths, and so on.

Because each depth is roughly half the previous depth, the letter values provide estimates of the quantiles with probabilities $p=1/2,1/4,1/8,\dots$.

## Kernel density estimation {#sec:kde}

Kernel density estimation is the most popular method for nonparametric estimation of a probability density function.

Suppose we have $n$ univariate observations, $\{y_1,\dots,y_n\}$, which are independent draws from a probability distribution, and we want to estimate the underlying probability density function. The kernel estimate [see @WJ1995] is given by
\begin{equation}
  \hat{f}(y) = \frac{1}{n} \sum_{i=1}^n K_h(y-y_i),
  (\#eq:kde)
\end{equation}
where $K_h$ is a "kernel" function and $h$ is a bandwidth to be determined. We will use kernel functions that are probability density functions with mean 0 and standard deviation $h$. For example, $K_h(u) = \exp(-u^2/h^2)/(h\sqrt{2\pi})$ is the Gaussian density function with mean zero and standard deviation $h$. 

```{r of2021, include=FALSE}
of2021 <- oldfaithful %>%
  filter(as.Date(time) > "2021-01-01")  %>%
  head(10) %>%
  mutate(eruption = row_number())
s <- sd(of2021$duration)
iqr <- IQR(of2021$duration)
h <- 0.9 * min(s, iqr/1.34)*NROW(of2021)^(-1/5)
```

Now we will apply \@ref(eq:kde) to the first ten Old Faithful eruption durations from 2021 that are in the `oldfaithful` data set. The kernel density estimate can be visualized as a sum of kernel functions centered over each observation with width determined by $h$, and height given by $K_h(0)/n$.

Let's suppose $K_h$ is a Gaussian density function and let $h = `r round(h, 1)`$ (I will explain this choice below). Then we get the following set of kernel functions.

```{r kde1, fig.asp=0.2, echo=FALSE, dependson="of2021"}
k <- tibble(x = seq(-3*h, 3*h, l = 1000)) %>%
  mutate(y = dnorm(x, 0, h)/10)
of2021kde <- of2021 %>%
  mutate(k = list(k)) %>%
  unnest(k) %>%
  mutate(x = x + duration)
ggplot() +
  geom_line(data = of2021kde, aes(x=x,y=y,group=eruption), col='gray') +
  geom_rug(data = of2021, mapping=aes(x=duration, y=0),
           size=1, length=unit(0.06,"npc"), sides='b') +
  labs(x="y = Duration (seconds)", y = latex2exp::TeX("$K_h(y - y_i)/n$"))
```

The vertical ticks show the location of the ten observations, while the grey lines show $\frac{1}{n}K_h(y - y_i)$ for $i=1,\dots,n$.

These functions are then added together, as in \@ref(eq:kde), to give the density estimate.

```{r kde2, fig.asp=0.5, echo=FALSE, dependson="kde1"}
ggplot() +
  geom_line(data = of2021kde, aes(x=x,y=y,group=eruption), col='gray') +
  geom_rug(data = of2021, mapping=aes(x=duration, y=0), sides='b', size=1) +
  geom_density(data = of2021, mapping=aes(x=duration), bw=h) +
  labs(x="y = Duration (seconds)", y = "Density")
```

We made two choices when producing this estimate: the value of $h$ and the type of kernel $K_h$. If either was replaced with a different choice, the estimate would be different. For large data sets, it does not make much difference which kernel function is used, and it is common to use a Gaussian kernel.

The choice of $h$ is more difficult and will change the shape of the density estimate substantially. Here are three versions of the density estimate with bandwidths given by $h=5$, $h=15$ and $h=40$.

```{r kde3, fig.asp=0.5, echo=FALSE, dependson="of2021"}
of2021 %>%
  ggplot(aes(x=duration)) +
  geom_rug(sides='b', size=1) +
  geom_density(bw=5, col="#E69F00") +
  geom_density(bw=15, col="#56B4E9") +
  geom_density(bw=40, col="#009E73") +
  labs(x="y = Duration (seconds)", y = "Density") +
  xlim(120,320) +
  geom_label(x=260, y=.02, label="h = 5", col="#E69F00") +
  geom_label(x=275, y=.01, label="h = 15", col="#56B4E9") +
  geom_label(x=320, y=.004, label="h = 40", col="#009E73")
```

When $h$ is too small, the density estimate is very rough with many peaks and troughs. When $h$ is too large, the density estimate is too smooth and we fail to see any features in the data. A popular choice for $h$ uses Silverman's "rule-of-thumb" [@Silverman1986,48]:
\begin{equation}
  h = 0.9 \min(s, \text{IQR}/1.34) n^{-1/5},
  (\#eq:ruleofthumb)
\end{equation}
where $s$ is the sample standard deviation and $\text{IQR} = \hat{q}_{0.75} - \hat{q}_{0.25}$. This tends to work reasonably well for most data sets. For the 10 observations in the example above, it gives $h = `r round(h, 1)`$, which is the value we used above.

The preceding example using only 10 observations was purely to illustrate the method. Let's now estimate a kernel density estimate for the full data set (other than that one pesky duration of 2 hours).

We will use the `geom_density()` function, which uses the Gaussian kernel and Silverman's rule-of-thumb by default, although other choices are available.

```{r oldfaithful3, fig.asp=0.45, fig.cap="Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
oldfaithful %>%
  filter(duration < 7000) %>%
  ggplot(aes(x=duration)) +
  geom_density() +
  geom_rug() +
  labs(x="Duration (seconds)")
```

```{r ofbw2, include=FALSE}
of <- oldfaithful %>%
  filter(duration < 7000)
s <- sd(of$duration)
iqr <- IQR(of$duration)
h <- 0.9 * min(s, iqr/1.34)*NROW(of)^(-1/5)
```

As this is a much bigger data set (with `r NROW(of)` observations), the selected bandwidth is smaller and is now $h = `r round(h, 2)`$. Here the estimate has clearly identified the two groups of eruptions, one much larger than the other. The extreme observation of 1 second, and the unusual observations between 140 and 180 seconds are in the areas of low density. Later we will use density estimates at each observation to identify anomalous points.

### Statistical properties {-}

The statistical properties of the kernel density estimator \@ref(eq:kde) have been extensively studied, and are described in several books including @WJ1995 and @Scott2015. Equation \@ref(eq:kde) gives a consistent estimator of the underlying density $f$ when 
\begin{equation}
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh = \infty.
  (\#eq:asymptotickde)
\end{equation}
That is, $h$ should decrease slowly as $n$ increases. 

Another important asymptotic result is that the mean square error (MSE) of $\hat{f}(y)$ is 
\begin{equation}
  \text{E}\left[(\hat{f}(y) - f(y))^2\right] \approx
  \frac{1}{4}h^4[f''(y)]^2 + \frac{f(y)R(K)}{nh} ,
  (\#eq:mse)
\end{equation}
where $R(K) = \int K^2(u)du$ is the "roughness" of the kernel function. If we integrate the MSE over $y$ (assuming $f$ is sufficiently smooth for the integral to exist), we obtain the mean integrated squared error (MISE) given by
\begin{equation}
  \text{E}\int \left[(\hat{f}(y) - f(y))^2\right] dy \approx
  \frac{1}{4}h^4R(f'') + \frac{R(K)}{nh} ,
  (\#eq:mise)
\end{equation}
where $R(f'') = \int [f''(y)]^2 dy$ is the roughness of the second derivative of the underlying density.

The optimal overall bandwidth is obtained by minimizing the MISE. This can be calculated by differentiating \@ref(eq:mise) with respect to $h$ and setting the derivative to zero, yielding
\begin{equation}
  h = \left(\frac{R(K)}{R(f'')n}\right)^{1/5}.
  (\#eq:opth)
\end{equation}
So the optimal $h$ is proportional to $n^{-1/5}$, which clearly satisfies the conditions \@ref(eq:asymptotickde).

However, this value of $h$ depends on the underlying density $f$ which we don't know. For a Gaussian density $g$ with variance $\sigma^2$, $R(g) = \frac{1}{2\sqrt{\pi}}\sigma^{-1}$ and $R(g'') = \frac{3}{8\sqrt{\pi}}\sigma^{-5}$. So if we use a Gaussian kernel, and assume the underlying density has the same roughness as a Gaussian density, we obtain the "normal reference rule":
\begin{equation}
  h = \sigma\left(\frac{4}{3n}\right)^{1/5} = 1.06\sigma n^{-1/5},
  (\#eq:nrr)
\end{equation}
where $\sigma$ is the standard deviation of the underlying density. This is often too large as a Gaussian distribution is relatively smooth (and so has a relatively low $R(f'')$ value). Silverman proposed replacing 1.06 by 0.9 and $\sigma$ by a robust estimate given by $\min(s, IQR/1.34)$, giving his rule-of-thumb \@ref(eq:ruleofthumb). 

Another popular bandwidth choice is the Sheather-Jones "plug-in" bandwidth [@SJ91], obtained by replacing $R(f'')$ in \@ref(eq:opth) by an estimate based on the data.  The resulting bandwidth is often better than \@ref(eq:ruleofthumb). To use this bandwidth, just set `bw = "SJ"` in `geom_density()`.

Bandwidths obtained in this way are designed to give a good overall estimate of the underlying density, but may not be optimal for any particular point of the density. Our goal is to find anomalies in the data, rather than find a good representation for the rest of the data, and so we are interested in the regions of low density. 

If we optimized MSE \@ref(eq:mse) rather than MISE \@ref(eq:mise), we would obtain 
$$
h = \left(\frac{f(y)R(K)}{n[f''(y)]^2}\right)^{1/5}.
$$
This shows that larger bandwidths are required when $f(y)/[f''(y)]^2$ is relatively large, which occurs in the extreme tails of a distribution. Often the usual bandwidth selection methods result in bandwidths that are too small and can cause the kernel density estimates of observations in the tails to be confused with anomalies. So bandwidths for outlier detection tend to be a little larger than bandwidths for other purposes. 

We can adjust the bandwidth using the argument `adjust` as follows.

```{r oldfaithful5, fig.asp=0.45, fig.cap="Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
oldfaithful %>%
  filter(duration < 7000) %>%
  ggplot(aes(x=duration)) +
  geom_density(adjust=2) +
  geom_rug() +
  labs(x="Duration (seconds)")
```

Here we have doubled the default bandwidth obtained using \@ref(eq:ruleofthumb).

### Boundaries

When a Gaussian kernel is used, a kernel density estimate assumes that the underlying density $f$ is smooth and non-zero on the whole real line. This will cause problems when the true density is actually zero for some regions of the sample space. A common situation is when $f(u)=0$ for $u<0$, for example.

There are modified estimators which deal with this situation, but we won't concern ourselves with them here as this situation will not come up very often in the context of anomaly detection.

## Multivarate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate version of \@ref(eq:kde) is given by [@Scott2015]
\begin{equation}
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_h(\bm{y} - \bm{y}_i),
  (\#eq:mkde)
\end{equation}
where $K_h$ is a multivariate probability density with covariance matrix $\bm{H}$. We will use a multivariate Gaussian kernel whenever we estimate a multivariate kernel density estimate.

```{r bivariatebandwidths, include=FALSE}
h1 <- MASS::bandwidth.nrd(of2021$duration)
h2 <- MASS::bandwidth.nrd(of2021$waiting)
```

We will illustrate the idea using a simple bivariate example of 10 observations: the same 10 eruption durations discussed above, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by $\bm{H} = \left[\begin{array}{cc}`r round(h1,0)` & 0 \\ 0 & `r round(h2,0)`\end{array}\right]$.

```{r ofdw, dependson="of2021", echo=FALSE, warning=FALSE}
h <- c(13,300)
k <- expand_grid(
    x = seq(-3*h[1], 3*h[1], l = 100),
    y = seq(-3*h[2], 3*h[2], l = 100)
  ) %>%
  mutate(z = dnorm(x, 0, h[1]) * dnorm(y, 0, h[2]))
of2021kde <- of2021 %>%
  mutate(k = list(k)) %>%
  unnest(k) %>%
  mutate(x = x + duration, y=y+waiting)
ggplot() +
  geom_contour(data = of2021kde, aes(x=x,y=y,group=eruption,z=z), bins=4, col='gray') +
  geom_point(data = of2021, mapping=aes(x=duration, y=waiting)) +
  labs(x="Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147,287) + ylim(3070,6860)
```

If we add the bivariate kernel functions as in \@ref(eq:mkde), we obtain the bivariate kernel density estimate shown below.

```{r ofbivariate1, dependson="of2021", echo=FALSE}
of2021 %>% 
  ggplot(aes(x=duration, y=waiting)) +
  geom_point() +
  geom_density_2d() +
  labs(x="Duration (seconds)", y="Waiting time (seconds)") +
  xlim(147,287) + ylim(3070,6860)
```  

Now we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors). 

We will use the `geom_density_2d()` function, which by default uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by \@ref(eq:nrr).

```{r ofbivariate2, fig.cap="Bivariate kernel density estimate with default bandwidths."}
oldfaithful %>% 
  filter(duration < 7000, waiting < 7000) %>%
  ggplot(aes(x=duration, y=waiting)) +
  geom_point(color='gray') +
  geom_density_2d() +
  labs(x="Duration (seconds)")
```  

Here we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There is one anomalous eruption where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.

If $\bm{H}$ is diagonal with values $h_1,\dots,h_d$, then the estimator is consistent when
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh^d = \infty,
$$
where $h = (h_1h_2\dots,h_d)^{1/d}$ is the geometric mean of the diagonal values of $\bm{H}$. The default values for `geom_density_2d()` satisfy this property.

Note that the diagonal values $h_1,\dots,h_d$ tend to be larger than the values used in the corresponding univariate density estimates, as the convergence properties of the estimate are slower for larger $d$. Consequently, the default bandwidths for `geom_density_2d()` tend to be too small. Further, because we are interested in the tails of the distribution, we usually want larger bandwidths than would be suitable for obtaining good estimates of the density function. 

Figure \@ref(fig:ofbivariate3) shows a bivariate kernel density estimate where the bandwidths are double the default values.

```{r ofbivariate3, fig.cap="Bivariate kernel density estimate with double the default bandwidths."}
oldfaithful %>% 
  filter(duration < 7000, waiting < 7000) %>%
  ggplot(aes(x=duration, y=waiting)) +
  geom_point(color='gray') +
  geom_density_2d(adjust=2) +
  labs(x="Duration (seconds)")
```  

## Highest and lowest density regions

As already indicated, one way to think of an anomaly is a value with low probability density. That leads to the idea of finding all values with density above or below a given threshold.

A **highest density region** is defined as the region of the sample space where the density is higher than a given threshold [@HDR96]. Suppose we have a multivariate random variable $\bm{Y}$ with a smooth, continuous density function $f$. Then the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{\bm{y}: f(\bm{y}) \ge f_\alpha\}
$$
where $P(\bm{Y} \in R_\alpha) = 1-\alpha$.

Let's illustrate the idea with some simple examples.


```{r hdr, eval=FALSE}
# remotes::install_github("ropenscilabs/gghdr")
library(gghdr)
oldfaithful %>%
  filter(duration < 7000) %>%
  ggplot() +
  geom_point(aes(x=0, y=duration)) +
  geom_hdr_boxplot(aes(y=duration), prob=c(0.5, 0.99)) +
  labs(y="Duration (seconds)") +
  coord_flip()
```



## Depth measures

## Pairwise distances and nearest neighbours

 * kNN
 * Approximate NN

## Extreme value theory {#sec:evt}

Extreme Value Theory is used to model rare, extreme events and is useful in anomaly detection. Suppose we have $n$ independent and identically distributed random variables $Y_1, \dots, Y_n$ with a distribution function $F(y) = P\{Y \leq y\}$. Then the maximum of these $n$ random variables is $M_n = \max \{Y_1, \dots, Y_n\}$. If $F$ is known, the distribution of $M_n$ is given by $P\{M_n \leq z \} = \left(F(z)\right)^n$. However, $F$ is usually not known in practice. This gap is filled by Extreme Value Theory, which studies approximate families of models for $F^n$ so that extremes can be modeled and their uncertainty quantified. 

It is well known, due to the central limit theorem, that the average of a set of iid random variables will converge to the normal distribution under relatively weak conditions on the underlying distribution. The Fisher-Tippet-Gnedenko (FTG) Theorem provides an analagous result for the maximum. It was developed in a series of papers by @Frechet1927, @Fisher1928, and @Gnedenko1943. Independently, @Mises1936 proposed a similar result. The FTG Theorem states that if the maximum can be scaled so that it converges, then the scaled maximum will converge to either a Gumbel, Frechet or Weibull distribution [@coles2001introduction, 46]; no other limits are possible. 

### Fisher-Tippett-Gnedenko Theorem {-}

If there exist sequences $\{a_n\}$ and $\{b_n\}$ such that
$$
	P\left\{ \frac{(M_n - a_n)}{b_n} \leq z \right\} \rightarrow G(z) \quad \text{as} \quad n \to \infty,
$$
where $G$ is a non-degenerate distribution function, then $G$ belongs to one of the following families:
\begin{align}\label{eq:EVT3}
	&\text{Gumbel} :  && G(z) = \exp\left(-\exp \left[- \Big(\frac{z-b}{a}\Big) \right] \right), \quad -\infty < z < \infty , \\
	&\text{Fréchet} : && G(z) =
  	\begin{cases}
			0 ,                                                           & z \leq b  , \\
			\exp \left( - \left( \frac{z-b}{a}\right)^{-\alpha} \right) , & z > b    ,
		\end{cases}                     \\
	&\text{Weibull} : && G(z) =
		\begin{cases}
			\exp \left( - \left(- \left[\frac{z-b}{a}\right]\right)^{\alpha} \right) , & z < b  ,    \\
			1 ,                                                                        & z \geq b  ,
		\end{cases}
\end{align}
for parameters $a, b$ and $\alpha$ where $a, \alpha >0$.

These three families of distributions can be further combined into a single family by using the following distribution function known as the Generalized Extreme Value (GEV) distribution,
\begin{equation}\label{eq:EVT4}
	G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\} ,
\end{equation}
where the domain of the function is $\{z: 1 + \xi (z - \mu)/\sigma >0 \}$. The location parameter is $\mu\in\mathbb{R}$, $\sigma>0$ is the scale parameter, while $\xi\in\mathbb{R}$ is the shape parameter. When $\xi = 0$ we obtain a Gumbel distribution with exponentially decaying tails. When $\xi < 0$ we get a Weibull distribution with a finite upper end, and when $\xi > 0$ we get a Fréchet family of distributions with heavy tails including polynomial tails.

If we take the negative of the random variables $Y_1,\dots,Y_n$, it becomes clear that a similar result holds for the minimum.

The three types of limits correspond to different forms of the tail behaviour of $F$. 

  * When $F$ has a finite upper bound, such as with a uniform distribution, then $G$ is a Weibull distribution. 
  * When $F$ has exponential tails, such as with a normal distribution or an exponential distribution, then $G$ is a Gumbel distribution.
  * When $F$ has heavy tails including polynomial decay, then $G$ is a Fréchet distribution. One example is when $F$ itself is a Fréchet distribution with $F(y)= e^{-1/y}$, $y>0$.

To illustrate, suppose $F$ is a standard normal distribution N(0,1) and we have $n=1000$ observations. Then the distribution of the maximum can be obtained via simulation. We will fit both a kernel density estimate and an extreme value distribution to the resulting maximums obtained.

```{r evdexample, fig.cap="Distribution of the maximum of 1000 N(0,1) draws. Here we have simulated 10000 such maximums and shown a kernel density estimate along with the density from an estimated GEV distribution."}
m <- 10000 # Number of simulations
n <- 1000 # Number of observations in each data set
maximums <- numeric(m)
for(i in seq(m))
  maximums[i] <- max(rnorm(n))
gev <- evd::fgev(maximums)$estimate
truedensity <- tibble(y = seq(2,6,l=100)) %>%
  mutate(fy = evd::dgev(y, loc = gev['loc'], scale = gev['scale'], shape = gev['shape']))
tibble(maximum = maximums) %>%
  ggplot(aes(x=maximums)) +
  geom_density() +
  geom_line(data=truedensity, aes(x=y, y=fy), col='red')
```


### The Generalized Pareto Distribution {-}

The Peaks Over Threshold (POT) approach regards extremes as observations greater than a threshold $u$. The probability distribution of *exceedances* above a specified threshold $u$ can be expressed as
\begin{equation}\label{eq:POT3}
	H(y) = P\left \{Y \leq u + y \mid Y > u \right \} 
	= \frac{ F(u+y) - F(u)}{1 - F(u)}.
\end{equation}
When the distribution $F$ satisfies the FTG theorem, then [@coles2001introduction, 75] $H$ is a  **Generalized Pareto Distribution** (GPD) defined by
	\begin{equation}
		H(y) \approx 1 - \Big( 1 + \frac{\xi y}{\sigma_u} \Big)^{-1/\xi} ,
		(\#eq:POT)
	\end{equation}
where the domain of $H$ is $\{y: y >0 \text{ and } (1 + \xi y)/\sigma_u >0 \}$, and $\sigma_u = \sigma + \xi(u- \mu)$. The GPD parameters are determined from the associated GEV parameters. In particular, the shape parameter $\xi$ is the same in both distributions.

Continuing the previous example, we now look at the probability distribution of exceedances above 3 from a N(0,1) distribution. We simulate 1 million N(0,1) values and only keep those above 3. Then a kernel density estimate and two GPD estimates are drawn. The red GPD uses the parameters obtained previously from the GEV estimate, while the blue GPD estimates the parameters from the exceedances. 

```{r pot, dependson="evdexample", fig.cap="Conditional distribution of exceedances above 3 from N(0,1) draws. Here we have simulated 1 million values and only kept those above 3. A kernel density estimate is shown (in black) along with the density implied by the GEV distribution (in red), and an estimated GPD distribution (in blue). The boundary at 3 causes the kernel density estimate to be biased around 3."}
df <- tibble(y = rnorm(1e6)) %>%
  filter(y > 3) 
gpd <- evd::fpot(df$y, 3)$estimate
truedensity <- truedensity %>%
  mutate(
    hy = evd::dgpd(y, loc = 3, scale = gev['scale'], shape = gev['shape']),
    hy2 = evd::dgpd(y, loc = 3, scale = gpd['scale'], shape = gpd['shape'])
  )
df %>% 
  ggplot(aes(x=y)) +
  geom_density() +
  geom_line(data=truedensity, aes(x=y, y=hy), col='red') +
  geom_line(data=truedensity, aes(x=y, y=hy2), col='blue')
```

The red estimate is better because it is based on more information (10000 maximums rather than `r NROW(df)` exceedances). The kernel density estimate (in black) is biased around 3 because of the boundary problem discussed in Section \@ref(sec:kde). Otherwise, the kernel density estimate closely matches the GPD estimates.


## Principal component analysis

## Multi-dimensional scaling

