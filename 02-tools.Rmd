# Tools {#ch-tools}

This chapter covers various statistical and computational tools that will be used in subsequent chapters, collected here for easy reference.

Maybe this will be re-worked into later chapters.

## Extreme value theory {#sec-evt}

Extreme Value Theory is used to model rare, extreme events and is useful in anomaly detection. Suppose we have $n$ independent and identically distributed random variables $Y_1, \dots, Y_n$ with a distribution function $F(y) = P\{Y \leq y\}$. Then the maximum of these $n$ random variables is $M_n = \max \{Y_1, \dots, Y_n\}$. If $F$ is known, the distribution of $M_n$ is given by $P\{M_n \leq z \} = \left(F(z)\right)^n$. However, $F$ is usually not known in practice. This gap is filled by Extreme Value Theory, which studies approximate families of models for $F^n$ so that extremes can be modeled and their uncertainty quantified.

It is well known, due to the central limit theorem, that the average of a set of iid random variables will converge to the normal distribution under relatively weak conditions on the underlying distribution. The Fisher-Tippett-Gnedenko (FTG) Theorem provides an analagous result for the maximum. It was developed in a series of papers by @Frechet1927, @Fisher1928, and @Gnedenko1943. Independently, @Mises1936 proposed a similar result. The FTG Theorem states that if the maximum can be scaled so that it converges, then the scaled maximum will converge to either a Gumbel, Frechet or Weibull distribution [@coles2001introduction, 46]; no other limits are possible.

### Fisher-Tippett-Gnedenko Theorem {-}

If there exist sequences $\{a_n\}$ and $\{b_n\}$ such that
$$
	P\left\{ \frac{(M_n - a_n)}{b_n} \leq z \right\} \rightarrow G(z) \quad \text{as} \quad n \to \infty,
$$
where $G$ is a non-degenerate distribution function, then $G$ belongs to one of the following families:
\begin{align}\label{eq:EVT3}
	&\text{Gumbel} :  && G(z) = \exp\left(-\exp \left[- \Big(\frac{z-b}{a}\Big) \right] \right), \quad -\infty < z < \infty , \\
	&\text{Fréchet} : && G(z) =
  	\begin{cases}
			0 ,                                                           & z \leq b  , \\
			\exp \left( - \left( \frac{z-b}{a}\right)^{-\alpha} \right) , & z > b    ,
		\end{cases}                     \\
	&\text{Weibull} : && G(z) =
		\begin{cases}
			\exp \left( - \left(- \left[\frac{z-b}{a}\right]\right)^{\alpha} \right) , & z < b  ,    \\
			1 ,                                                                        & z \geq b  ,
		\end{cases}
\end{align}
for parameters $a, b$ and $\alpha$ where $a, \alpha >0$.

These three families of distributions can be further combined into a single family by using the following distribution function known as the Generalized Extreme Value (GEV) distribution,
\begin{equation}\label{eq:EVT4}
	G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\} ,
\end{equation}
where the domain of the function is $\{z: 1 + \xi (z - \mu)/\sigma >0 \}$. The location parameter is $\mu\in\mathbb{R}$, $\sigma>0$ is the scale parameter, while $\xi\in\mathbb{R}$ is the shape parameter. When $\xi = 0$ we obtain a Gumbel distribution with exponentially decaying tails. When $\xi < 0$ we get a Weibull distribution with a finite upper end, and when $\xi > 0$ we get a Fréchet family of distributions with heavy tails including polynomial tails.

If we take the negative of the random variables $Y_1,\dots,Y_n$, it becomes clear that a similar result holds for the minimum.

The three types of limits correspond to different forms of the tail behaviour of $F$.

  * When $F$ has a finite upper bound, such as with a uniform distribution, then $G$ is a Weibull distribution.
  * When $F$ has exponential tails, such as with a normal distribution or an exponential distribution, then $G$ is a Gumbel distribution.
  * When $F$ has heavy tails including polynomial decay, then $G$ is a Fréchet distribution. One example is when $F$ itself is a Fréchet distribution with $F(y)= e^{-1/y}$, $y>0$.

To illustrate, suppose $F$ is a standard normal distribution N(0,1) and we have $n=1000$ observations. Then the distribution of the maximum can be obtained via simulation. We will fit both a kernel density estimate and an extreme value distribution to the resulting maximums obtained.

```{r evdexample, fig.cap="Distribution of the maximum of 1000 N(0,1) draws. Here we have simulated 10000 such maximums and shown a kernel density estimate along with the density from an estimated GEV distribution."}
m <- 10000 # Number of simulations
n <- 1000 # Number of observations in each data set
maximums <- numeric(m)
for (i in seq(m)) {
  maximums[i] <- max(rnorm(n))
}
gev <- evd::fgev(maximums)$estimate
truedensity <- tibble(y = seq(2, 6, l = 100)) %>%
  mutate(fy = evd::dgev(y, loc = gev["loc"], scale = gev["scale"], shape = gev["shape"]))
tibble(maximum = maximums) %>%
  ggplot(aes(x = maximums)) +
  geom_density() +
  geom_line(data = truedensity, aes(x = y, y = fy), col = "red")
```


### The Generalized Pareto Distribution {-}

The Peaks Over Threshold (POT) approach regards extremes as observations greater than a threshold $u$. The probability distribution of *exceedances* above a specified threshold $u$ can be expressed as
\begin{equation}\label{eq:POT3}
	H(y) = P\left \{Y \leq u + y \mid Y > u \right \}
	= \frac{ F(u+y) - F(u)}{1 - F(u)}.
\end{equation}
When the distribution $F$ satisfies the FTG theorem, then [@coles2001introduction, 75] $H$ is a  **Generalized Pareto Distribution** (GPD) defined by
	\begin{equation}
		H(y) \approx 1 - \Big( 1 + \frac{\xi y}{\sigma_u} \Big)^{-1/\xi} ,
		(\#eq:POT)
	\end{equation}
where the domain of $H$ is $\{y: y >0 \text{ and } (1 + \xi y)/\sigma_u >0 \}$, and $\sigma_u = \sigma + \xi(u- \mu)$. The GPD parameters are determined from the associated GEV parameters. In particular, the shape parameter $\xi$ is the same in both distributions.

Continuing the previous example, we now look at the probability distribution of exceedances above 3 from a N(0,1) distribution. We simulate 1 million N(0,1) values and only keep those above 3. Then a kernel density estimate and two GPD estimates are drawn. The red GPD uses the parameters obtained previously from the GEV estimate, while the blue GPD estimates the parameters from the exceedances.

```{r pot, dependson="evdexample", fig.cap="Conditional distribution of exceedances above 3 from N(0,1) draws. Here we have simulated 1 million values and only kept those above 3. A kernel density estimate is shown (in black) along with the density implied by the GEV distribution (in red), and an estimated GPD distribution (in blue). The boundary at 3 causes the kernel density estimate to be biased around 3."}
df <- tibble(y = rnorm(1e6)) %>%
  filter(y > 3)
gpd <- evd::fpot(df$y, 3)$estimate
truedensity <- truedensity %>%
  mutate(
    hy = evd::dgpd(y, loc = 3, scale = gev["scale"], shape = gev["shape"]),
    hy2 = evd::dgpd(y, loc = 3, scale = gpd["scale"], shape = gpd["shape"])
  )
df %>%
  ggplot(aes(x = y)) +
  geom_density() +
  geom_line(data = truedensity, aes(x = y, y = hy), col = "red") +
  geom_line(data = truedensity, aes(x = y, y = hy2), col = "blue")
```

The red estimate is better because it is based on more information (10000 maximums rather than `r NROW(df)` exceedances). The kernel density estimate (in black) is biased around 3 because of the boundary problem discussed in Section \@ref(sec-kde). Otherwise, the kernel density estimate closely matches the GPD estimates.
