# Time series data {#sec-noneuclidean}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(11 / .step))
  ) |>
    group_by(.id) |>
    mutate(
      observation = case_when(
        time <= ((min(.id) - 1) * .step + .init) ~ "train",
        time %in% c((min(.id) - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) |>
    ungroup() |>
    filter(.id <= 26 - .init) |>
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", linewidth = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#c14b14", unused = "gray")) +
    guides(col = "none") +
    labs(x = "time", y = "") +
    theme_void() +
    theme(axis.title.x = element_text(margin = margin(t = 2))) +
    theme(text = ggplot2::element_text(family = "Fira Sans"))
}

pbs_plot <- function(pbs, anomalies, series) {
  dates <- anomalies |>
    dplyr::filter(ATC2 == series) |>
    dplyr::pull(Month) |>
    as.Date()
    pbs |>
      dplyr::filter(ATC2 == series) |>
      ggplot2::ggplot(ggplot2::aes(x = Month, y = Scripts)) +
      tsibble::scale_x_yearmonth(date_breaks = "2 years", date_labels = "%Y") +
      ggplot2::geom_vline(xintercept = dates, color = "red", alpha = 0.4) +
      ggplot2::geom_line() +
      ggplot2::labs(title = paste("Scripts for ATC group", series))
}
```

For this chapter, we will load some additional packages designed to work with time series data. To learn more about these packages and their use in time series analysis, see @fpp3.

```{r}
#| label: tsibble
#| code-fold: false
library(tsibble)
library(fable)
library(feasts)
```

```{r}
#| label: egdata
#| include: false
# Loaded again later, but needed here for the paradigm examples
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
```

## Time series anomaly detection paradigms

Time series data are observations that are collected over time. In this book, we will only consider time series which are observed at regular intervals, such as annually, monthly, or hourly.

When considering time series data, we can distinguish between four different anomaly detection paradigms:

1. **Weird times**: Identifying anomalies within a time series in historical data.
1. **Weird subseries**: aka contextual anomalies
1. **Weird series**: Identifying an anomalous time series within a collection of time series.
1. **Surveillance**: Identifying anomalies within a time series in real time

These can be illustrated using the following examples.

```{r}
#| label: fig-tsparadigms
#| echo: false
#| fig-cap: The three main anomaly detection paradigms. In the top panel, we aim to identify an unusual observation in the next time period. In the middle panel, we aim to identify unusual observations within historical data. In the bottom panel, we aim to identify an unusual time series within a collection of time series.
a12 <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup() |>
  filter(ATC2 == "A12", Month <= yearmonth("2006 Feb")) |>
  mutate(paradigm = "Surveillance")
p1 <- fr_mortality |>
  filter(Age == 25, Sex == "Male") |>
  mutate(paradigm = "Weird times") |>
  autoplot(Mortality) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)
p2 <- tibble(x = 0, y=0) |> 
  mutate(paradigm = "Weird subseries") |>
  ggplot() +
  facet_grid(paradigm ~ .) 
p3 <- fr_mortality |>
  filter(Age %in% c(2:6, 60), Sex == "Female") |>
  mutate(paradigm = "Weird series") |>
  autoplot(Mortality) +
  guides(col = "none") +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)
p4 <- a12 |>
  autoplot(Scripts) +
  geom_point(data = tail(a12, 1)) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_continuous(breaks = NULL)

patchwork::wrap_plots(p1, p2, p3, p4, ncol = 1)
```

**Weird times**: In the top plot, we are looking for historical anomalies within time series data. This is common in quality control, or in data cleaning. The aim is to find time periods where the observations are different from the rest of the data. Again, it can easily be extended to multivariate time series, where we look for time periods when one or more of the series may display unusual observations

**Weird subseries**: In the second plot, we illustrate the case of anomalous subseries, which is similar to identifying anomalous times, but where the anomalous period spans several times. Here, we are looking for sections of a time series that appear very different from the rest of the series. This could happen, for example, when there is an event that affects multiple consecutive time periods

**Weird series**: In the third plot, we are looking for an anomalous time series within a collection of time series. The observations within each series may all be consistent, but the series as a whole may be unusual. This is, by its nature, a multivariate problem. It is common in finance, where we look for unusual behaviour of a stock within a collection of stocks, or in health, where we look for unusual behaviour of a patient within a collection of patients.

**Surveillance**: In the bottom plot, we are looking for an anomaly in the next observation in the time series. This is commonly done in surveillance, when a time is monitored in real time to identify an unusual observation, and appropriate action taken. An anomaly in this context, is an observation that is very different from what was forecast. Although only a single time series is shown in this plot, the idea easily extends to multivariate time series, where we look for something unusual occurring in the next time period across a set of time series.

We will discuss each of these paradigms in turn.


## Weird times


```{r}
#| label: fr_mortality
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
fr_mortality
```

```{r}
#| label: fr_mortality_time_plots
fr_mortality |>
  ggplot(aes(
    x = Year, y = Mortality,
    color = as.factor(Age), group = Age
  )) +
  geom_line() +
  facet_grid(. ~ Sex) +
  scale_y_log10() +
  theme(legend.position = "none")
```

```{r}
#| label: fr_fit
fr_fit <- fr_mortality |>
  model(stl = STL(log(Mortality)))

fr_sigma <- augment(fr_fit) |>
  as_tibble() |>
  group_by(Age, Sex) |>
  summarise(sigma = IQR(.innov) / 1.349, .groups = "drop")

fr_scores <- augment(fr_fit) |>
  as_tibble() |>
  left_join(fr_sigma) |>
  mutate(
    s = -log(dnorm(.innov / sigma)),
    prob = lookout(density_scores = s, threshold_probability = 0.9)
  ) |>
  select(-.model, -.resid, -.fitted, -sigma)
```

```{r}
#| label: fr_scores3
fr_scores |> arrange(prob)

fr_anomalies <- fr_scores |>
  filter(prob < 0.05) |>
  as_tibble() |>
  select(Year, Sex, Age) |>
  distinct() |>
  left_join(fr_mortality)
yrs <- fr_anomalies |>
  select(Year, Sex) |>
  distinct()
```


* 1870--1872: Franco-Prussian war and repression of the ‘Commune de Paris’
* 1914--1918: World War I
* 1918: Spanish flu
* 1939--1945: World War II

```{r}
#| label: fr_25
fr_anomalies_plot_male2 <- fr_anomalies |>
  filter(Sex == "Male") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Male"]),
    alpha = 0.5, color = "grey"
  ) +
  geom_point(col = "#478cb2") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Male", ],
    aes(y = 75, label = Year), col = "#478cb2", size = 3, seed = 1967
  ) +
  ylim(4, 85)
fr_anomalies_plot_female2 <- fr_anomalies |>
  filter(Sex == "Female") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Female"]),
    alpha = 0.5, color = "grey"
  ) +
  labs(title = "French mortality anomalies") +
  geom_point(col = "#c1653a") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Female", ],
    aes(y = 75, label = Year), col = "#c1653a", size = 3, seed = 1967
  ) +
  ylim(4, 85)
patchwork::wrap_plots(
  fr_anomalies_plot_female2 +
    geom_hline(yintercept = 25, color = "gray"),
  fr_anomalies_plot_male2 +
    geom_hline(yintercept = 25, color = "gray") +
    annotate("text", x = 1890, y = 27, label = "Age 25", family = "Fira Sans"),
  nrow = 1
)

fr_mortality |>
  filter(Age == 25) |>
  ggplot(aes(x = Year, y = Mortality, color = Sex)) +
  geom_line() +
  facet_grid(Sex ~ ., scales = "free_y") +
  geom_point(data = fr_anomalies |> filter(Age == 25)) +
  labs(title = "French mortality: Age 25")
```

## Weird subseries


## Weird series

To illustrate the ideas we will use a data set of monthly observations on
the Australian Pharmaceutical Benefits Scheme (PBS), from July 1991 to June 2008. The PBS involves the Australian government subsidising certain pharmaceutical products, to allow more equitable access to essential medicines. The data set contains monthly sales volumes of those products being subsidised, classified according to the Anatomical Therapeutic Chemical (ATC) classification system. 


```{r}
#| label: weirdseries
#| message: false
#| warning: false
library(broom)
library(tsibbledata)

## Compute features (and omit series with missing features)
PBS_feat <- PBS |>
  features(Cost, feature_set(pkgs = "feasts")) |>
  select(-`...26`) |>
  na.omit()

## Compute principal components
PBS_prcomp <- PBS_feat |>
  select(-Concession, -Type, -ATC1, -ATC2) |>
  prcomp(scale = TRUE) |>
  augment(PBS_feat)

## Plot the first two components
PBS_prcomp |>
  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +
  geom_point()

## Pull out most unusual series from first principal component
outliers <- PBS_prcomp |>
  filter(.fittedPC1 > 6)
outliers |>
  select(ATC1, ATC2, Type, Concession)

## Visualise the unusual series
PBS |>
  semi_join(outliers, by = c("Concession", "Type", "ATC1", "ATC2")) |>
  autoplot(Cost) +
  facet_grid(vars(Concession, Type, ATC1, ATC2)) +
  labs(title = "Outlying time series in PC space")
```

These series are either all zeros, or have a single non-zero observation.


## Surveillance

In surveillance, we are interested in identifying anomalies in real time. This is common in monitoring systems, where we want to identify unusual behaviour as soon as it occurs. For example, in a manufacturing plant, we may want to identify when a machine is not operating as expected, and take action to prevent further problems. In retail, we may want to identify when sales are unusually high or low, and adjust stock levels if necessary.

Again, we will use the PBS data. For this example, we will combine the data into ATC level 2 groups, and look at total sales volumes (measured in thousands of scripts).

```{r}
#| label: pbs
pbs <- PBS |>
  arrange(ATC2, Month) |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3, .groups = "drop") |>
  mutate(t = as.numeric(Month - min(Month) + 1))
```

```{r}
#| label: pbsdata
#| code-fold: false
pbs
```

The data set is a `tsibble` object, which is a special type of tibble designed for time series data. Two columns have special purposes: the `Month` column is the time index, defining when each observation was made; and the `ATC2` column is a key variable, defining the separate time series in the data set. This data set contains 84 separate time series, one for each `ATC2` value.

Let's first look at the time series for just one ATC group: A12 (Mineral supplements). In fact, this is the same series as was shown in the top panel of @fig-tsparadigms.

```{r}
#| label: fig-a12
#| fig-asp: 0.36
#| fig-cap: The monthly script volume for mineral supplements (ATC group A12) on the Australian PBS.
pbs |>
  filter(ATC2 == "A12") |>
  autoplot(Scripts) +
  labs(title = "Scripts for ATC group A12 (Mineral supplements)")
```

Here there has been a sudden drop in sales at the end of 2005, most likely because of some products no longer being eligible for subsidy. There can also be sudden jumps in sales when a new product becomes available, or a new class of drugs is added to the scheme.

The goal of surveillance is to identify these anomalies as soon as they occur. We can do this by fitting a model to the data, and then comparing the observed values to the model's forecasts. If the observed values are very different from the forecast, then we have an anomaly.

Statistical forecasting models provide forecasts in the form of probability distributions. Let $y_t$ denote the observation of a time series at time $t$. Then a forecast can be expressed as a conditional distribution
$$
  f(y_{t+h} | y_1, \dots, y_{t}, \bm{x}_t),
$$
where $y_1,\dots,y_t$ denotes the observed history of the series, and $\bm{x}_t$ contains any other information available at time $t$ that is used in the model. The forecast "horizon" is given by $h$, denoting the number of time periods into the future that we wish to forecast. Different forecasting methods use different conditioning information, and result in a different form of the forecast distribution. See @fpp3 for a detailed discussion of forecasting methods.

Once we observe the value of $y_{t+h}$, we can calculate the corresponding density scores in the same way as we discussed in @sec-lookout. Because we are interested in real-time surveillance, we need only consider the one-step-ahead forecast density, $f(y_{t+1} | y_1, \dots, y_{t})$. We can then calculate the density score as
$$
  s_{t+1} = -\log \hat{f}(y_{t+1} | y_1, \dots, y_{t}, \bm{x}_t).
$$
This needs to be done iteratively for each time period, updating the model as new data becomes available. Since we need some observations with which to fit a model, we can't begin the process at time $t=1$. Instead, we'll begin at time $t=I$, where $I$ is the smallest number of observations with which we can reasonably estimate the time series model. The value of $I$ will depend on the complexity of the model being used. For very simple models with few parameters, we may be able to set $I$ to around 20, but for complex models with many parameters, $I$ may need to be much larger.

We also can't estimate the Generalized Pareto Distribution until we have computed sufficient density scores. Fortunately, we are often working with many time series, not just one, and we can use the density scores from all series when computing the GPD. Suppose we have $m$ series we are monitoring, then at time $t$, we will have computed $m(t-I)$ anomaly scores. Usually we would need at least a few hundred anomaly scores before we could reasonably estimate a GPD from the top 10% of the available scores. Suppose we required at least 200 scores, then we could only compute a GPD once $t \ge J$ where $J = 200/m+I$.

We can summarise the anomaly detection algorithm for surveillance as follows. First, we'll change the notation slightly to allow for more than one series. Let $y_{i,t}$ denote the observation of the $i$th series at time $t$.

For each $t = I,I+1,\dots$, and for all series $i=1,\dots,m$:

* Fit a time series model to the series $y_{i,1},\dots,y_{i,t}$, and estimate the one-step forecast density, $f_{i,t+1}(y \mid y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* Compute the anomaly score: $s_{i,t+1} = -\log\hat{f}_{i,t+1}(y_{i,t+1}\mid y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* If $t\ge J$, fit a Generalized Pareto Distribution $S$ to the top 10% of anomaly scores $\{s_{i,t}\}$, $i=1,\dots,m$, $t=I+1,I+2,\dots,t$.
* Designate $y_{i,t+1}$ as an anomaly if $P(S > s_{i,t+1}) < 0.05$ under the GPD.

To illustrate, let's apply this to the `pbs` data. Starting with $I=36$ (3 years of data), we will fit ETS models to all available series [Ch 8, @fpp3], and forecast one step ahead in each case. Then we repeat the exercise using 37 observations, then 38 observations, and so on. This is known as a "rolling origin forecast" because the forecast rolls forward by one period each iteration. The process can be illustrated as in @fig-tscvplots.

```{r}
#| label: fig-tscvplots
#| fig-cap: Rolling origin forecasts, with the training sets expanding by one observation at each iteration, and one-step forecasts computed for each training set.
#| echo: false
tscv_plot(.init = 8, .step = 1, h = 1) +
  annotate("text",
    x = 9, y = 0, label = "h = 1",
    color = "#c14b14", family = "Fira Sans"
  )
```

Let's step through the process for the first iteration, with $t=36$. We fit ETS models to each of the time series, with the specific ETS model selected according to the characteristics of the series. See [Ch8, @fpp3] for details of how this is done.

```{r}
#| label: pbsfit
#| code-fold: false
pbs_fit <- pbs |>
  filter(t <= 36) |>
  model(ets = ETS(Scripts))
pbs_fit
```

Forecasts are produced from all fitted models by applying the `forecast()` function to the model table.

```{r}
#| label: pbsfc
#| code-fold: false
pbs_fc <- forecast(pbs_fit, h = 1)
pbs_fc
```

The forecasts are in the `Scripts` column. Note that each of them is a normal distribution where the mean and variance has been estimated using the ETS model. The mean of the forecast distribution is given as `.mean`. 

We can compute the density scores using the `log_likelihood()` function from the `distributional` package, and calculate the lookout probabilities.

```{r}
#| label: pbs_scores
#| code-fold: false
pbs_scores <- pbs_fc |>
  rename(fcast = Scripts) |>
  left_join(pbs |> filter(t == 37), by = c("Month", "ATC2")) |>
  mutate(
    s = -distributional::log_likelihood(fcast, Scripts),
    prob = lookout(density_scores = s, threshold = 0.9)
  )
pbs_scores
```

This process is repeated as we increase the size of the training set. The following code block carries out the calculations in a loop. Before starting the calculations, we will set up some new columns in which to store the forecasts, scores and probabilities at each iteration.

```{r}
#| label: pbsloop
#| code-fold: false
#| eval: false
pbs <- pbs |>
  mutate(s = NA_real_, prob = NA_real_, fmean = NA_real_, fvar = NA_real_)

for (i in 36:(max(pbs$t) - 1)) {
  # Fit ETS model to all series and compute 1-step forecasts
  pbs_fc <- pbs |>
    filter(t <= i) |>
    model(ets = ETS(Scripts)) |>
    forecast(h = 1) |>
    rename(fcast = Scripts) |>
    filter(!is.na(.mean))
  # Calculate anomaly scores and probabilities
  pbs_scores <- pbs |>
    filter(t == i+1) |>
    right_join(pbs_fc, by = c("Month", "ATC2")) |>
    mutate(
      newmean = mean(fcast),
      newvar = distributional::variance(fcast),
      news = -distributional::log_likelihood(fcast, Scripts),
      newprob = lookout(density_scores = news, threshold = 0.9)
    ) |>
    select(Month, ATC2, news, newprob, newmean, newvar)
  # Add scores to pbs data set
  pbs <- pbs |>
    left_join(pbs_scores, by = c("Month", "ATC2")) |>
    mutate(
      fmean = if_else(!is.na(newmean), newmean, fmean),
      fvar = if_else(!is.na(newvar), newvar, fvar),
      s = if_else(!is.na(news), news, s),
      prob = if_else(!is.na(newprob), newprob, prob)
    ) |>
    select(-news, -newprob, -newmean, -newvar)
}
```

```{r}
pbs
```

We compare the distribution against the observation for February 2006.

```{r}
#| label: a12plot0
#| eval: false
observed <- pbs |> filter(ATC2 == "A12", Month == yearmonth("2006 Feb"))
fc_a12 |>
  autoplot(a12) +
  geom_point(data = observed, aes(x = Month, y = Scripts)) +
  labs(
    y = "Scripts (thousands)",
    title = "Forecast of A12 scripts: Feb 2006"
  ) +
  theme(legend.position = "none") +
  ylim(35, 145)
```


```{r}
#| label: pbs_scores3
#| eval: false
pbs_anomalies <- pbs |> filter(prob < 0.05)
pbs_anomalies
```



```{r}
#| label: pbs_scores4
#| eval: false
pbs_plot(pbs, pbs_anomalies, "L03")
```

```{r}
#| label: pbs_scores5
#| eval: false
pbs_plot(pbs, pbs_anomalies, "N07")
```

Consecutive anomalies are hard to identify because the preceding anomalies corrupt the model.

```{r}
#| label: pbs_scores6
#| eval: false
pbs_plot(pbs, pbs_anomalies, "R06")
```

A sequence of near anomalies makes it hard to spot a true anomaly.


## Somewhere

* AO vs IO
* tsoutliers package and tsoutliers() function
* smooth() function
*
