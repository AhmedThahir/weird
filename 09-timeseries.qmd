# Time series data {#sec-noneuclidean}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(11 / .step))
  ) |>
    group_by(.id) |>
    mutate(
      observation = case_when(
        time <= ((min(.id) - 1) * .step + .init) ~ "train",
        time %in% c((min(.id) - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) |>
    ungroup() |>
    filter(.id <= 26 - .init) |>
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", linewidth = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#c14b14", unused = "gray")) +
    guides(col = "none") +
    labs(x = "time", y = "") +
    theme_void() +
    theme(axis.title.x = element_text(margin = margin(t = 2))) +
    theme(text = ggplot2::element_text(family = "Fira Sans"))
}

pbs_plot <- function(pbs, anomalies, series) {
  dates <- anomalies |>
    dplyr::filter(ATC2 == series) |>
    dplyr::pull(Month) |>
    as.Date()
  pbs |>
    dplyr::filter(ATC2 == series) |>
    ggplot2::ggplot(ggplot2::aes(x = Month, y = Scripts)) +
    tsibble::scale_x_yearmonth(date_breaks = "2 years", date_labels = "%Y") +
    ggplot2::geom_vline(xintercept = dates, color = "red", alpha = 0.4) +
    ggplot2::geom_line() +
    ggplot2::labs(title = paste("Scripts for ATC group", series))
}
```

```{r}
library(fpp3)
library(distributional)
```

```{r}
#| label: egdata
#| include: false
# Loaded again later, but needed here for the paradigm examples
pbs <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup()
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
```

## Time series anomaly detection paradigms

Time series data are observations that are collected over time. In this book, we will only consider time series which are observed at regular intervals, such as annually, monthly, or hourly.

When considering time series data, we can distinguish between three different anomaly detection paradigms:

1. **Surveillance**: Identifying anomalies within a time series in real time
2. **Weird times**: Identifying anomalies within a time series in historical data.
3. **Weird series**: Identifying an anomalous time series within a collection of time series.

These can be illustrated using the following examples.

```{r}
#| label: fig-tsparadigms
#| echo: false
#| fig-cap: The three main anomaly detection paradigms. In the top panel, we aim to identify an unusual observation in the next time period. In the middle panel, we aim to identify unusual observations within historical data. In the bottom panel, we aim to identify an unusual time series within a collection of time series.
a12 <- pbs |>
  filter(ATC2 == "A12", Month <= yearmonth("2006 Feb")) |>
  mutate(paradigm = "Surveillance")
p1 <- a12 |>
  autoplot(Scripts) +
  geom_point(data = tail(a12,1)) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_continuous(breaks = NULL)
p2 <- fr_mortality |>
  filter(Age == 25, Sex == "Male") |>
  mutate(paradigm = "Weird times") |>
  autoplot(Mortality) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)
p3 <- fr_mortality |>
  filter(Age %in% c(2:6, 60), Sex == "Female") |>
  mutate(paradigm = "Weird series") |>
  autoplot(Mortality) +
  guides(col = "none") +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)

patchwork::wrap_plots(p1, p2, p3, ncol = 1)
```

**Surveillance**: In the top plot, we are looking for an anomaly in the next observation in the time series. This is commonly done in surveillance, when a time is monitored in real time to identify an unusual observation, and appropriate action taken. An anomaly in this context, is an observation that is very different from what was forecast. Although only a single time series is shown in this plot, the idea easily extends to multivariate time series, where we look for something unusual occurring in the next time period across a set of time series.

**Weird times**: In the middle plot, we are looking for historical anomalies within time series data. This is common in quality control, or in data cleaning. The aim is to find time periods where the observations are different from the rest of the data. Again, it can easily be extended to multivariate time series, where we look for time periods when one or more of the series may display unusual observations

**Weird series**: In the bottom plot, we are looking for an anomalous time series within a collection of time series. The observations within each series may all be consistent, but the series as a whole may be unusual. This is, by its nature, a multivariate problem. It is common in finance, where we look for unusual behaviour of a stock within a collection of stocks, or in health, where we look for unusual behaviour of a patient within a collection of patients.

We will discuss each of these paradigms in turn.

## Surveillance

In surveillance, we are interested in identifying anomalies in real time. This is common in monitoring systems, where we want to identify unusual behaviour as soon as it occurs. For example, in a manufacturing plant, we may want to identify when a machine is not operating as expected, and take action to prevent further problems. In retail, we may want to identify when sales are unusually high or low, and adjust stock levels if necessary.

To illustrate the ideas we will use a data set of monthly observations on
the Australian Pharmaceutical Benefits Scheme (PBS), from July 1991 to June 2008. The PBS involves the Australian government subsidising certain pharmaceutical products, to allow more equitable access to essential medicines. The data set contains monthly sales volumes of those products being subsidised, classified according to the Anatomical Therapeutic Chemical (ATC) classification system. For our purposes, we will combine the data into ATC level 2 groups, and look at total sales volumes (measured in thousands of scripts).

```{r}
#| label: pbs
pbs <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup()
```

```{r}
#| label: pbsdata
#| code-fold: false
pbs
```

The data set is a `tsibble` object, which is a special type of tibble designed for time series data. Two columns have special purposes: the `Month` column is the time index, defining when each observation was made; and the `ATC2` column is a key variable, defining the separate time series in the data set. This data set contains 84 separate time series, one for each `ATC2` value.

Let's first look at the time series for just one ATC group: A12 (Mineral supplements). In fact, this is the same series as was shown in the top panel of @fig-tsparadigms.

```{r}
#| label: fig-a12
#| fig-asp: 0.36
#| fig-cap: The monthly script volume for mineral supplements (ATC group A12) on the Australian PBS.
pbs |>
  filter(ATC2 == "A12") |>
  autoplot(Scripts) +
  labs(title = "Scripts for ATC group A12 (Mineral supplements)")
```

Here there has been a sudden drop in sales at the end of 2005, most likely because of some products no longer being eligible for subsidy. There can also be sudden jumps in sales when a new product becomes available, or a new class of drugs is added to the scheme.

The goal of surveillance is to identify these anomalies as soon as they occur. We can do this by fitting a model to the data, and then comparing the observed values to the model's forecasts. If the observed values are very different from the forecast, then we have an anomaly.

Statistical forecasting models provide forecasts in the form of probability distributions. Let $y_t$ denote the observation of a time series at time $t$. Then a forecast can be expressed as a conditional distribution
$$
  f(y_{t+h} | y_1, \dots, y_{t}, \bm{x}_t),
$$
where $y_1,\dots,y_t$ denotes the observed history of the series, and $\bm{x}_t$ contains any other information available at time $t$ that is used in the model. The forecast "horizon" is given by $h$, denoting the number of time periods into the future that we wish to forecast. Different forecasting methods use different conditioning information, and result in a different form of the forecast distribution. See @fpp3 for a detailed discussion of forecasting methods.

Once we observe the value of $y_{t+h}$, we can calculate the corresponding density scores in the same way as we discussed in @sec-lookout. Because we are interested in real-time surveillance, we need only consider the one-step-ahead forecast density, $f(y_{t+1} | y_1, \dots, y_{t})$. We can then calculate the density score as
$$
  s_{t+1} = -\log \hat{f}(y_{t+1} | y_1, \dots, y_{t}, \bm{x}_t).
$$
This needs to be done iteratively for each time period, updating the model as new data becomes available. Since we need some observations with which to fit a model, we can't begin the process at time $t=1$. Instead, we'll begin at time $t=I$, where $I$ is the smallest number of observations with which we can reasonably estimate the time series model. The value of $I$ will depend on the complexity of the model being used. For very simple models with few parameters, we may be able to set $I$ to around 20, but for complex models with many parameters, $I$ may need to be much larger.

We also can't estimate the Generalized Pareto Distribution until we have computed sufficient density scores. Fortunately, we are often working with many time series, not just one, and we can use the density scores from all series when computing the GPD. Suppose we have $m$ series we are monitoring, then at time $t=J$, we will have computed $m(J-I)$ anomaly scores. Usually we would need at least a few hundred anomaly scores before we could reasonably estimate a GPD from the top 10% of the available scores. Suppose we required at least 200 scores, then we could only compute a GPD once $m(J-I) \ge 200$, so that $J \ge 200/m+I$.

We can summarise the anomaly detection algorithm for surveillance as follows. First, we'll change the notation slightly to allow for more than one series. Let $y_{i,t}$ denote the observation of the $i$th series at time $t$.

For each $t = I,I+1,\dots,$, and for all series $i=1,\dots,m$:

* Fit a time series model to each series, and estimate the one-step forecast density, $f_i(y_{i,t+1} | y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* Compute the anomaly score: $s_{i,t+1} = -\log\hat{f}_i(y_{i,t+1}| y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* If $t>J$, fit a Generalized Pareto Distribution $S$ to the top 10% of anomaly scores $\{s_{i,t}\}$, $i=1,\dots,m$, $t=I+1,I+2,\dots,t$.
* Designate $y_{i,t+1}$ as an anomaly if $P(S > s_{i,t+1}) < 0.05$ under the GPD.

To illustrate, let's apply this to the pbs data. Starting with $I=36$ (3 years of data), we will fit ETS models to all available series [Ch 8, @fpp3], and forecast one step ahead in each case. Then we repeat the exercise using 37 observations, then 38 observations, and so on. This is known as a "rolling origin forecast" because the forecast rolls forward by one period each iteration. The process can be illustrated as in @fig-tscvplots.

```{r}
#| label: fig-tscvplots
#| fig-cap: Rolling origin forecasts, with the training sets expanding by one observation at each iteration, and one-step forecasts computed for each training set.
#| echo: false
tscv_plot(.init = 8, .step = 1, h = 1) +
  annotate("text",
    x = 9, y = 0, label = "h = 1",
    color = "#c14b14", family = "Fira Sans"
  )
```

In practice, a new iteration of the process must be done each time a new observation is recorded. But to simplify the process for this illustration, we will use the `stretch_tsibble()` function which constructs all the training sets at once. The `.id` column is an additional key variable, to allow the time series to be distinguished.

```{r}
#| label: pbsstretch
#| code-fold: false
pbs_stretch <- pbs |>
  stretch_tsibble(.step = 1, .init = 36)
pbs_stretch
```

```{r}
#| label: pbsfit
#| eval: false
#| code-fold: false
pbs_fit <- pbs_stretch |>
   model(ets = ETS(Scripts))
```

Next we fit ETS models to each of the time series, with the specific ETS model selected according to the characteristics of the series. See [Ch8, @fpp3] for details of how this is done.

```{r}
#| label: pbsfitcache
#| echo: false
pbs_fit <- pbs_stretch |>
  model(ets = ETS(Scripts)) |>
  cache("pbs_fit")
```

```{r}
#| label: pbsfitshow
#| code-fold: false
pbs_fit
```

Forecasts are produced from all fitted models by applying the `forecast()` function to the model table.

```{r}
#| label: pbsfc
#| code-fold: false
pbs_fc <- forecast(pbs_fit, h = 1)
pbs_fc
```

The forecasts are in the `Scripts` column. Note that each of them is a normal distribution where the mean and variance has been estimated using the ETS model. The mean of the forecast distribution is given as `.mean`. 

The next state is to compute the density scores, which is easily done since they are equivalent to -log likelihood values, and the `log_likelihood()` function from the `distributional` package provides a convenient tool for computing them.

```{r}
#| label: pbs_scores
#| code-fold: false
pbs_scores <- pbs_fc |>
  left_join(pbs |> rename(actual = Scripts), by = c("ATC2", "Month")) |>
  mutate(s = -log_likelihood(Scripts, actual))
pbs_scores
```


Finally, we compute the lookout probabilities using all density scores available at the time of each forecast.

```{r}
#| label: pbs_anomalies
#| code-fold: false
#| warning: false
pbs_scores <- pbs_scores |>
  filter(!is.na(s)) |>
  group_by(.id) |> # Need to make this < = .id
  mutate(prob = lookout(density_scores = s, threshold = 0.9)) |>
  ungroup()
```


 We compare the distribution against the observation for Feburary 2006.

```{r}
#| label: a12plot0
#| eval: false
observed <- pbs |> filter(ATC2 == "A12", Month == yearmonth("2006 Feb")) 
fc_a12 |>
  autoplot(a12) +
  geom_point(data = observed, aes(x = Month, y = Scripts)) +
  labs(
    y = "Scripts (thousands)",
    title = "Forecast of A12 scripts: Feb 2006"
  ) +
  theme(legend.position = "none") +
  ylim(35, 145)
```


```{r}
#| label: pbs_scores3
pbs_anomalies <- pbs_scores |> filter(prob < 0.05)
pbs_anomalies
```

### Example: Australian pharmaceutical sales

```{r}
#| label: pbs_scores4
pbs_plot(pbs, pbs_anomalies, "L03")
```

```{r}
#| label: pbs_scores5
pbs_plot(pbs, pbs_anomalies, "N07")
```

Consecutive anomalies are hard to identify because the preceding anomalies corrupt the model.

```{r}
#| label: pbs_scores6
#| echo: false
pbs_plot(pbs, pbs_anomalies, "R06")
```

A sequence of near anomalies makes it hard to spot a true anomaly.

## Weird times

### Example: French mortality

```{r}
#| label: fr_mortality
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
fr_mortality
```

```{r}
#| label: fr_mortality_time_plots
fr_mortality |>
  ggplot(aes(
    x = Year, y = Mortality,
    color = as.factor(Age), group = Age
  )) +
  geom_line() +
  facet_grid(. ~ Sex) +
  scale_y_log10() +
  theme(legend.position = "none")
```

```{r}
#| label: fr_fit
fr_fit <- fr_mortality |>
  model(stl = STL(log(Mortality)))

fr_sigma <- augment(fr_fit) |>
  as_tibble() |>
  group_by(Age, Sex) |>
  summarise(sigma = IQR(.innov) / 1.349, .groups = "drop")

fr_scores <- augment(fr_fit) |>
  as_tibble() |>
  left_join(fr_sigma) |>
  mutate(
    s = -log(dnorm(.innov / sigma)),
    prob = lookout(density_scores = s, threshold_probability = 0.9)
  ) |>
  select(-.model, -.resid, -.fitted, -sigma)
```

```{r}
#| label: fr_scores3
fr_scores |> arrange(prob)

fr_anomalies <- fr_scores |>
  filter(prob < 0.05) |>
  as_tibble() |>
  select(Year, Sex, Age) |>
  distinct() |>
  left_join(fr_mortality)
yrs <- fr_anomalies |>
  select(Year, Sex) |>
  distinct()
```


* 1870--1872: Franco-Prussian war and repression of the ‘Commune de Paris’
* 1914--1918: World War I
* 1918: Spanish flu
* 1939--1945: World War II

```{r}
#| label: fr_25
fr_anomalies_plot_male2 <- fr_anomalies |>
  filter(Sex == "Male") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Male"]),
    alpha = 0.5, color = "grey"
  ) +
  geom_point(col = "#478cb2") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Male", ],
    aes(y = 75, label = Year), col = "#478cb2", size = 3, seed = 1967
  ) +
  ylim(4, 85)
fr_anomalies_plot_female2 <- fr_anomalies |>
  filter(Sex == "Female") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Female"]),
    alpha = 0.5, color = "grey"
  ) +
  labs(title = "French mortality anomalies") +
  geom_point(col = "#c1653a") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Female", ],
    aes(y = 75, label = Year), col = "#c1653a", size = 3, seed = 1967
  ) +
  ylim(4, 85)
patchwork::wrap_plots(
  fr_anomalies_plot_female2 +
    geom_hline(yintercept = 25, color = "gray"),
  fr_anomalies_plot_male2 +
    geom_hline(yintercept = 25, color = "gray") +
    annotate("text", x = 1890, y = 27, label = "Age 25", family = "Fira Sans"),
  nrow = 1
)

fr_mortality |>
  filter(Age == 25) |>
  ggplot(aes(x = Year, y = Mortality, color = Sex)) +
  geom_line() +
  facet_grid(Sex ~ ., scales = "free_y") +
  geom_point(data = fr_anomalies |> filter(Age == 25)) +
  labs(title = "French mortality: Age 25")
```

## Weird series

Use PBS again

## Somewhere

* AO vs IO
* tsoutliers package and tsoutliers() function
* smooth() function
*
