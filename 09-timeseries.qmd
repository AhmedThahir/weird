# Time series data {#sec-noneuclidean}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(11 / .step))
  ) |>
    group_by(.id) |>
    mutate(
      observation = case_when(
        time <= ((min(.id) - 1) * .step + .init) ~ "train",
        time %in% c((min(.id) - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) |>
    ungroup() |>
    filter(.id <= 26 - .init) |>
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", linewidth = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#c14b14", unused = "gray")) +
    guides(col = "none") +
    labs(x = "time", y = "") +
    theme_void() +
    theme(axis.title.x = element_text(margin = margin(t = 2))) +
    theme(text = ggplot2::element_text(family = "Fira Sans"))
}

pbs_plot <- function(pbs, anomalies, series) {
  dates <- anomalies |>
    dplyr::filter(ATC2 == series) |>
    dplyr::pull(Month) |>
    as.Date()
  pbs |>
    dplyr::filter(ATC2 == series) |>
    ggplot2::ggplot(ggplot2::aes(x = Month, y = Scripts)) +
    tsibble::scale_x_yearmonth(date_breaks = "2 years", date_labels = "%Y") +
    ggplot2::geom_vline(xintercept = dates, color = "red", alpha = 0.4) +
    ggplot2::geom_line() +
    ggplot2::labs(title = paste("Scripts for ATC group", series))
}
```

```{r}
library(fpp3)
library(distributional)
```

```{r}
#| label: egdata
#| include: false
# Loaded again later, but needed here for the paradigm examples
pbs <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup()
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
```

## Time series anomaly detection paradigms

When considering time series data, we can distinguish between three different anomaly detection paradigms:

1. **Surveillance**: Identifying anomalies within a time series in real time
2. **Weird times**: Identifying anomalies within a time series in historical data.
3. **Weird series**: Identifying an anomalous time series within a collection of time series.

These can be illustrated using the following examples.

```{r}
#| label: fig-tsparadigms
#| echo: false
#| fig-cap: The three main anomaly detection paradigms. In the top panel, we aim to identify an unusual observation in the next time period. In the middle panel, we aim to identify unusual observations within historical data. In the bottom panel, we aim to identify an unusual time series within a collection of time series.
a12 <- pbs |>
  filter(ATC2 == "A12", Month <= yearmonth("2006 Feb")) |>
  mutate(paradigm = "Surveillance")
p1 <- a12 |>
  autoplot(Scripts) +
  geom_point(data = tail(a12,1)) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_continuous(breaks = NULL)
p2 <- fr_mortality |>
  filter(Age == 25, Sex == "Male") |>
  mutate(paradigm = "Weird times") |>
  autoplot(Mortality) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)
p3 <- fr_mortality |>
  filter(Age %in% c(2:6, 60), Sex == "Female") |>
  mutate(paradigm = "Weird series") |>
  autoplot(Mortality) +
  guides(col = "none") +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)

patchwork::wrap_plots(p1, p2, p3, ncol = 1)
```

**Surveillance**: In the top plot, we are looking for an anomaly in the next observation in the time series. This is commonly done in surveillance, when a time is monitored in real time to identify an unusual observation, and appropriate action taken. An anomaly in this context, is an observation that is very different from what was forecast. Although only a single time series is shown in this plot, the idea easily extends to multivariate time series, where we look for something unusual occurring in the next time period across a set of time series.

**Weird times**: In the middle plot, we are looking for historical anomalies within time series data. This is common in quality control, or in data cleaning. The aim is to find time periods where the observations are different from the rest of the data. Again, it can easily be extended to multivariate time series, where we look for time periods when one or more of the series may display unusual observations

**Weird series**: In the bottom plot, we are looking for an anomalous time series within a collection of time series. The observations within each series may all be consistent, but the series as a whole may be unusual. This is, by its nature, a multivariate problem. It is common in finance, where we look for unusual behaviour of a stock within a collection of stocks, or in health, where we look for unusual behaviour of a patient within a collection of patients.

We will discuss each of these paradigms in turn.

## Surveillance

In surveillance, we are interested in identifying anomalies in real time. This is common in monitoring systems, where we want to identify unusual behaviour as soon as it occurs. For example, in a manufacturing plant, we may want to identify when a machine is not operating as expected, and take action to prevent further problems. In retail, we may want to identify when sales are unusually high or low, and adjust stock levels if necessary.

To illustrate the ideas we will use a data set of monthly observations on
the Australian Pharmaceutical Benefits Scheme (PBS), from July 1991 to June 2008. The PBS involves the Australian government subsidising certain pharmaceutical products, to allow more equitable access to essential medicines. The data set contains monthly sales volumes of those products being subsidised, classified according to the Anatomical Therapeutic Chemical (ATC) classification system. For our purposes, we will combine the data into ATC level 2 groups, and look at total sales volumes (measured in thousands of scripts).

```{r}
#| label: pbs
pbs <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup()
pbs
```

The data set is a `tsibble` object, which is a special type of tibble designed for time series data. This data set contains 84 separate time series, one for each `ATC2` value.

Let's first look at the time series for just one ATC group: A12 (Mineral supplements). In fact, this is the same series as was shown in the top panel of @fig-tsparadigms.

```{r}
#| label: fig-a12
#| fig-asp: 0.3
#| fig-cap: The monthly script volume for mineral supplements (ATC group A12) on the Australian PBS.
pbs |>
  filter(ATC2 == "A12") |>
  autoplot(Scripts) +
  labs(title = "Scripts for ATC group A12 (Mineral supplements)")
```

Here there has been a sudden drop in sales at the end of 2005, most likely because of some products no longer being eligible for subsidy. There can also be sudden jumps in sales when a new product becomes available, or a new class of drugs is added to the scheme.

The goal of surveillance is to identify these anomalies as soon as they occur. We can do this by fitting a model to the data, and then comparing the observed values to the model's forecasts. If the observed values are very different from the forecast, then we have an anomaly.

Statistical forecasting models provide forecasts in the form of probability distributions. Let $y_t$ denote the observation of a time series at time $t$. Then a forecast can be expressed as a conditional distribution
$$
  f(y_{t+h} | y_1, \dots, y_{t}, \bm{x}_t),
$$
where $\bm{x}_t$ contains any exogenous information available at time $t$ that is used in the model. The forecast "horizon" is given by $h$, denoting the number of time periods into the future that we wish to forecast. Different forecasting methods use different conditioning information, and result in a different form of the forecast distribution. See @fpp3 for a detailed discussion of forecasting methods.

Once we observe the value of $y_{t+h}$, we can calculate the corresponding density scores in the same way as we discussed in @sec-lookout. Because we are interested in real-time surveillance, we are only interested in the one-step-ahead forecast density, $f(y_{t+1} | y_1, \dots, y_{t})$. We can then calculate the density score as
$$
  s_{t+1} = -\log \hat{f}(y_{t+1} | y_1, \dots, y_{t}, \bm{x}_t).
$$
This needs to be done iteratively for each time period, updating the model as new data becomes available. Since we need some observations with which to fit a model, we can't begin the process at time $t=1$. Instead, we'll begin at time $t=I$, where $I$ is the smallest number of observations with which we can reasonably estimate the time series model. The value of $I$ will depend on the complexity of the model being used. For very simple models with few parameters, we may be able to set $I$ to around 20, but for complex models with many parameters, $I$ may need to be much larger.

We also can't estimate the Generalized Pareto Distribution until we have computed sufficient density scores. Fortunately, we are often working with many time series, not just one, and we can use the density scores from all series when computing the GPD. Suppose we have $m$ series we are monitoring, then at time $t=J$, we will have computed $m(J-I)$ anomaly scores. Usually we would need at least a few hundred anomaly scores before we could reasonably estimate a GPD from the top 10% of the available scores.

We can summarise the anomaly detection algorithm for surveillance as follows. First, we'll change the notation slightly to allow for more than one series. Let $y_{i,t}$ denote the observation of the $i$th series at time $t$.

For each $t = I,I+1,\dots,$, and for all series $i=1,\dots,m$:

* Fit a time series model to each series, and estimate the one-step forecast density, $f_i(y_{i,t+1} | y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* Compute the anomaly score: $s_{i,t+1} = -\log\hat{f}_i(y_{i,t+1}| y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* Fit a Generalized Pareto Distribution $S$ to the top 10% of anomaly scores \{s_{i,t}\}, $i=1,\dots,m$, $t=I+1,I+2,\dots,t$.
* Designate $y_{i,t}$ as an anomaly if $P(S > s_{i,t+1}) < 0.05$ under the GPD.

To illustrate, let's apply this to the February 2006 observation in the A12 series.

```{r}
#| label: a12
#| fig-height: 3.6
a12 <- pbs |> filter(ATC2 == "A12", Month <= yearmonth("2006 Jan"))
a12plus <- pbs |> filter(ATC2 == "A12", Month <= yearmonth("2006 Feb"))
fc_a12 <- a12 |>
  model(ets = ETS(Scripts)) |>
  forecast(h = 1)
```

```{r}
#| label: a12plot0
fc_a12 |>
  autoplot(a12plus) +
  labs(
    y = "Scripts (thousands)",
    title = "Forecast of A12 scripts: Feb 2006"
  ) +
  theme(legend.position = "none") +
  ylim(35, 145)
```

### Rolling origin forecasts

```{r}
#| label: tscvplots
tscv_plot(.init = 8, .step = 1, h = 1) +
  annotate("text",
    x = 9, y = 0, label = "h = 1",
    color = "#c14b14", family = "Fira Sans"
  )
```

```{r}
#| label: pbsstretch
pbs_stretch <- stretch_tsibble(pbs, .step = 1, .init = 36)
pbs_stretch
```

```{r}
#| label: pbsfit
#| eval: false
pbs_fit <- pbs_stretch |> model(ets = ETS(Scripts))
```

```{r}
#| label: pbsfitcache
#| echo: false
pbs_fit <- pbs_stretch |>
  model(ets = ETS(Scripts)) |>
  cache("pbs_fit")
```

```{r}
#| label: pbsfitshow
pbs_fit
```

```{r}
#| label: pbsfc
pbs_fc <- forecast(pbs_fit, h = 1)
pbs_fc
```

```{r}
#| label: pbs_scores
pbs_scores <- pbs_fc |>
  left_join(pbs |> rename(actual = Scripts), by = c("ATC2", "Month")) |>
  group_by(.id) |>
  mutate(
    s = -log_likelihood(Scripts, actual),
    prob = lookout(density_scores = s, threshold = 0.9)
  ) |>
  ungroup()
pbs_scores
```

```{r}
#| label: pbs_scores3
pbs_anomalies <- pbs_scores |> filter(prob < 0.05)
pbs_anomalies
```

### Example: Australian pharmaceutical sales

```{r}
#| label: pbs_scores4
pbs_plot(pbs, pbs_anomalies, "L03")
```

```{r}
#| label: pbs_scores5
pbs_plot(pbs, pbs_anomalies, "N07")
```

Consecutive anomalies are hard to identify because the preceding anomalies corrupt the model.

```{r}
#| label: pbs_scores6
#| echo: false
pbs_plot(pbs, pbs_anomalies, "R06")
```

A sequence of near anomalies makes it hard to spot a true anomaly.

## Example: French mortality

```{r}
#| label: fr_mortality
fr_mortality <- vital::read_hmd_files(here::here("data/Mx_1x1.txt")) |>
  filter(Sex != "Total", Year < 2000) |>
  vital::collapse_ages(max_age = 85) |>
  as_tsibble() |>
  select(Year, Age, Sex, Mortality)
fr_mortality
```

```{r}
#| label: fr_mortality_time_plots
fr_mortality |>
  ggplot(aes(
    x = Year, y = Mortality,
    color = as.factor(Age), group = Age
  )) +
  geom_line() +
  facet_grid(. ~ Sex) +
  scale_y_log10() +
  theme(legend.position = "none")
```

```{r}
#| label: fr_fit
fr_fit <- fr_mortality |>
  model(stl = STL(log(Mortality)))

fr_sigma <- augment(fr_fit) |>
  as_tibble() |>
  group_by(Age, Sex) |>
  summarise(sigma = IQR(.innov) / 1.349, .groups = "drop")

fr_scores <- augment(fr_fit) |>
  as_tibble() |>
  left_join(fr_sigma) |>
  mutate(
    s = -log(dnorm(.innov / sigma)),
    prob = lookout(density_scores = s, threshold_probability = 0.9)
  ) |>
  select(-.model, -.resid, -.fitted, -sigma)
```

```{r}
#| label: fr_scores3
fr_scores |> arrange(prob)

fr_anomalies <- fr_scores |>
  filter(prob < 0.05) |>
  as_tibble() |>
  select(Year, Sex, Age) |>
  distinct() |>
  left_join(fr_mortality)
yrs <- fr_anomalies |>
  select(Year, Sex) |>
  distinct()
```


* 1870--1872: Franco-Prussian war and repression of the ‘Commune de Paris’
* 1914--1918: World War I
* 1918: Spanish flu
* 1939--1945: World War II

```{r}
#| label: fr_25
fr_anomalies_plot_male2 <- fr_anomalies |>
  filter(Sex == "Male") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Male"]),
    alpha = 0.5, color = "grey"
  ) +
  geom_point(col = "#478cb2") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Male", ],
    aes(y = 75, label = Year), col = "#478cb2", size = 3, seed = 1967
  ) +
  ylim(4, 85)
fr_anomalies_plot_female2 <- fr_anomalies |>
  filter(Sex == "Female") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Female"]),
    alpha = 0.5, color = "grey"
  ) +
  labs(title = "French mortality anomalies") +
  geom_point(col = "#c1653a") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Female", ],
    aes(y = 75, label = Year), col = "#c1653a", size = 3, seed = 1967
  ) +
  ylim(4, 85)
patchwork::wrap_plots(
  fr_anomalies_plot_female2 +
    geom_hline(yintercept = 25, color = "gray"),
  fr_anomalies_plot_male2 +
    geom_hline(yintercept = 25, color = "gray") +
    annotate("text", x = 1890, y = 27, label = "Age 25", family = "Fira Sans"),
  nrow = 1
)

fr_mortality |>
  filter(Age == 25) |>
  ggplot(aes(x = Year, y = Mortality, color = Sex)) +
  geom_line() +
  facet_grid(Sex ~ ., scales = "free_y") +
  geom_point(data = fr_anomalies |> filter(Age == 25)) +
  labs(title = "French mortality: Age 25")
```

# Somewhere

* AO vs IO
* tsoutliers package and tsoutliers() function
* smooth() function
*
