# Tools {#sec-tools}

```{r}
#| echo: false
#| include: false
source("before-each-chapter.R")
```

This chapter covers various statistical and computational tools that will be used in subsequent chapters. Some of this material (especially @sec-univariate) should be familiar to most readers, but is covered here as a refresher, and to introduce some notation that will be used in later chapters.

We are interested in data obtained from a range of possible sample spaces in this book, but for now we will assume that all data are real numbers, and come from continuous probability distributions. Later we will consider other types of data, including discrete data, data that are functions, images, video, networks, text, or even probability distributions. But for the first half of the book, we will only deal with numerical data.

References will be provided at the end of each section for those who want to learn more about the topics covered here.

## Univariate probability distributions {#sec-univariate}

Let $Y$ denote a continuous random variable taking values in $\mathbb{R}$ (the real numbers), with its cumulative probability distribution (cdf) given by
$$F(y) = \text{Pr}(Y <= y).
$$ {#eq-cdf}
This function is monotonically increasing in $y$, and takes values between 0 and 1. Because we are assuming that $Y$ is a continuous random variable, $F(y)$ is a continuous function of $y$. The probability density function (pdf) is defined as the derivative of the cdf, so $f(y) = F'(y)$, assuming the derivative exists at $y$. The pdf is non-negative, and integrates to 1.

The probability that $Y$ lies in the interval $[a,b]$ is given by $\text{Pr}(a \leq Y \leq b) = \int_a^b f(y)dy$.

The expected value (or mean) of $Y$ is given by
$$
\text{E}(Y) = \int_{-\infty}^\infty y f(y)dy,
$$
and the variance is given by
$$
\text{Var}(Y) = \text{E}[(Y-\text{E}(Y))^2] = \int_{-\infty}^\infty (y-\text{E}(Y))^2 f(y)dy.
$$
The standard deviation is the square root of the variance.

### Uniform distribution

The simplest continuous distribution is the **Uniform** distribution, which takes two parameters, $a$ and $b$, and has pdf given by
$$
  f(y;a,b) = \begin{cases}
    \frac{1}{b-a} & \text{if } a \leq y \leq b, \\
    0 & \text{otherwise}.
  \end{cases}
$$
Thus, it can only take values between $a$ and $b$, and every value in that range is equally likely. We refer to a Uniform random variable with parameters $a$ and $b$ as $Y \sim \text{U}(a,b)$. It has mean $(a+b)/2$ and variance $(b-a)^2/12$.

The $U(0,1)$ distribution is shown in @fig-uniform.

```{r}
#| label: fig-uniform
#| fig-cap: Probability density function for a $U(0,1)$ distribution. The mean is 0.5 and the variance is 1/12.
tibble(
    y = seq(-0.5, 1.5, l = 501),
    fy = dunif(y, 0, 1)
  ) |>
  ggplot(aes(x = y, y = fy)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)")
```

### Normal (Gaussian) distribution

The most widely used continuous distribution is the **Normal** distribution, also known as the **Gaussian** distribution, named after the German mathematician Carl Friedrich Gauss. It takes two parameters, the mean $\mu$, and the variance, $\sigma^2$. The pdf is given by
$$
  f(y; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(y-\sigma)^2}{2\sigma^2}\right).
$$
We refer to a Normal random variable with mean $\mu$ and variance $\sigma^2$ as $Y \sim N(\mu, \sigma^2)$. When $\mu=0$ and $\sigma^2=1$, we call it the **standard Normal** distribution N(0,1), and denote the pdf as $\phi(y)$ and the cdf as $\Phi(y)$.

Three Normal distributions are shown in @fig-normal.

```{r}
#| label: fig-normal
#| fig-cap: Probability density functions for three Normal distributions. The mean $\mu$ controls the location of the distribution, while the variance $\sigma^2$ controls the spread.
tibble(
    y = seq(-7.5, 7.5, l = 501),
    `N(0,1)` = dnorm(y, 0, 1),
    `N(0,4)` = dnorm(y, 0, 2),
    `N(1,1)` = dnorm(y, 1, 1)
  ) |>
  pivot_longer(-y, names_to = "Distribution", values_to = "density") |>
  ggplot(aes(x = y, y = density, col = Distribution)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)")
```

### $\chi^2$ distribution

The $\chi^2$ (or **chi-squared**) distribution takes one parameter, the degrees of freedom $k$. The pdf is given by
$$
  f(y;k) = \frac{y^{k/2-1}e^{-y/2}}{2^{k/2}\Gamma(k/2)}.
$$
where $\Gamma(u) = \int_0^\infty x^{u-1}e^{-x}dx$ is the gamma function. For positive integers $u$, $\Gamma(u) = (u-1)!$. The $\chi^2$ distribution arises naturally as the sum of the squares of $k$ independent standard Normal random variables. We refer to a $\chi^2$ random variable with $k$ degrees of freedom as $Y \sim \chi^2(k)$. It has mean $k$ and variance $2k$.

Three $\chi^2$ distributions are shown in @fig-chisq.

```{r}
#| label: fig-chisq
#| fig-cap: Probability density functions for three $\chi^2$ distributions. As the degrees of freedom $k$ increases, the distribution becomes more symmetric, and converges to the Normal distribution N(k,2k)$.
tibble(
    y = seq(0, 15, l = 501),
    `x1` = dchisq(y, 1),
    `x2` = dchisq(y, 2),
    `x5` = dchisq(y, 5)
  ) |>
  pivot_longer(-y, names_to = "Distribution", values_to = "density") |>
  ggplot(aes(x = y, y = density, col = Distribution)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)") +
  coord_cartesian(ylim = c(0, 1)) +
  scale_color_manual(
    breaks = c("x1", "x2", "x5"),
    values = discrete_colors[1:3],
    labels = c(
      latex2exp::TeX("$\\chi^2(1)$"),
      latex2exp::TeX("$\\chi^2(2)$"),
      latex2exp::TeX("$\\chi^2(5)$")
    )
  )
```

### t distribution

The **t** distribution was first described by the English statistician and chemist, William Gosset, who worked for the Guinness brewery in Dublin, Ireland, and published under the pseudonym "Student". It arises naturally as the ratio $Z/\sqrt{S/k}$, where $Z\sim N(0,1)$ and $S\sim \chi^2(k)$ are independent, and is commonly used in variations of $t$-tests. It takes one parameter, the degrees of freedom $k$. The pdf is given by
$$
  f(y;k) = \frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}
  \left(1 + \frac{y^2}{k}\right)^{-\frac{k+1}{2}}.
$$
We refer to a t random variable with $k$ degrees of freedom as $Y \sim t(k)$. The t distribution is similar in shape to the standard Normal distribution, but has heavier tails. When $k=1$, the t distribution is known as the "Cauchy" distribution (after the French mathematician Augustin-Louis Cauchy), which has undefined mean and variance. For $k=2$, it has mean 0 and infinite variance. For $k > 2$, it has mean $0$ and variance $k/(k-2)$. When $k\rightarrow\infty$, the t distribution converges to the standard Normal distribution.

Three t distributions are shown in @fig-t.

```{r}
#| label: fig-t
#| fig-cap: Probability density functions for three $t$ distributions. As the degrees of freedom $k$ increases, the distribution converges to a N(0,1) distribution.
tibble(
    y = seq(-7.5, 7.5, l = 501),
    `t(1)` = dt(y, 1),
    `t(5)` = dt(y, 5),
    `t(99)` = dt(y, 99)
  ) |>
  pivot_longer(-y, names_to = "Distribution", values_to = "density") |>
  ggplot(aes(x = y, y = density, col = Distribution)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)")
```

### Gamma distribution

The **gamma** distribution is a generalization of the $\chi^2$ distribution, and is used in many statistical models. There are several parameterizations. Here, we use the shape/rate parameterization which uses the shape $\alpha$ and the rate $\beta$. The pdf is given by
$$
  f(y; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{\alpha-1}e^{-\beta y}.
$$
We refer to a gamma random variable with shape $\alpha$ and rate $\beta$ as $Y \sim \text{Gamma}(\alpha, \beta)$. The mean and variance of a gamma random variable are $\alpha/\beta$ and $\alpha/\beta^2$ respectively. The $\chi^2(k)$ distribution is a special case of the gamma distribution, with $\alpha = k/2$ and $\beta = 1/2$. The Exponential distribution is also a special case of the gamma distribution, with $\alpha = 1$. The sum of $k$ independent Exponential random variables with rate $\beta$ has a $\text{Gamma}(k, \beta)$ distribution.

Three gamma distributions are shown in @fig-gamma.

```{r}
#| label: fig-gamma
#| fig-cap: Probability density functions for three Gamma distributions. As $\alpha$ increases, the distribution to a $N(\alpha/\beta, \alpha/\beta^2)$ distribution.
tibble(
  y = seq(0, 7.5, l = 501),
  `Gamma(1,1)` = dgamma(y, 1, 1),
  `Gamma(1,2)` = dgamma(y, 1, 2),
  `Gamma(2,1)` = dgamma(y, 2, 1)
) |>
  pivot_longer(-y, names_to = "Distribution", values_to = "density") |>
  ggplot(aes(x = y, y = density, col = Distribution)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)")
```

### Mixture distributions

A **mixture distribution** is a distribution that is formed by mixing together other distributions. That is, a sample value is drawn from a mixture distribution by first randomly selecting one of the component distributions with some specified probability, and then sampling from the selected component distribution.

The pdf of a mixture distribution is given by the weighted sum of the pdfs of the component distributions, where the weights correspond to the probability of sampling from each component. For example, suppose $Y$ is a mixture of two Normal distributions, with parameters $(\mu,\sigma_1^2)$ and $(\mu_2,\sigma_2^2$), where the first distribution has weight $p$ and the second has weight $1-p$. Then the resulting pdf of the mixture distribution is given by
$$
  f(y) = pf(y;\mu_1,\sigma_1^2) + (1-p)f(y;\mu_2,\sigma_2^2).
$$
The mean and variance of the mixture distribution are given by
$$
  \text{E}(Y) = p\mu_1 + (1-p)\mu_2,
$$
and
$$
\text{Var}(Y) = p\sigma_1^2 + (1-p)\sigma_2^2 + p(1-p)(\mu_1 - \mu_2)^2,
$$
respectively.

An example is shown in @fig-mixture, where $\mu_1 = -2$, $\mu_2 = 2$, $\sigma_1^2 = \sigma_2^2 = 1$, and $p=1/3$.

```{r}
#| label: fig-mixture
#| fig-cap: Probability density functions for a mixture of two Normal distributions. The dashed lines show the (scaled) component densities, and the solid line shows the mixture density, equal to the sum of the dashed lines.
tibble(
    y = seq(-5, 5, l = 501),
    pdf1 = 1/3 * dnorm(y, -2, 1),
    pdf2 = 2/3 * dnorm(y, 2, 1),
    fy = pdf1 + pdf2
  ) |>
  ggplot(aes(x = y, y = fy)) +
  geom_line() +
  geom_line(aes(y = pdf1), linetype = "dashed") +
  geom_line(aes(y = pdf2), linetype = "dashed") +
  labs(x = "y", y = "Probability Density Function: f(y)")
```

This is an example of a multimodal density function, with modes at $-2$ and $2$. The mixture distribution is not Normal, even though both component distributions are Normal. The mixture distribution is also not symmetric, even though both component distributions are symmetric.

### Central limit theorem {#sec-clt}

The central limit theorem (CLT) is one of the most important results in statistics. There are several variations of the theorem; the simplest version states that the sum of a large number of iid random variables, with finite mean and variance, is approximately Normally distributed. That is, if $X_1,\dots,X_n$ are iid random variables with mean $\mu$ and variance $\sigma^2 < \infty$, then
$$
  \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}} \longrightarrow N(0,1).
$$

This is a remarkable result as it does not require any assumptions about the underlying distribution of the random variables $X_1,\dots,X_n$, other than that they have finite mean and variance. It is also a very useful result, as it allows us to use the Normal distribution to approximate the distribution of many statistics.

For example, the sample mean (or average), is simply the sum of the observations divided by the sample size. So by the CLT, for a large sample size, the sample mean will have a Normal distribution. That is, for a large sample size, if the observations come from a distribution with mean $\mu$ and variance $\sigma^2$, then the sample mean will have distribution $N(\mu, \sigma^2/n)$, regardless of the underlying distribution of the observations.

Variations on the theorem allow the variables $X_1,\dots,X_n$ to be independent (but not necessarily identically distributed), or to be weakly dependent.

It is because of the CLT that a $\chi^2(k)$ distribution is approximately Normal for large $k$. (Recall that the sum of $k$ independent N(0,1) random variables has a $\chi^2(k)$ distribution.)

Similarly, a Gamma($\alpha,\beta$) distribution is approximately Normal for large $\alpha$. (Recall that the sum of $\alpha$ independent $\text{Gamma}(1,\beta)$, or Exponential, random variables has a $\text{Gamma}(\alpha,\beta)$ distribution.)

@Wasserman2004 provides some further details and examples of the CLT.

### Quantiles

The quantiles of a distribution are given by the inverse of the cumulative distribution function, $Q(p) = F^{-1}(p)$. That is, the $p$th quantile is such that $\text{Pr}(Y \le Q(p)) = p$. So it divides the sample space into two regions, with probability $p$ in the lower region and $1-p$ in the upper region.

Some quantiles are given special names. The $p=0.5$ quantile is called the **median**. The $p=0.25$ and $p=0.75$ quantiles are called the **lower** and **upper quartiles** respectively. The **deciles** are defined when $p=0.1,0.2,\dots,0.9$, while the **percentiles** are defined with $p=0.01,0.02,\dots,0.99$.

The quantile functions for a standard normal distribution, a $t(5)$ distribution, and a $\chi^2(5)$ distribution are shown in @fig-quantile.

```{r}
#| label: fig-quantile
#| fig-cap: Quantile function for a standard Normal distribution, a $t(5)$ distribution, and a $\chi^2(5)$ distribution.
tibble(
    p = seq(0, 1, l = 501),
    qy1 = qnorm(p, 0, 1),
    qy2 = qt(p, 5),
    qy3 = qchisq(p, 5)
  ) |>
  pivot_longer(-p, names_to = "Distribution", values_to = "Q") |>
  ggplot(aes(x = p, y = Q, col = Distribution)) +
  geom_line() +
  labs(x = "p", y = "Quantile function: Q(p)") +
  scale_color_manual(
    values = discrete_colors[1:3],
    labels = c(
      latex2exp::TeX("N(0,1)"),
      latex2exp::TeX("t(5)"),
      latex2exp::TeX("$\\chi^2$$(5)$")
    )
  ) +
  theme(legend.text.align = 0)
```

### Highest density regions {#sec-hdr}

A **highest density region** is defined as the region of the sample space where the density is higher than a given threshold [@HDR96]. It is commonly specified using the probability mass that the HDR contains. For example, a 95% HDR is the region of the sample space that contains 95% of the probability mass, and where every point inside the region has higher density than any point outside the region.

For the univariate random variable $Y$, with a smooth, continuous density function $f$, the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{y: f(y) \ge f_\alpha\}
$$
where $P(Y \in R_\alpha) = 1-\alpha$.

A useful property of the HDR is that it is the smallest region containing $1-\alpha$ of the probability mass.

For a symmetric unimodal distribution such as the Normal or t distributions, the HDR is an interval centered on the mean (or median), with the ends given by the $\alpha/2$ and $1-\alpha/2$ quantiles. For a skewed unimodal distribution such as the $\chi^2$ or Gamma distributions, the HDR is also an interval, but it is not centered on the mean (or median), and the ends are not given by symmetric quantiles. For example, @fig-hdr shows two 90% regions for a $\chi^2(5)$ distribution. The one shown in blue is based on the 0.05 and 0.95 quantiles, while the red region is the 90% HDR. Notice that both are intervals, and that the red interval is smaller than the blue interval. The value of the density at the ends of the HDR is the same, and is given by $f_\alpha$.

```{r}
#| label: fig-hdr
#| fig-cap: Highest density region for a $\chi^2(5)$ distribution. The blue bar shows the 90% interval based on the 0.05 and 0.95 quantiles. The red bar shows the 90% HDR, which is the smallest region containing 90% of the probability mass.
interval1 <- qchisq(p = c(0.05, 0.95), 5)
den <- list(x = seq(0, 20, l = 501))
den$y <- dchisq(den$x, 5)
hdr <- hdrcde::hdr(prob = 90, den = den)
interval2 <- as.numeric(hdr$hdr)
falpha <- hdr$falpha
tibble(
    y = seq(0, 15, l = 501),
    fy = dchisq(y, 5)
  ) |>
  ggplot(aes(x = y, y = fy)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)") +
  geom_line(data = tibble(x = interval1), aes(x = x, y = -0.002), 
            col = "blue", linewidth = 2) +
  geom_line(data = tibble(x = interval2), aes(x = x, y = 0.001), 
            col = "red", linewidth = 2) +
  geom_line(
    data = tibble(x = rep(interval2[1], 2), y = c(0, falpha)),
    aes(x = x, y = y), col = "red", linetype = "dashed"
  ) +
  geom_line(
    data = tibble(x = rep(interval2[2], 2), y = c(0, falpha)),
    aes(x = x, y = y), col = "red", linetype = "dashed"
  ) +
  geom_hline(aes(yintercept = falpha), col = "red", linetype = "dashed") +
  scale_y_continuous(
    breaks = c(seq(0, 0.15, by = 0.05), falpha),
    minor_breaks = NULL,
    labels = latex2exp::TeX(c(seq(0, 0.15, by = 0.05), "$f_\\alpha$")),
  )
```

For a multimodal distribution, the HDR may be a union of intervals. For example, the HDR for the mixture shown in @fig-mixture is displayed in @fig-hdr-mixture.

```{r}
#| label: fig-hdr-mixture
#| fig-cap: Highest density region for the mixture distribution shown in @fig-mixture. The blue bar shows the 90% interval based on the 0.05 and 0.95 quantiles. The red bar shows the 90% HDR, which is the smallest region containing 90% of the probability mass.
den <- list(x = seq(-5, 5, l = 501))
den$y <- 1/3 * dnorm(den$x, -2, 1) + 2/3 * dnorm(den$x, 2, 1)
interval1 <- quantile(c(rnorm(100000, -2, 1), rnorm(200000, 2, 1)), c(0.05, 0.95))
hdr <- hdrcde::hdr(prob = 90, den = den)
interval2 <- as.numeric(hdr$hdr)
falpha <- hdr$falpha
p <- as_tibble(den) |>
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  labs(x = "y", y = "Probability Density Function: f(y)") +
  geom_line(data = tibble(x = interval1), aes(x = x, y = -0.0028), 
            col = "blue", linewidth = 2) +
  geom_line(data = tibble(x = interval2[1:2]), aes(x = x, y = 0.0018), 
            col = "red", linewidth = 2) +
  geom_line(data = tibble(x = interval2[3:4]), aes(x = x, y = 0.0018), 
            col = "red", linewidth = 2)
for (i in seq_along(interval2)) {
  p <- p + geom_line(
    data = tibble(x = rep(interval2[i], 2), y = c(0, falpha)),
    aes(x = x, y = y), col = "red", linetype = "dashed"
  )
}
p +
  geom_hline(aes(yintercept = falpha), col = "red", linetype = "dashed") +
  scale_y_continuous(
    breaks = c(seq(0, 0.2, by = 0.1), falpha),
    minor_breaks = NULL,
    labels = latex2exp::TeX(c(
      seq(0, 0.2, by = 0.1),
      "$f_\\alpha$"
    )),
  )
```



### Further reading

A good introduction to probability distributions on real sample spaces is provided by @Forbes2011, with more advanced treatments in @JKunicontinuous1 and @JKunicontinuous2. @Wasserman2004 provides an introduction to the CLT. For a discussion of HDRs, see @HDR96

## Multivariate probability distributions {#sec-multivariate}

While we will cover anomaly detection in univariate data, most of the methods we will discuss are for multivariate data. We will therefore need to understand some basic concepts of multivariate probability distributions.

Suppose $\bm{Y} = [Y_1,\dots,Y_n]'$ is a random variable taking values in $\mathbb{R}^d$, the $d$-dimensional real numbers. Then the joint distribution of $\bm{y}$ is defined by the joint cdf
$$
F(\bm{y}) = \text{Pr}(\bm{Y} = \bm{y}),
$$
while the joint density function is given by
$$
f(\bm{y}) = \frac{\partial^n F(\bm{y})}{\partial y_1 \dots \partial y_d}.
$$

The marginal cdfs are defined by $F_i(y) = \text{Pr}(Y_i \le y)$, with corresponding marginal pdfs given by $f_i(y) = F_i'(y)$, $i=1,\dots,d$.

If the variables are independent, then the joint pdf is the product of the marginal pdfs, $f(\bm{y}) = \prod_{i=1}^d f_i(y_i)$.

The expected value of $\bm{y}$ is given by
$$
\text{E}(\bm{Y}) = \int_{\mathbb{R}^d} \bm{y} f(\bm{y})d\bm{y},
$$
and the covariance matrix is given by
$$
\text{Var}(\bm{Y}) = \text{E}[(\bm{Y}-\text{E}(\bm{Y}))(\bm{Y}-\text{E}(\bm{Y}))'].
$$
The covariance matrix is a $d\times d$ matrix, with $(i,j)$th element given by $\text{Cov}(Y_i,Y_j) = \text{E}[(Y_i-\text{E}(Y_i))(Y_j-\text{E}(Y_j))]$. The diagonal elements are the variances of the individual variables, while the off-diagonal elements are the covariances between the variables.

### Multivariate Normal distribution

If random variable $\bm{Y}$ has a multivariate Normal distribution, we write $\bm{Y} \sim N(\bm{\mu}, \bm{\Sigma})$, where $\bm{\mu}$ is the mean and $\bm{\Sigma}$ is the covariance matrix.

The multivariate Normal distribution has pdf given by
$$
f(\bm{y}; \bm{\mu}, \bm{\Sigma}) = (2\pi)^{-d/2}|\bm{\Sigma}|^{-1/2} \exp\left\{-\frac{1}{2}(\bm{y}-\bm{\mu})'\bm{\Sigma}^{-1}(\bm{y}-\bm{\mu})\right\}.
$$
The notation $|\bm{\Sigma}|$ denotes the determinant of the matrix $\bm{\Sigma}$.

Multivariate Normal distributions have the interesting property that the marginal distributions are also Normal.

### Highest density regions

As with univariate distributions, a **highest density region** for a multivariate distribution is defined as the region of the sample space where the density is higher than a given threshold. Suppose we have a multivariate random variable $\bm{Y}$ with a smooth, continuous density function $f$. Then the $100(1-\alpha)$% HDR is the set
$$
  R_\alpha = \{\bm{y}: f(\bm{y}) \ge f_\alpha\}
$$
where $P(\bm{Y} \in R_\alpha) = 1-\alpha$.

HDRs are equivalent to level sets of the density function, and so can be plotted as contours for bivariate density functions. For example, the bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ is shown in @fig-bivariate as a series of HDR contours, each containing an additional 10% of the probability mass.

```{r}
#| label: fig-bivariate
#| message: false
#| fig-cap: Bivariate Normal distribution with mean $(0,0)$ and covariance matrix $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
library(mvtnorm)
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
# Random sample from the distribution
samp <- rmvnorm(10000, mu, Sigma)
# Define the density function on a grid
grid <- seq(-5.5, 5.5, l = 999)
den <- list(x = grid, y = grid)
den$z <- dmvnorm(expand_grid(grid, grid), mu, Sigma) |>
  matrix(nrow = length(grid), ncol = length(grid), byrow = TRUE)
# Find the HDRs with probability 0.1, 0.2, ..., 0.9
prob <- seq(9) / 10
hdr <- hdrcde::hdr.2d(x = samp[, 1], y = samp[, 2], den = den, prob = prob)
# Plot the contours
  expand_grid(
    x = seq(-3, 3, l = 199),
    y = seq(-3, 3, l = 199)
  ) |>
  mutate(z = dmvnorm(cbind(x, y), mu, Sigma)) |>
  ggplot(aes(x = x, y = y, z = z)) +
  geom_contour_filled(breaks = rev(c(hdr$falpha, 100))) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  ) +
  scale_fill_manual(
    values = rev(viridisLite::viridis(10)),
    labels = rev(paste0(100 * rev(prob), "%"))
  ) +
  guides(fill = guide_legend(title = "HDR coverage"))
```

Similarly, we can obtain HDRs for a mixture distribution. Suppose we had two bivariate Normal distributions with means $(0,0)$ and $(3,1)$, and covariance matrices equal to  $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and the identity matrix $\bm{I}_2$ respectively. Then the HDRs for an equal mixture of these two distributions is shown in @fig-bivariate-mixture.

```{r}
#| label: fig-bivariate-mixture
#| message: false
#| fig-cap: Bivariate mixture distribution of two equally weighted Normal components with means $(0,0)$ and $(3,1)$, and covariance matrices $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$ and $\bm{I}_2$ respectively. The HDR contours cover 10%, 20%, $\dots$, 90% of the probability mass.
library(mvtnorm)
p1 <- 0.5
p2 <- 0.5
mu1 <- c(0, 0)
mu2 <- c(3, 1)
Sigma1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
Sigma2 <- diag(2)
n1 <- round(10000 * p1)
n2 <- round(10000 * p2)
# Random sample from mixture distribution
samp <- rbind(rmvnorm(n1, mu1, Sigma1), rmvnorm(n2, mu2, Sigma2))
# Define the density function on a grid
minx <- round(min(samp[, 1]) - 0.5, 1)
maxx <- round(max(samp[, 1]) + 0.5, 1)
miny <- round(min(samp[, 2]) - 0.5, 1)
maxy <- round(max(samp[, 2]) + 0.5, 1)
den <- list(x = seq(minx, maxx, l = 999), y = seq(miny, maxy, l = 999))
den$z <- matrix(
  p1 * dmvnorm(expand_grid(den$x, den$y), mu1, Sigma1) +
  p2 * dmvnorm(expand_grid(den$x, den$y), mu2, Sigma2),
  nrow = length(den$x), ncol = length(den$y)
)
# Find the HDRs with probability 0.1, 0.2, ..., 0.9
prob <- seq(9) / 10
hdr <- hdrcde::hdr.2d(x = samp[, 1], y = samp[, 2], den = den, prob = prob)
# Plot the contours
mixture_density <- expand_grid(
    x = seq(minx, maxx, by = 0.05),
    y = seq(miny, maxy, by = 0.05)
  ) |>
  mutate(
    z = p1 * dmvnorm(cbind(x, y), mu1, Sigma1) +
      p2 * dmvnorm(cbind(x, y), mu2, Sigma2)
  )
mixture_plot <- mixture_density |>
  ggplot(aes(x = x, y = y, z = z)) +
  geom_contour_filled(breaks = rev(c(hdr$falpha, 100))) +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Contours of $f(y_1,y_2)$")
  ) +
  scale_fill_manual(
    values = rev(viridisLite::viridis(10)),
    labels = rev(paste0(100 * rev(prob), "%"))
  )
mixture_plot +
  guides(fill = guide_legend(title = "HDR coverage"))
```

Here, the 10% and 20% HDRs contain disconnected regions, but for the larger HDRs, there is just one region for each.

### Conditional probability distributions

A fundamental concept in statistics is a **conditional probability distribution**; that is, the distribution of a random variable conditional on the values of other random variables. Almost all statistical modelling involves the estimation of conditional distributions. For example, a regression is a model for the conditional distribution of a response variable given the values of a set of predictor variables. In its simplest form, we assume the conditional distribution is normal, with constant variance, and mean equal to a linear function of the predictor values. Generalized linear models allow for non-normal conditional distributions, while generalized additive models allow for non-linear relationships between the response and the predictors.

The conditional cdf of $Y$ given $X_1,\dots,X_n$ is defined by the conditional cdf
$$
 F(y\mid x_1,\dots,x_n) = \text{Pr}(Y \le y \mid  X_1 = x_1,\dots,X_n = x_n).
$$
The conditional pdf is given by
$$
f(y \mid  x_1, \dots, x_n) = \frac{f(y,x_1,\dots,x_n)}{f(x_1,\dots,x_n)}.
$$
The conditional pdf can be thought of as slices of the joint pdf, with the values of $x_1,\dots,x_n$ fixed, rescaled to ensure the conditional pdfs integrate to 1. For example, $f(y_1 | y_2)$ is equal to a scaled slice of the joint pdf $f(y_1,y_2)$ at $y_2$. @fig-conditional shows some examples for the distribution shown at @fig-bivariate-mixture at several values of $y_2$. The left plot shows the joint density, with horizontal lines indicating where conditioning (or slicing) occurs at different values of $y_2$. The right plot shows the resulting conditional density functions.

```{r}
#| label: fig-conditional
#| message: false
#| depends: fig-bivariate-mixture
#| fig-cap: Conditional distribution of $Y_2|Y_1$, where $(Y_1,Y_2)$ has the joint distribution plotted in @fig-bivariate-mixture. The left plot shows the joint density with the values of $y_2$ where we will condition, while the right plot shows conditional density functions at different values of $y_2$.
# Conditioning points
y2 <- -2:3
# Scaling factor for each density
scale_cond_density <- mixture_density |>
  filter(round(y, 2) %in% round(y2, 2)) |>
  summarise(scale = sum(z), .by = y) |>
  transmute(
    scale = scale / max(scale),
    y2 = factor(y, levels = y2)
  )
# Joint density plot
plot1 <- mixture_plot + 
  guides(fill = "none") +
  scale_y_continuous(breaks = y2, minor_breaks = NULL) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6)) +
  geom_hline(aes(yintercept = y2), data = tibble(y2 = y2), color = "#8ab3f6")
# Conditional density plots
plot2 <- mixture_density |>
  filter(round(y, 2) %in% round(y2, 2)) |>
  mutate(y2 = factor(y, levels = y2)) |>
  left_join(scale_cond_density, by = "y2") |>
  mutate(
    z = z / scale,
    z = z / max(z) * 0.9
  ) |>
  ggplot(aes(x = x, y = z + y, group = y2)) +
  geom_ribbon(aes(ymin = y, ymax = z + y, xmin = -2, xmax = 4), 
              col = "#8ab3f6", fill = "#8ab3f6") +
  labs(
    x = latex2exp::TeX("$y_1$"), y = latex2exp::TeX("$y_2$"),
    title = latex2exp::TeX("Conditional densities: $f(y_1|y_2)$")
  ) +
  scale_y_continuous(minor_breaks = NULL, breaks = y2) +
  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6))
# Show plots side by side
plot1 | plot2
```

Another neat property of Normal distributions is that the conditional distribution of a subset of variables is also Normal. For example, suppose $\bm{Y} = (Y_1,Y_2,Y_3)$ is a multivariate Normal random variable. Then the conditional distribution of $Y_1$ given $Y_2$ and $Y_3$ is also Normally distributed.

### Multivariate quantiles

Unlike the univariate case, there is no unique definition of a multivariate quantile. There are many different definitions, and each has its own advantages and disadvantages. In this book, we are mostly concerned with sample multivariate quantiles, and we will defer a more detailed discussion of multivariate quantiles to @sec-quantile-methods.

### Further reading

A good reference on multivariate probability distributions is @JKmulticontinuous1.

## Pairwise distances

Many anomaly detection algorithms are based on pairwise distances between observations. If there are $n$ observations, then there are $n(n-1)/2$ pairwise distances to compute, so this is an $O(n^2)$ operation which can take a very long time for large $n$.

Suppose our observations are denoted by $\bm{y}_1,\dots,\bm{y}_n$. When we have numerical data, we will usually want to use a Euclidean distance, named after the famous Greek mathematician Euclid. That is, the distance between points $\bm{y}_i = (y_{i1},\dots,y_{id})'$ and $\bm{y}_j=(y_{j1},\dots,y_{jd})'$ is given by
$$\|\bm{y}_i - \bm{y}_j\| = \sqrt{\sum_{k=1}^d (y_{ik}-y_{jk})^2}.$$
This is also known as the $L_2$ distance.

For $d=1$ or $d=2$, this is the physical distance between the points when plotted on a strip plot or a scatterplot (provided there is no jittering used).

The `dist()` function will return a distance matrix containing all pairwise distances computed in this way. Here is an example using only the first five observations of the `old_faithful` data set (omitting the time stamp). Because the distances are symmetric, only the lower triangle of the matrix is computed.

```{r}
oldfaithful |>
  select(-time) |>
  head(5) |>
  dist()
```

When the variables have very different scales, the variables with the largest ranges will dominate the distance measures. In this example, durations are much longer than waiting times, and so the duration variable is dominating the calculation of neighbours. Consequently, it is often preferable to scale the data before computing distances.

```{r}
oldfaithful |>
  select(-time) |>
  scale() |>
  head(5) |>
  dist()
```

The `scale()` function subtracts the mean and divides by the standard deviation for each column of data. Therefore, all variables in the resulting scaled data have mean zero and standard deviation equal to 1.

The `dist()` function will also compute other types of distances which may be more appropriate for some kinds of data. These are specified using the `method` argument and defined below.

Manhattan:
: Absolute distance between the two vectors:
$$\|\bm{y}_i - \bm{y}_j\| = \sum_{k=1}^d |y_{ik}-y_{jk}|.$$
This is also known as the $L_1$ distance.  It is called the Manhattan distance as it gives the shortest path  between the corners of city blocks (denoted as points on a grid) when those blocks are rectangular, as they mostly are in Manhattan. For the same reason, it is also sometimes called the "taxicab" distance or the "city block" distance.

Minkowski:
: This generalizes the Manhattan and Euclidean distances to use powers of $p$ to define the $L_p$ distance:
$$\|\bm{y}_i - \bm{y}_j\| = \left(\sum_{k=1}^d (y_{ik}-y_{jk})^p\right)^{1/p}.$$
The value of $p$ is specified using the `p` argument. It is named after the German mathematician Hermann Minkowski.

Maximum:
: Maximum distance between any components of $\bm{y}_i$ and $\bm{y}_j$:
$$\|\bm{y}_i - \bm{y}_j\| = \max_k |y_{ik}-y_{jk}|.$$

Canberra:
: $$\|\bm{y}_i - \bm{y}_j\| = \sum_{k=1}^d \frac{|y_{ik}-y_{jk}|}{|y_{ik}|+|y_{jk}|}.$$
Terms with zero numerator and denominator are omitted from the sum. This distance was introduced by two Australian scientists, Godfrey Lance and Bill Williams, who named it after their home city of Canberra. It scales the distances between components by the size of the components, so there is an inbuilt scaling which avoids problems with variables on different units. However, if there are components which are close to zero, but not identical to zero, the measure is numerically unstable. So it is better to scale the data explicitly before computing a distance, unless the variables cannot take small values.

Binary:
: This is designed for binary vectors where each element is either 0 or 1. The vectors are regarded as binary bits, so non-zero elements are ‘on’ and zero elements are ‘off’. The distance is the proportion of bits in which only one is on amongst those in which at least one is on. This may be useful for distances between logical variables.

For more information about pairwise distances, see @Borg2005.

## Nearest neighbours

Some algorithms only compute the pairwise distances of the $k$ nearest observations, although finding those observations requires some additional distances to be computed. For some types of distances, efficient solutions are available using kd trees [@Bentley1975;@Arya1998] that find the $k$ nearest neighbours to each observation in $O(n\log(n))$ time.

The calculation of $k$ nearest neighbours is useful for more than anomaly detection problems. It is also the basis of a popular classification method due to the Berkeley statisticians Evelyn Fix and Joe Hodges [@knn] which is often known as the "kNN algorithm".

Suppose we use the Old Faithful data to find eruptions that are neighbours in the (duration, waiting) space. The `dbscan` package uses kd trees to quickly identify the $k$ nearest observations to each eruption. As noted earlier, we will scale the data before computing any distances.

```{r}
# Find 5 nearest neighbours to each eruption
knn <- oldfaithful |>
  select(duration, waiting) |>
  scale() |>
  dbscan::kNN(k = 5)
# First eruption in the data set
oldfaithful[1, ]
# Five closest observations
oldfaithful[knn$id[1, ], ]
```

For very large data sets, approximations are available which speed up the computation even more, but are less accurate in finding the $k$ nearest neighbours. The `approx` argument specifies a distance tolerance which makes the process faster for large data sets, although the neighbours returned may not be the exact nearest  neighbours.

```{r}
# Find 5 approximate nearest neighbours to each eruption
kann <- oldfaithful |>
  select(duration, waiting) |>
  scale() |>
  dbscan::kNN(k = 5, approx = 2)
# Five closest observations
oldfaithful[kann$id[1, ], ]
```

Here the fifth closest observation has been omitted, and another nearby observation has been included instead, but otherwise the approximation has identified four of the five nearest observations.

## Kernel density estimation {#sec-kde}

Unlike the examples discussed in @sec-univariate and @sec-multivariate, we will not normally know what distribution our data come from, and they will almost never be from a standard parametric distribution such as a Normal, t, $\chi^2$ or Gamma distribution. Instead, we will need to estimate the probability density function from the data. Kernel density estimation is the most popular method for nonparametric estimation of a probability density function.

Suppose we have $n$ univariate observations, $\{y_1,\dots,y_n\}$, which are independent draws from a probability distribution, and we want to estimate the underlying probability density function. The kernel estimate [see @WJ1995] is given by
$$
  \hat{f}(y) = \frac{1}{n} \sum_{i=1}^n K_h(y-y_i),
$$ {#eq-kde}
where $K_h$ is a "kernel" function and $h$ is a bandwidth to be determined. We will use kernel functions that are themselves probability density functions with mean 0 and standard deviation $h$. Thus, a kernel density estimate is a mixture distribution with $n$ components, each of which is a kernel function centred at one of the observations. For example, the "Gaussian" kernel is $K_h(u) = \exp(-u^2/h^2)/(h\sqrt{2\pi})$, equal to the Normal density function with mean zero and standard deviation $h$. (When we use the Normal density function as a kernel, we will call it a "Gaussian" kernel, to distinguish it from the Normal distribution.) Another popular kernel is the quadratic Epanechnikov kernel given by
$K_h(u) = [1-u^2/(5h^2)]_+ / (h4\sqrt{5}/3)$, where $x_+ = \max(x,0)$.

```{r of2021, include=FALSE}
of2021 <- oldfaithful |>
  filter(as.Date(time) > "2021-01-01") |>
  head(10) |>
  mutate(eruption = row_number())
s <- sd(of2021$duration)
iqr <- IQR(of2021$duration)
h <- 0.9 * min(s, iqr / 1.34) * NROW(of2021)^(-1 / 5)
```

Now we will apply @eq-kde to the first ten Old Faithful eruption durations from 2021 that are in the `oldfaithful` data set. The kernel density estimate can be visualized as a sum of kernel functions centered over each observation with width determined by $h$, and height given by $K_h(0)/n$.

Let's suppose $K_h$ is a Gaussian kernel and let $h = `r round(h, 1)`$ (I will explain this choice below). Then we get the following set of kernel functions.

```{r kde1, fig.asp=0.2, echo=FALSE, dependson="of2021"}
#| label: fig-kde1
#| fig-cap: Kernel functions centered over the observations.
k <- tibble(x = seq(-3 * h, 3 * h, l = 1000)) |>
  mutate(y = dnorm(x, 0, h) / 10)
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration)
ggplot() +
  geom_line(data = of2021kde, aes(x = x, y = y, group = eruption), col = "gray") +
  geom_rug(
    data = of2021, mapping = aes(x = duration, y = 0),
    linewidth = 1, length = unit(0.06, "npc"), sides = "b"
  ) +
  labs(x = "y = Duration (seconds)", y = latex2exp::TeX("$K_h(y - y_i)/n$"))
```

The vertical ticks show the location of the ten observations, while the grey lines show $\frac{1}{n}K_h(y - y_i)$ for $i=1,\dots,n$.

These functions are then added together, as in @eq-kde, to give the density estimate.

```{r kde2, fig.asp=0.5, echo=FALSE, dependson="kde1"}
#| label: fig-kde2
#| fig-cap: Density estimate formed by summing the kernel functions centered on the observations.
ggplot() +
  geom_line(data = of2021kde, aes(x = x, y = y, group = eruption), col = "gray") +
  geom_rug(data = of2021, mapping = aes(x = duration, y = 0), sides = "b", size = 1) +
  geom_density(data = of2021, mapping = aes(x = duration), bw = h) +
  labs(x = "y = Duration (seconds)", y = "Density")
```

We made two choices when producing this estimate: the value of $h$ and the type of kernel $K_h$. If either was replaced with a different choice, the estimate would be different. For large data sets, it does not make much difference which kernel function is used.

The choice of $h$ is more difficult and will change the shape of the density estimate substantially. Here are three versions of the density estimate with bandwidths given by $h=5$, $h=15$ and $h=40$.

```{r kde3, fig.asp=0.5, echo=FALSE, dependson="of2021"}
#| label: fig-kde3
#| fig-cap: Kernel density estimates based on different bandwidth values $h$.
of2021 |>
  ggplot(aes(x = duration)) +
  geom_rug(sides = "b", size = 1) +
  geom_density(bw = 5, col = "#E69F00") +
  geom_density(bw = 15, col = "#56B4E9") +
  geom_density(bw = 40, col = "#009E73") +
  labs(x = "y = Duration (seconds)", y = "Density") +
  xlim(120, 320) +
  geom_label(x = 260, y = .02, label = "h = 5", col = "#E69F00") +
  geom_label(x = 275, y = .01, label = "h = 15", col = "#56B4E9") +
  geom_label(x = 320, y = .004, label = "h = 40", col = "#009E73")
```

When $h$ is too small, the density estimate is very rough with many peaks and troughs. When $h$ is too large, the density estimate is too smooth and we fail to see any features in the data. A popular choice for $h$ uses Silverman's "rule-of-thumb" [@Silverman1986,p48]:
$$
  h = 0.9 \min(s, \text{IQR}/1.34) n^{-1/5},
$$  {#eq-ruleofthumb}
where $s$ is the sample standard deviation and $\text{IQR} = \hat{q}_{0.75} - \hat{q}_{0.25}$. This tends to work reasonably well for most data sets. For the 10 observations in the example above, it gives $h = `r round(h, 1)`$, which is the value we used.

This example using only 10 observations was purely to illustrate the method. Let's now estimate a kernel density estimate for the full data set (other than that one pesky duration of 2 hours).

We will use the `geom_density()` function, which uses the Gaussian kernel and Silverman's rule-of-thumb by default, although other choices are available.

```{r oldfaithful4, fig.asp=0.45, fig.cap="Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
#| label: fig-oldfaithful4
oldfaithful |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration)) +
  geom_density() +
  geom_rug() +
  labs(x = "Duration (seconds)")
```

```{r ofbw2, include=FALSE}
of <- oldfaithful |>
  filter(duration < 7000)
s <- sd(of$duration)
iqr <- IQR(of$duration)
h <- 0.9 * min(s, iqr / 1.34) * NROW(of)^(-1 / 5)
```

As this is a much bigger data set (with `r NROW(of)` observations), the selected bandwidth is smaller and is now $h = `r round(h, 2)`$. Here the estimate has clearly identified the two groups of eruptions, one much larger than the other. The extreme observation of 1 second, and the unusual observations between 140 and 180 seconds are in the areas of low density. Later we will use density estimates at each observation to identify anomalous points.

### Statistical properties {-}

The statistical properties of the kernel density estimator @eq-kde have been extensively studied, and are described in several books including @WJ1995 and @Scott2015.

An important asymptotic result is that the mean square error (MSE) of $\hat{f}(y)$ is
$$
  \text{E}\left[(\hat{f}(y) - f(y))^2\right] \approx
  \frac{1}{4}h^4[f''(y)]^2 + \frac{f(y)R(K)}{nh} ,
$$ {#eq-mse}
where $R(K) = \int K^2(u)du$ is the "roughness" of the kernel function. An estimator is "consistent" if the MSE goes to zero as the sample size $n$ goes to infinity. So @eq-kde gives a consistent estimator of the underlying density $f$ when both terms in @eq-mse go to zero. That is,
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh = \infty,
$$ {#eq-asymptotickde}
and so $h$ should decrease slowly as $n$ increases. Note that these conditions hold for @eq-ruleofthumb.

If we integrate the MSE given by @eq-mse over $y$ (assuming $f$ is sufficiently smooth for the integral to exist), we obtain the mean integrated squared error (MISE) given by
$$
  \text{E}\int \left[(\hat{f}(y) - f(y))^2\right] dy \approx
  \frac{1}{4}h^4R(f'') + \frac{R(K)}{nh} ,
$$ {#eq-mise}
where $R(f'') = \int [f''(y)]^2 dy$ is the roughness of the second derivative of the underlying density.

The optimal overall bandwidth is obtained by minimizing the MISE. This can be calculated by differentiating @eq-mise with respect to $h$ and setting the derivative to zero, yielding
$$
  h = \left(\frac{R(K)}{R(f'')n}\right)^{1/5}.
$$ {#eq-opth}
So the optimal $h$ is proportional to $n^{-1/5}$, which clearly satisfies the conditions @eq-asymptotickde.

However, this value of $h$ depends on the underlying density $f$ which we don't know. For a Normal density $g$ with variance $\sigma^2$, $R(g) = \frac{1}{2\sqrt{\pi}}\sigma^{-1}$ and $R(g'') = \frac{3}{8\sqrt{\pi}}\sigma^{-5}$. So if we use a Gaussian kernel, and assume the underlying density has the same roughness as a Normal density, we obtain the "normal reference rule":
$$
  h = \sigma\left(\frac{4}{3n}\right)^{1/5} = 1.06\sigma n^{-1/5},
$$ {#eq-nrr}
where $\sigma$ is the standard deviation of the underlying density. This is often too large as a Normal distribution is relatively smooth (and so has a relatively low $R(f'')$ value). Silverman proposed replacing 1.06 by 0.9 and $\sigma$ by a robust estimate given by $\min(s, IQR/1.34)$, giving his rule-of-thumb @eq-ruleofthumb.

Another popular bandwidth choice is the Sheather-Jones "plug-in" bandwidth [@SJ91], obtained by replacing $R(f'')$ in @eq-opth by an estimate based on the data.  To use this bandwidth, just set `bw = "SJ"` in `geom_density()`.

Bandwidths obtained in this way are designed to give a good overall estimate of the underlying density, but may not be optimal for any particular point of the density. Our goal is to find anomalies in the data, rather than find a good representation for the rest of the data, and so we are interested in the regions of low density.

If we optimized MSE @eq-mse rather than MISE @eq-mise, we would obtain
$$
h = \left(\frac{f(y)R(K)}{n[f''(y)]^2}\right)^{1/5}.
$$
This shows that larger bandwidths are required when $f(y)/[f''(y)]^2$ is relatively large, which occurs in the extreme tails of a distribution. Often the usual bandwidth selection methods result in bandwidths that are too small and can cause the kernel density estimates of observations in the tails to be confused with anomalies. So bandwidths for anomaly detection tend to be a little larger than bandwidths for other purposes.

We can adjust the bandwidth using the argument `adjust` as follows.

```{r oldfaithful5, fig.asp=0.45, fig.cap="Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours."}
#| label: fig-oldfaithful5
oldfaithful |>
  filter(duration < 7000) |>
  ggplot(aes(x = duration)) +
  geom_density(adjust = 2) +
  geom_rug() +
  labs(x = "Duration (seconds)")
```

Here we have doubled the default bandwidth obtained using @eq-ruleofthumb.

### Boundaries

When a Gaussian kernel is used, a kernel density estimate assumes that the underlying density $f$ is smooth and non-zero on the whole real line. This will cause problems when the true density is actually zero for some regions of the sample space. For example, if all data must be positive, then $f(u)=0$ for $u<0$.

There are modified estimators which deal with this situation, but we won't concern ourselves with them here as this situation will not come up very often in the context of anomaly detection.

### Multivarate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate version of @eq-kde is given by [@Scott2015]
$$
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_H(\bm{y} - \bm{y}_i),
$$ {#eq-mkde}
where $K_H$ is a multivariate probability density with covariance matrix $\bm{H}$. Whenever we estimate a multivariate kernel density estimate, we will use a multivariate Gaussian kernel given by
$$
  K_H(\bm{u}) = (2\pi)^{-d/2} |\bm{H}|^{-1/2} \exp \{-\textstyle\frac12 \bm{u}'\bm{H}^{-1}\bm{u} \}.
$$

```{r bivariatebandwidths, include=FALSE}
h1 <- MASS::bandwidth.nrd(of2021$duration)
h2 <- MASS::bandwidth.nrd(of2021$waiting)
```

We will illustrate the idea using a simple bivariate example of 10 observations: the same 10 eruption durations discussed above, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by $\bm{H} = \left[\begin{array}{cc}`r round(h1,0)` & 0 \\ 0 & `r round(h2,0)`\end{array}\right]$.

```{r ofdw, dependson="of2021", echo=FALSE, warning=FALSE}
#| label: fig-ofdw
#| fig-cap: Contours of bivariate kernels centered over the observations.
h <- c(13, 300)
k <- expand_grid(
  x = seq(-3 * h[1], 3 * h[1], l = 100),
  y = seq(-3 * h[2], 3 * h[2], l = 100)
) |>
  mutate(z = dnorm(x, 0, h[1]) * dnorm(y, 0, h[2]))
of2021kde <- of2021 |>
  mutate(k = list(k)) |>
  unnest(k) |>
  mutate(x = x + duration, y = y + waiting)
ggplot() +
  geom_contour(data = of2021kde, aes(x = x, y = y, group = eruption, z = z), bins = 4, col = "gray") +
  geom_point(data = of2021, mapping = aes(x = duration, y = waiting)) +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

If we add the bivariate kernel functions as in @eq-mkde, we obtain the bivariate kernel density estimate shown below.

```{r ofbivariate1, dependson="of2021", echo=FALSE}
#| label: fig-ofbivariate1
#| fig-cap: Bivariate kernel density estimate formed by summing the kernels shown in @fig-ofdw.
of2021 |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point() +
  geom_density_2d() +
  labs(x = "Duration (seconds)", y = "Waiting time (seconds)") +
  xlim(147, 287) +
  ylim(3070, 6860)
```

Now we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors).

We will use the `geom_density_2d()` function, which by default uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by @eq-nrr.

```{r ofbivariate2, fig.cap="Bivariate kernel density estimate with default bandwidths."}
#| label: fig-ofbivariate2
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d() +
  labs(x = "Duration (seconds)")
```

Here we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There is one anomalous eruption where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.

If $\bm{H}$ is diagonal with values $h_1,\dots,h_d$, then the estimator is consistent when
$$
  \lim_{n\rightarrow\infty} h = 0
  \qquad\text{and}\qquad
  \lim_{n\rightarrow\infty} nh^d = \infty,
$$
where $h = (h_1h_2\dots,h_d)^{1/d}$ is the geometric mean of the diagonal values of $\bm{H}$. The default values for `geom_density_2d()` satisfy this property.

Note that the diagonal values $h_1,\dots,h_d$ tend to be larger than the values used in the corresponding univariate density estimates, as the convergence properties of the estimate are slower for larger $d$. Consequently, the default bandwidths for `geom_density_2d()` tend to be too small. Further, because we are interested in the tails of the distribution, we usually want larger bandwidths than would be suitable for obtaining good estimates of the density function.

@fig-ofbivariate3 shows a bivariate kernel density estimate where the bandwidths are double the default values.

```{r ofbivariate3, fig.cap="Bivariate kernel density estimate with double the default bandwidths."}
#| label: fig-ofbivariate3
oldfaithful |>
  filter(duration < 7000, waiting < 7000) |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(color = "gray") +
  geom_density_2d(adjust = 2) +
  labs(x = "Duration (seconds)")
```


### Further reading

There is a rich literature on kernel density estimation. A good starting point is @Scott2015 or @chacon2018multivariate.

## Extreme value theory {#sec-evt}

Extreme Value Theory is used to model rare, extreme events and is useful in anomaly detection. Suppose we have $n$ independent and identically distributed random variables $Y_1, \dots, Y_n$ with a cdf $F(y) = P\{Y \leq y\}$. Then the maximum of these $n$ random variables is $M_n = \max \{Y_1, \dots, Y_n\}$. If $F$ is known, the cdf of $M_n$ is given by $P\{M_n \leq z \} = \left(F(z)\right)^n$. However, $F$ is usually not known in practice. This gap is filled by Extreme Value Theory, which studies approximate families of models for $F^n$ so that extremes can be modeled and their uncertainty quantified.

It is well known, due to the [central limit theorem](02-tools.html#sec-clt), that the average of a set of iid random variables will converge to the normal distribution if the mean and variance both exist and are finite. The Fisher-Tippett-Gnedenko (FTG) Theorem provides an analogous result for the maximum. It was developed in a series of papers by @Frechet1927, @Fisher1928, and @Gnedenko1943. Independently, @Mises1936 proposed a similar result. The FTG Theorem states that if the maximum can be scaled so that it converges, then the scaled maximum will converge to either a Gumbel, Fréchet or Weibull distribution [@coles2001introduction, 46]; no other limits are possible.

### Fisher-Tippett-Gnedenko Theorem {-}

If there exist sequences $\{a_n\}$ and $\{b_n\}$ such that
$$
  P\left\{ \frac{(M_n - a_n)}{b_n} \leq z \right\} \rightarrow G(z) \quad \text{as} \quad n \to \infty,
$$
where $G$ is a non-degenerate cumulative distribution function, then $G$ belongs to one of the following families:
\begin{align*}
  &\text{Gumbel} :  && G(z) = \exp\left(-\exp \left[- \Big(\frac{z-b}{a}\Big) \right] \right), \quad -\infty < z < \infty , \\
  &\text{Fréchet} : && G(z) =
    \begin{cases}
      0 ,                                                           & z \leq b  , \\
      \exp \left( - \left( \frac{z-b}{a}\right)^{-\alpha} \right) , & z > b    ,
    \end{cases}                     \\
  &\text{Weibull} : && G(z) =
    \begin{cases}
      \exp \left( - \left(- \left[\frac{z-b}{a}\right]\right)^{\alpha} \right) , & z < b  ,    \\
      1 ,                                                                        & z \geq b  ,
    \end{cases}
\end{align*}
for parameters $a, b$ and $\alpha$ where $a, \alpha >0$.

These three families of distributions can be further combined into a single family by using the following distribution function known as the Generalized Extreme Value (GEV) distribution,
$$
  G(z) = \exp\left\{ -\left[ 1 + \xi\Big(\frac{z - \mu}{\sigma} \Big)\right]^{-1/\xi} \right\} ,
$$ {#eq-EVT4}
where the domain of the function is $\{z: 1 + \xi (z - \mu)/\sigma >0 \}$. The location parameter is $\mu\in\mathbb{R}$, $\sigma>0$ is the scale parameter, while $\xi\in\mathbb{R}$ is the shape parameter. When $\xi = 0$ we obtain a Gumbel distribution with exponentially decaying tails. When $\xi < 0$ we get a Weibull distribution with a finite upper end, and when $\xi > 0$ we get a Fréchet family of distributions with heavy tails including polynomial tails.

If we take the negative of the random variables $Y_1,\dots,Y_n$, it becomes clear that a similar result holds for the minimum.

The three types of limits correspond to different forms of the tail behaviour of $F$.

  * When $F$ has a finite upper bound, such as with a uniform distribution, then $G$ is a Weibull distribution.
  * When $F$ has exponential tails, such as with a normal distribution or a Gamma distribution, then $G$ is a Gumbel distribution.
  * When $F$ has heavy tails including polynomial decay, then $G$ is a Fréchet distribution. One example is when $F$ itself is a Fréchet distribution with $F(y)= e^{-1/y}$, $y>0$.

To illustrate, suppose $F$ is a standard normal distribution N(0,1) and we have $n=1000$ observations. Then the distribution of the maximum can be obtained via simulation. We will fit both a kernel density estimate and an extreme value distribution to the resulting maximums obtained.

```{r evdexample, fig.cap="Distribution of the maximum of 1000 N(0,1) draws. Here we have simulated 10000 such maximums and shown a kernel density estimate along with the density from an estimated GEV distribution."}
#| label: "fig-evdexample"
m <- 10000 # Number of simulations
n <- 1000 # Number of observations in each data set
maximums <- numeric(m)
for (i in seq(m)) {
  maximums[i] <- max(rnorm(n))
}
gev <- evd::fgev(maximums)$estimate
truedensity <- tibble(y = seq(2, 6, l = 100)) |>
  mutate(fy = evd::dgev(y, loc = gev["loc"], scale = gev["scale"], shape = gev["shape"]))
tibble(maximum = maximums) |>
  ggplot(aes(x = maximums)) +
  geom_density() +
  geom_line(data = truedensity, aes(x = y, y = fy), col = "red")
```

### The Generalized Pareto Distribution {-}

The Peaks Over Threshold (POT) approach regards extremes as observations greater than a threshold $u$. The probability distribution of *exceedances* above a specified threshold $u$ can be expressed as the conditional distribution
$$
  H(y) = P\left \{Y \leq u + y \mid Y > u \right \}
  = \frac{ F(u+y) - F(u)}{1 - F(u)}.
$$ {#eq-POT3}
When the distribution $F$ satisfies the FTG theorem, then [@coles2001introduction, 75] $H$ is a  **Generalized Pareto Distribution** (GPD) defined by
  $$
    H(y) \approx 1 - \Big( 1 + \frac{\xi y}{\sigma_u} \Big)^{-1/\xi} ,
  $$ {#eq-POT}
where the domain of $H$ is $\{y: y >0 \text{ and } (1 + \xi y)/\sigma_u >0 \}$, and $\sigma_u = \sigma + \xi(u- \mu)$. The GPD parameters are determined from the associated GEV parameters. In particular, the shape parameter $\xi$ is the same in both distributions.

Continuing the previous example, we now look at the probability distribution of exceedances above 3 from a N(0,1) distribution. We simulate 1 million N(0,1) values and only keep those above 3. Then a kernel density estimate and two GPD estimates are drawn. The red GPD uses the parameters obtained previously from the GEV estimate, while the blue GPD estimates the parameters from the exceedances.

```{r pot, dependson="evdexample", fig.cap="Conditional distribution of exceedances above 3 from N(0,1) draws. Here we have simulated 1 million values and only kept those above 3. A kernel density estimate is shown (in black) along with the density implied by the GEV distribution (in red), and an estimated GPD distribution (in blue). The boundary at 3 causes the kernel density estimate to be biased around 3."}
#| label: fig-pot
df <- tibble(y = rnorm(1e6)) |>
  filter(y > 3)
gpd <- evd::fpot(df$y, 3)$estimate
truedensity <- truedensity |>
  mutate(
    hy = evd::dgpd(y, loc = 3, scale = gev["scale"], shape = gev["shape"]),
    hy2 = evd::dgpd(y, loc = 3, scale = gpd["scale"], shape = gpd["shape"])
  )
df |>
  ggplot(aes(x = y)) +
  geom_density() +
  geom_line(data = truedensity, aes(x = y, y = hy), col = "red") +
  geom_line(data = truedensity, aes(x = y, y = hy2), col = "blue")
```

The red estimate is better because it is based on more information (10000 maximums rather than `r NROW(df)` exceedances). The kernel density estimate (in black) is biased around 3 because of the boundary problem discussed in @sec-kde. Otherwise, the kernel density estimate closely matches the GPD estimates.
